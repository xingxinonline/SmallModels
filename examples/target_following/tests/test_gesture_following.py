"""
æ‰‹åŠ¿æ§åˆ¶ç›®æ ‡è·Ÿéšæµ‹è¯•
Gesture-Controlled Target Following

æ‰‹åŠ¿æ§åˆ¶:
  - ğŸ‘‹ å¼ å¼€æ‰‹æŒæŒç»­3ç§’: Toggle å¯åŠ¨/åœæ­¢è·Ÿéš
    - ç©ºé—²çŠ¶æ€ â†’ å¯åŠ¨è·Ÿéš (é”å®šæœ€è¿‘çš„äºº)
    - è·Ÿè¸ªçŠ¶æ€ â†’ åœæ­¢è·Ÿéš (æ¸…é™¤ç›®æ ‡)

ç³»ç»ŸçŠ¶æ€:
  - IDLE: ç©ºé—²çŠ¶æ€ï¼Œç­‰å¾…æ‰‹åŠ¿å¯åŠ¨
  - TRACKING: è·Ÿéšä¸­ï¼ŒæŒç»­è·Ÿè¸ªç›®æ ‡
  - LOST_TARGET: ç›®æ ‡ä¸¢å¤±ï¼Œç­‰å¾…é‡æ–°å‡ºç°æˆ–æ‰‹åŠ¿åœæ­¢

é”®ç›˜æ§åˆ¶ (å¤‡ç”¨):
  - 's': æ‰‹åŠ¨ä¿å­˜ç›®æ ‡
  - 'a': æ‰‹åŠ¨æ·»åŠ è§†è§’
  - 'c': æ‰‹åŠ¨æ¸…é™¤ç›®æ ‡
  - 'm': åˆ‡æ¢è‡ªåŠ¨å­¦ä¹ 
  - 'q': é€€å‡º
"""

import cv2
import numpy as np
import sys
import os
import time

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import (
    MODELS_DIR, GestureType, GestureConfig, SystemState,
    YOLOv5PersonConfig, FaceDetectorConfig, MobileFaceNetConfig
)
from detectors.yolov5_person_detector import YOLOv5PersonDetector
from detectors.face_detector import FaceDetector
from detectors.mobilefacenet_recognizer import MobileFaceNetRecognizer
from detectors.gesture_detector import GestureDetector, GestureResult
from detectors.enhanced_reid import EnhancedReIDExtractor, EnhancedReIDConfig
from detectors.multiview_recognizer import (
    MultiViewRecognizer, MultiViewConfig, ViewFeature
)
from core.state_machine import StateMachine


# æ‰‹åŠ¿é…ç½®
GESTURE_HOLD_DURATION = 3.0  # è§¦å‘éœ€è¦ä¿æŒçš„ç§’æ•°
GESTURE_COOLDOWN_SECONDS = 3.0  # è§¦å‘åå†·å´ç§’æ•° (é˜²æ­¢è¿ç»­è§¦å‘)

# ä»…äººè„¸åŒ¹é…é˜ˆå€¼
# é—®é¢˜ï¼šä¸åŒäººä¹‹é—´ä¹Ÿå¯èƒ½æœ‰ 0.55-0.65 çš„ç›¸ä¼¼åº¦
# è§£å†³ï¼šæé«˜é˜ˆå€¼åˆ° 0.70ï¼Œç‰ºç‰²ä¸€äº›å¬å›ç‡æ¢å–ç²¾ç¡®ç‡
FACE_ONLY_THRESHOLD = 0.70

# è‡ªåŠ¨å­¦ä¹ é˜ˆå€¼ - åªæœ‰éå¸¸é«˜ä¿¡å¿ƒæ—¶æ‰å­¦ä¹ ï¼Œé¿å…æ±¡æŸ“
FACE_LEARN_THRESHOLD = 0.80

# é‡æ–°é”å®šé˜ˆå€¼ - ä»ä¸¢å¤±çŠ¶æ€æ¢å¤éœ€è¦æ›´é«˜ä¿¡å¿ƒ
RELOCK_FACE_THRESHOLD = 0.75


def extract_view_feature(
    frame: np.ndarray,
    person_bbox: np.ndarray,
    faces: list,
    face_recognizer,
    enhanced_reid
) -> ViewFeature:
    """æå–è§†è§’ç‰¹å¾"""
    view = ViewFeature(timestamp=time.time())
    
    px1, py1, px2, py2 = person_bbox.astype(int)
    
    # æŸ¥æ‰¾äººè„¸
    for face in faces:
        fx1, fy1, fx2, fy2 = face.bbox.astype(int)
        fc_x, fc_y = (fx1 + fx2) // 2, (fy1 + fy2) // 2
        
        if px1 <= fc_x <= px2 and py1 <= fc_y <= py2:
            face_feature = face_recognizer.extract_feature(
                frame, face.bbox, face.keypoints
            )
            if face_feature:
                view.has_face = True
                view.face_embedding = face_feature.embedding
            break
    
    # äººä½“ç‰¹å¾
    body_feature = enhanced_reid.extract_feature(frame, person_bbox)
    if body_feature:
        view.part_color_hists = body_feature.part_color_hists
        view.part_lbp_hists = body_feature.part_lbp_hists
        view.geometry = body_feature.geometry
    
    return view


def find_nearest_person(persons: list, frame_center: tuple):
    """æ‰¾åˆ°ç¦»ç”»é¢ä¸­å¿ƒæœ€è¿‘çš„äºº"""
    if not persons:
        return None, -1
    
    min_dist = float('inf')
    nearest_idx = 0
    
    for i, person in enumerate(persons):
        px1, py1, px2, py2 = person.bbox
        cx = (px1 + px2) / 2
        cy = (py1 + py2) / 2
        dist = (cx - frame_center[0])**2 + (cy - frame_center[1])**2
        if dist < min_dist:
            min_dist = dist
            nearest_idx = i
    
    return persons[nearest_idx], nearest_idx


def draw_gesture_indicator(frame, gesture: GestureResult, state: SystemState, hold_progress: float = 0.0):
    """ç»˜åˆ¶æ‰‹åŠ¿æŒ‡ç¤ºå™¨"""
    h, w = frame.shape[:2]
    
    # ç»˜åˆ¶æ‰‹éƒ¨æ¡†
    if gesture.hand_bbox is not None:
        hx1, hy1, hx2, hy2 = gesture.hand_bbox.astype(int)
        
        if gesture.gesture_type == GestureType.OPEN_PALM:
            color = (0, 255, 255)  # é»„è‰²
            if state == SystemState.IDLE:
                text = "HOLD TO START"
            else:
                text = "HOLD TO STOP"
        else:
            color = (255, 165, 0)
            text = ""
        
        cv2.rectangle(frame, (hx1, hy1), (hx2, hy2), color, 2)
        if text:
            cv2.putText(frame, text, (hx1, hy1 - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        
        # ç»˜åˆ¶æŒç»­è¿›åº¦æ¡
        if hold_progress > 0:
            bar_width = hx2 - hx1
            bar_height = 8
            bar_y = hy2 + 5
            
            # èƒŒæ™¯
            cv2.rectangle(frame, (hx1, bar_y), (hx2, bar_y + bar_height), (50, 50, 50), -1)
            # è¿›åº¦
            progress_width = int(bar_width * hold_progress)
            progress_color = (0, 255, 0) if hold_progress < 1.0 else (0, 255, 255)
            cv2.rectangle(frame, (hx1, bar_y), (hx1 + progress_width, bar_y + bar_height), progress_color, -1)
            # è¾¹æ¡†
            cv2.rectangle(frame, (hx1, bar_y), (hx2, bar_y + bar_height), (255, 255, 255), 1)
            
            # è¿›åº¦ç™¾åˆ†æ¯”
            pct_text = f"{int(hold_progress * 100)}%"
            cv2.putText(frame, pct_text, (hx1, bar_y + bar_height + 15),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
    
    # çŠ¶æ€æŒ‡ç¤ºå™¨ (å³ä¸Šè§’)
    state_colors = {
        SystemState.IDLE: (128, 128, 128),          # ç°è‰²
        SystemState.TRACKING: (0, 255, 0),          # ç»¿è‰²
        SystemState.LOST_TARGET: (0, 165, 255)      # æ©™è‰²
    }
    state_color = state_colors.get(state, (255, 255, 255))
    
    cv2.circle(frame, (w - 30, 30), 15, state_color, -1)
    cv2.putText(frame, state.value, (w - 120, 55),
               cv2.FONT_HERSHEY_SIMPLEX, 0.5, state_color, 1)


def main():
    print("=" * 60)
    print("    æ‰‹åŠ¿æ§åˆ¶ç›®æ ‡è·Ÿéšç³»ç»Ÿ")
    print("=" * 60)
    
    # æ£€æŸ¥æ¨¡å‹
    yolo_path = os.path.join(MODELS_DIR, "yolov5n.onnx")
    scrfd_path = os.path.join(MODELS_DIR, "scrfd_500m_bnkps.onnx")
    mobilefacenet_path = os.path.join(MODELS_DIR, "mobilefacenet.onnx")
    
    missing = []
    if not os.path.exists(yolo_path):
        missing.append("yolov5n.onnx")
    if not os.path.exists(scrfd_path):
        missing.append("scrfd_500m_bnkps.onnx")
    if not os.path.exists(mobilefacenet_path):
        missing.append("mobilefacenet.onnx")
    
    if missing:
        print(f"\n[é”™è¯¯] ç¼ºå°‘æ¨¡å‹: {missing}")
        return
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    person_detector = YOLOv5PersonDetector(YOLOv5PersonConfig(model_path=yolo_path))
    face_detector = FaceDetector(FaceDetectorConfig(model_path=scrfd_path))
    face_recognizer = MobileFaceNetRecognizer(MobileFaceNetConfig(model_path=mobilefacenet_path))
    
    # æ‰‹åŠ¿æ£€æµ‹å™¨ (confirm_frames=1ï¼Œå› ä¸ºæŒç»­æ—¶é—´æ£€æµ‹åœ¨çŠ¶æ€æœºä¸­)
    gesture_config = GestureConfig(
        min_detection_confidence=0.7,
        min_tracking_confidence=0.5,
        gesture_confirm_frames=1  # ç«‹å³å“åº”ï¼ŒæŒç»­æ—¶é—´ç”±çŠ¶æ€æœºæ§åˆ¶
    )
    gesture_detector = GestureDetector(gesture_config)
    
    # å¢å¼ºç‰ˆ ReID
    enhanced_reid = EnhancedReIDExtractor(EnhancedReIDConfig(
        num_horizontal_parts=6,
        use_lbp=True,
        use_geometry=True
    ))
    
    # å¤šè§†è§’è¯†åˆ«å™¨
    mv_config = MultiViewConfig(
        face_weight=0.6,
        body_weight=0.4,
        face_threshold=0.65,      # æé«˜äººè„¸é˜ˆå€¼ï¼Œé˜²æ­¢è¯¯åŒ¹é…
        body_threshold=0.60,      # äººä½“é˜ˆå€¼
        fused_threshold=0.55,     # èåˆé˜ˆå€¼
        motion_weight=0.10,       # é™ä½è¿åŠ¨æƒé‡
        auto_learn=True,
        learn_interval=3.0,       # å­¦ä¹ é—´éš”
        smooth_window=5,
        confirm_threshold=3,
        part_weights=[0.05, 0.12, 0.20, 0.20, 0.25, 0.18]
    )
    mv_recognizer = MultiViewRecognizer(mv_config)
    
    # åŠ è½½æ¨¡å‹
    if not person_detector.load():
        print("[é”™è¯¯] äººä½“æ£€æµ‹å™¨åŠ è½½å¤±è´¥")
        return
    if not face_detector.load():
        print("[é”™è¯¯] äººè„¸æ£€æµ‹å™¨åŠ è½½å¤±è´¥")
        return
    if not face_recognizer.load():
        print("[é”™è¯¯] äººè„¸è¯†åˆ«å™¨åŠ è½½å¤±è´¥")
        return
    if not gesture_detector.load():
        print("[é”™è¯¯] æ‰‹åŠ¿æ£€æµ‹å™¨åŠ è½½å¤±è´¥")
        return
    
    enhanced_reid.load()
    
    # æ‰“å¼€æ‘„åƒå¤´
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("[é”™è¯¯] æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
        return
    
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    print("\n[æ‰‹åŠ¿æ§åˆ¶]")
    print(f"  ğŸ‘‹ å¼ å¼€æ‰‹æŒæŒç»­ {GESTURE_HOLD_DURATION:.0f} ç§’: Toggle å¯åŠ¨/åœæ­¢è·Ÿéš")
    print("\n[é”®ç›˜æ§åˆ¶]")
    print("  's': æ‰‹åŠ¨ä¿å­˜ç›®æ ‡")
    print("  'a': æ·»åŠ è§†è§’")
    print("  'c': æ¸…é™¤ç›®æ ‡")
    print("  'm': åˆ‡æ¢è‡ªåŠ¨å­¦ä¹ ")
    print("  'q': é€€å‡º")
    print()
    
    # çŠ¶æ€æœº (ä½¿ç”¨ä¹‹å‰å®ç°çš„æŒç»­æ—¶é—´æ£€æµ‹)
    state_machine = StateMachine(
        lost_timeout_frames=30,
        gesture_hold_duration=GESTURE_HOLD_DURATION,
        gesture_cooldown_seconds=GESTURE_COOLDOWN_SECONDS
    )
    
    lost_frames = 0
    max_lost_frames = 30
    
    frame_count = 0
    fps_start = time.time()
    fps = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame_count += 1
        h, w = frame.shape[:2]
        frame_center = (w // 2, h // 2)
        
        # è®¡ç®— FPS
        if frame_count % 30 == 0:
            fps = 30 / (time.time() - fps_start)
            fps_start = time.time()
        
        # æ£€æµ‹
        persons = person_detector.detect(frame)
        faces = face_detector.detect(frame)
        gesture = gesture_detector.detect(frame)
        
        # è°ƒè¯•æ—¥å¿— (æ¯30å¸§è¾“å‡ºä¸€æ¬¡)
        if frame_count % 30 == 0:
            print(f"[DEBUG] Frame {frame_count}: persons={len(persons)}, faces={len(faces)}, gesture={gesture.gesture_type.value}")
            if faces:
                for i, face in enumerate(faces):
                    print(f"        Face[{i}]: bbox={face.bbox.astype(int).tolist()}, conf={face.confidence:.2f}")
            if persons:
                for i, person in enumerate(persons):
                    print(f"        Person[{i}]: bbox={person.bbox.astype(int).tolist()}, conf={person.confidence:.2f}")
        
        # ============== æ‰‹åŠ¿çŠ¶æ€æœº (æŒç»­æ—¶é—´æ£€æµ‹) ==============
        current_time = time.time()
        old_state = state_machine.state
        
        # å¤„ç†æ‰‹åŠ¿ (éœ€è¦æŒç»­ GESTURE_HOLD_DURATION ç§’)
        state_changed = state_machine.process_gesture(gesture.gesture_type, current_time, debug=False)
        
        # è·å–æŒç»­è¿›åº¦
        hold_progress = state_machine.get_gesture_hold_progress()
        
        # çŠ¶æ€æœºè°ƒè¯•æ—¥å¿—
        if hold_progress > 0 and frame_count % 10 == 0:
            print(f"[STATE] gesture={gesture.gesture_type.value}, hold={hold_progress*100:.0f}%, state={state_machine.state.value}")
        
        # çŠ¶æ€å˜æ›´å¤„ç†
        if state_changed:
            if state_machine.state == SystemState.TRACKING and old_state == SystemState.IDLE:
                # å¯åŠ¨è·Ÿéš - ä¼˜å…ˆä½¿ç”¨äººä½“ï¼Œå…¶æ¬¡ä½¿ç”¨äººè„¸
                nearest_person, idx = find_nearest_person(persons, frame_center)
                
                if nearest_person is not None:
                    # æœ‰äººä½“æ£€æµ‹ç»“æœ
                    print(f"[DEBUG] é”å®šäººä½“: bbox={nearest_person.bbox.astype(int).tolist()}")
                    view = extract_view_feature(
                        frame, nearest_person.bbox, faces, 
                        face_recognizer, enhanced_reid
                    )
                    print(f"[DEBUG] æå–ç‰¹å¾: has_face={view.has_face}, has_body={view.part_color_hists is not None}")
                    if view.has_face and view.face_embedding is not None:
                        print(f"[DEBUG] äººè„¸embedding: shape={view.face_embedding.shape}, norm={np.linalg.norm(view.face_embedding):.3f}")
                    mv_recognizer.set_target(view, nearest_person.bbox)
                    lost_frames = 0
                    face_str = "æœ‰äººè„¸" if view.has_face else "æ— äººè„¸"
                    print(f"[æ‰‹åŠ¿å¯åŠ¨] ç›®æ ‡å·²é”å®š (äººä½“+{face_str})")
                elif faces:
                    # æ²¡æœ‰äººä½“ä½†æœ‰äººè„¸ - ç”¨äººè„¸æ¡†ä½œä¸ºä¸´æ—¶ç›®æ ‡
                    # æ‰¾ç¦»ç”»é¢ä¸­å¿ƒæœ€è¿‘çš„äººè„¸
                    min_dist = float('inf')
                    nearest_face = None
                    for face in faces:
                        fx1, fy1, fx2, fy2 = face.bbox
                        fcx, fcy = (fx1 + fx2) / 2, (fy1 + fy2) / 2
                        dist = (fcx - frame_center[0])**2 + (fcy - frame_center[1])**2
                        if dist < min_dist:
                            min_dist = dist
                            nearest_face = face
                    
                    if nearest_face is not None:
                        # ç”¨äººè„¸æ¡†æ‰©å±•ä¸ºä¼ªäººä½“æ¡†ï¼ˆå‘ä¸‹æ‰©å±•3å€ï¼‰
                        fx1, fy1, fx2, fy2 = nearest_face.bbox
                        face_h = fy2 - fy1
                        face_w = fx2 - fx1
                        print(f"[DEBUG] ä»…äººè„¸æ¨¡å¼: face_bbox={nearest_face.bbox.astype(int).tolist()}")
                        # äººè„¸å¤§çº¦æ˜¯äººä½“çš„1/7ï¼Œå‘ä¸‹æ‰©å±•
                        pseudo_bbox = np.array([
                            max(0, fx1 - face_w * 0.5),
                            fy1,
                            min(w, fx2 + face_w * 0.5),
                            min(h, fy2 + face_h * 5)
                        ])
                        print(f"[DEBUG] ä¼ªäººä½“æ¡†: pseudo_bbox={pseudo_bbox.astype(int).tolist()}")
                        
                        view = ViewFeature(timestamp=time.time())
                        view.has_face = True
                        face_feature = face_recognizer.extract_feature(
                            frame, nearest_face.bbox, nearest_face.keypoints
                        )
                        if face_feature:
                            view.face_embedding = face_feature.embedding
                            print(f"[DEBUG] äººè„¸ç‰¹å¾æå–æˆåŠŸ: embedding_shape={face_feature.embedding.shape}, norm={np.linalg.norm(face_feature.embedding):.3f}")
                        else:
                            print(f"[DEBUG] äººè„¸ç‰¹å¾æå–å¤±è´¥!")
                        
                        mv_recognizer.set_target(view, pseudo_bbox)
                        print(f"[DEBUG] ç›®æ ‡å·²è®¾ç½®: has_face_view={mv_recognizer.target.has_face_view if mv_recognizer.target else False}")
                        lost_frames = 0
                        print(f"[æ‰‹åŠ¿å¯åŠ¨] ç›®æ ‡å·²é”å®š (ä»…äººè„¸ï¼Œç­‰å¾…äººä½“è¡¥å……)")
                else:
                    # æ—¢æ²¡æœ‰äººä½“ä¹Ÿæ²¡æœ‰äººè„¸
                    state_machine.state = SystemState.IDLE
                    print("[æç¤º] æœªæ£€æµ‹åˆ°äººä½“æˆ–äººè„¸ï¼Œæ— æ³•å¯åŠ¨")
            
            elif state_machine.state == SystemState.IDLE and old_state == SystemState.TRACKING:
                # åœæ­¢è·Ÿéš - åªæœ‰ä» TRACKING çŠ¶æ€æ‰èƒ½åœæ­¢
                mv_recognizer.clear_target()
                lost_frames = 0
                print("[æ‰‹åŠ¿åœæ­¢] è·Ÿéšå·²åœæ­¢")
        
        # ============== ç›®æ ‡è·Ÿè¸ª ==============
        target_person_idx = -1
        target_face_idx = -1  # ä»…äººè„¸åŒ¹é…æ—¶çš„ç´¢å¼•
        current_match_info = None  # å½“å‰å¸§åŒ¹é…ä¿¡æ¯ï¼Œç”¨äºç•Œé¢æ˜¾ç¤º
        
        if state_machine.state == SystemState.TRACKING:
            matched_any = False
            
            # è°ƒè¯•: æ˜¾ç¤ºç›®æ ‡ä¿¡æ¯
            if frame_count % 30 == 0 and mv_recognizer.target:
                t = mv_recognizer.target
                print(f"[DEBUG] Target: num_views={t.num_views}, has_face_view={t.has_face_view}")
                for vi, v in enumerate(t.view_features):
                    print(f"        View[{vi}]: has_face={v.has_face}, has_body={v.part_color_hists is not None}")
            
            # 1. ä¼˜å…ˆé€šè¿‡äººä½“åŒ¹é…
            for idx, person in enumerate(persons):
                view = extract_view_feature(
                    frame, person.bbox, faces, face_recognizer, enhanced_reid
                )
                
                is_match, similarity, method = mv_recognizer.is_same_target(
                    view, person.bbox
                )
                
                if frame_count % 30 == 0:
                    print(f"[DEBUG] Person[{idx}] match: is_match={is_match}, sim={similarity:.3f}, method={method}")
                
                if is_match:
                    matched_any = True
                    target_person_idx = idx
                    lost_frames = 0
                    
                    # ä¿å­˜å½“å‰åŒ¹é…ä¿¡æ¯ç”¨äºæ˜¾ç¤º
                    current_match_info = {
                        'type': 'person',
                        'similarity': similarity,
                        'method': method,
                        'threshold': mv_recognizer.config.fused_threshold if 'fused' in method else mv_recognizer.config.body_threshold
                    }
                    
                    # æ›´æ–°è·Ÿè¸ª
                    mv_recognizer.update_tracking(person.bbox)
                    
                    # è‡ªåŠ¨å­¦ä¹ ç­–ç•¥:
                    # 1. å¦‚æœç›®æ ‡åªæœ‰äººè„¸æ²¡æœ‰äººä½“ -> ç§¯æå­¦ä¹ äººä½“ç‰¹å¾ï¼ˆè¡¥å……å¤šæ¨¡æ€ï¼‰
                    # 2. å¦åˆ™ç”¨é«˜é˜ˆå€¼è¿‡æ»¤
                    should_learn = False
                    target_has_body = any(v.has_body for v in mv_recognizer.target.view_features)
                    
                    if not target_has_body and view.has_body:
                        # ç›®æ ‡ç¼ºå°‘äººä½“ç‰¹å¾ï¼Œç§¯æå­¦ä¹ 
                        should_learn = True
                        learn_reason = "è¡¥å……äººä½“ç‰¹å¾"
                    elif similarity >= FACE_LEARN_THRESHOLD:
                        # é«˜ç½®ä¿¡åŒ¹é…ï¼Œå­¦ä¹ æ–°è§’åº¦
                        should_learn = True
                        learn_reason = f"é«˜ç½®ä¿¡(sim={similarity:.2f})"
                    
                    if should_learn and mv_recognizer.auto_learn(view, person.bbox, True):
                        print(f"[è‡ªåŠ¨å­¦ä¹ ] {learn_reason}, æ€»æ•°: {mv_recognizer.target.num_views}")
                    break
            
            # 2. å¦‚æœäººä½“æ²¡åŒ¹é…åˆ°ï¼Œå°è¯•ä»…é€šè¿‡äººè„¸åŒ¹é…ï¼ˆä½¿ç”¨æ›´ä¸¥æ ¼çš„é˜ˆå€¼ï¼‰
            if not matched_any and faces and mv_recognizer.target and mv_recognizer.target.has_face_view:
                if frame_count % 30 == 0:
                    print(f"[DEBUG] äººä½“åŒ¹é…å¤±è´¥ï¼Œå°è¯•ä»…äººè„¸åŒ¹é… (é˜ˆå€¼={FACE_ONLY_THRESHOLD})...")
                
                best_face_match = None
                best_face_sim = 0.0
                best_face_idx = -1
                best_view_idx = -1
                    
                for face_idx, face in enumerate(faces):
                    face_feature = face_recognizer.extract_feature(
                        frame, face.bbox, face.keypoints
                    )
                    if face_feature and face_feature.embedding is not None:
                        # ä¸ç›®æ ‡äººè„¸ç‰¹å¾æ¯”è¾ƒï¼Œæ‰¾æœ€é«˜ç›¸ä¼¼åº¦
                        for vi, view in enumerate(mv_recognizer.target.view_features):
                            if view.has_face and view.face_embedding is not None:
                                sim = float(np.dot(face_feature.embedding, view.face_embedding))
                                if frame_count % 30 == 0:
                                    print(f"[DEBUG] Face[{face_idx}] vs View[{vi}]: sim={sim:.3f}, threshold={FACE_ONLY_THRESHOLD}")
                                if sim > best_face_sim:
                                    best_face_sim = sim
                                    best_face_idx = face_idx
                                    best_view_idx = vi
                
                # ä½¿ç”¨æ›´ä¸¥æ ¼çš„é˜ˆå€¼åˆ¤æ–­
                if best_face_sim >= FACE_ONLY_THRESHOLD:
                    matched_any = True
                    target_face_idx = best_face_idx
                    lost_frames = 0
                    
                    # ä¿å­˜å½“å‰åŒ¹é…ä¿¡æ¯ç”¨äºæ˜¾ç¤º
                    current_match_info = {
                        'type': 'face_only',
                        'similarity': best_face_sim,
                        'method': f'face_only (vs View[{best_view_idx}])',
                        'threshold': FACE_ONLY_THRESHOLD
                    }
                    
                    # ç”¨äººè„¸æ¡†æ›´æ–°ä½ç½®
                    mv_recognizer.update_tracking(faces[best_face_idx].bbox)
                    if frame_count % 30 == 0:
                        print(f"[DEBUG] äººè„¸åŒ¹é…æˆåŠŸ! face_idx={best_face_idx}, sim={best_face_sim:.3f}")
                    
                    # é«˜ç›¸ä¼¼åº¦æ—¶å…è®¸è‡ªåŠ¨å­¦ä¹ ï¼ˆå¢åŠ å¤šè§’åº¦è§†å›¾ï¼‰
                    if best_face_sim >= FACE_LEARN_THRESHOLD:
                        face_only_view = ViewFeature(timestamp=time.time())
                        face_feature = face_recognizer.extract_feature(
                            frame, faces[best_face_idx].bbox, faces[best_face_idx].keypoints
                        )
                        if face_feature:
                            face_only_view.has_face = True
                            face_only_view.face_embedding = face_feature.embedding
                            if mv_recognizer.auto_learn(face_only_view, faces[best_face_idx].bbox, True):
                                print(f"[è‡ªåŠ¨å­¦ä¹ ] æ–°äººè„¸è§†è§’(sim={best_face_sim:.2f}), æ€»æ•°: {mv_recognizer.target.num_views}")
                elif frame_count % 30 == 0 and best_face_sim > 0:
                    print(f"[DEBUG] äººè„¸æœ€é«˜ç›¸ä¼¼åº¦ {best_face_sim:.3f} < é˜ˆå€¼ {FACE_ONLY_THRESHOLD}")
            
            if not matched_any:
                lost_frames += 1
                if frame_count % 30 == 0:
                    print(f"[DEBUG] æœªåŒ¹é…, lost_frames={lost_frames}/{max_lost_frames}")
                if lost_frames >= max_lost_frames:
                    state_machine.state = SystemState.LOST_TARGET
                    print("[ç›®æ ‡ä¸¢å¤±] ç­‰å¾…é‡æ–°å‡ºç°æˆ–æ‰‹åŠ¿åœæ­¢")
        
        elif state_machine.state == SystemState.LOST_TARGET:
            # å°è¯•é‡æ–°åŒ¹é… - å¿…é¡»åŒæ—¶æœ‰äººè„¸éªŒè¯ï¼Œæˆ–äººä½“ç›¸ä¼¼åº¦éå¸¸é«˜
            # è¿™æ˜¯é˜²æ­¢è¯¯é”å®šçš„å…³é”®ï¼
            matched_any = False
            
            # é‡æ–°é”å®šçš„é˜ˆå€¼è¦æ±‚æ›´é«˜
            RELOCK_BODY_THRESHOLD = 0.70  # ä»…äººä½“æ—¶éœ€è¦æ›´é«˜ç›¸ä¼¼åº¦
            RELOCK_FUSED_THRESHOLD = 0.65  # æœ‰äººè„¸æ—¶å¯ä»¥ç¨ä½
            
            for idx, person in enumerate(persons):
                view = extract_view_feature(
                    frame, person.bbox, faces, face_recognizer, enhanced_reid
                )
                
                is_match, similarity, method = mv_recognizer.is_same_target(
                    view, person.bbox
                )
                
                # é‡æ–°é”å®šéœ€è¦æ›´ä¸¥æ ¼çš„éªŒè¯
                if is_match:
                    # æ£€æŸ¥åŒ¹é…ç±»å‹å’Œé˜ˆå€¼
                    if 'fused' in method and view.has_face:
                        # æœ‰äººè„¸çš„èåˆåŒ¹é… - ä½¿ç”¨è¾ƒé«˜é˜ˆå€¼
                        if similarity >= RELOCK_FUSED_THRESHOLD:
                            state_machine.state = SystemState.TRACKING
                            target_person_idx = idx
                            lost_frames = 0
                            mv_recognizer.update_tracking(person.bbox)
                            print(f"[é‡æ–°é”å®š] ç›®æ ‡å·²æ¢å¤ (äººä½“+äººè„¸, sim={similarity:.2f})")
                            matched_any = True
                            break
                    elif similarity >= RELOCK_BODY_THRESHOLD:
                        # ä»…äººä½“åŒ¹é… - éœ€è¦æ›´é«˜ç›¸ä¼¼åº¦
                        state_machine.state = SystemState.TRACKING
                        target_person_idx = idx
                        lost_frames = 0
                        mv_recognizer.update_tracking(person.bbox)
                        print(f"[é‡æ–°é”å®š] ç›®æ ‡å·²æ¢å¤ (ä»…äººä½“, sim={similarity:.2f})")
                        matched_any = True
                        break
            
            # ä»…äººè„¸åŒ¹é…
            if not matched_any and faces and mv_recognizer.target and mv_recognizer.target.has_face_view:
                for face_idx, face in enumerate(faces):
                    face_feature = face_recognizer.extract_feature(
                        frame, face.bbox, face.keypoints
                    )
                    if face_feature and face_feature.embedding is not None:
                        for view in mv_recognizer.target.view_features:
                            if view.has_face and view.face_embedding is not None:
                                sim = float(np.dot(face_feature.embedding, view.face_embedding))
                                # é‡æ–°é”å®šç”¨æ›´é«˜é˜ˆå€¼ï¼Œç¡®ä¿æ˜¯åŒä¸€äºº
                                if sim >= RELOCK_FACE_THRESHOLD:
                                    state_machine.state = SystemState.TRACKING
                                    target_face_idx = face_idx
                                    lost_frames = 0
                                    mv_recognizer.update_tracking(face.bbox)
                                    print(f"[é‡æ–°é”å®š] ç›®æ ‡å·²æ¢å¤ (äººè„¸, ç›¸ä¼¼åº¦: {sim:.3f})")
                                    matched_any = True
                                    break
                        if matched_any:
                            break
        
        # ============== ç»˜åˆ¶ ==============
        # ç»˜åˆ¶äººä½“æ¡†
        for idx, person in enumerate(persons):
            px1, py1, px2, py2 = person.bbox.astype(int)
            
            if state_machine.state == SystemState.IDLE:
                color = (255, 165, 0)  # æ©™è‰²
                label = "Candidate"
            elif idx == target_person_idx:
                color = (0, 255, 0)  # ç»¿è‰²
                label = "TARGET"
            else:
                color = (0, 0, 255)  # çº¢è‰²
                label = "Other"
            
            cv2.rectangle(frame, (px1, py1), (px2, py2), color, 2)
            
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
            cv2.rectangle(frame, (px1, py1 - label_size[1] - 5),
                         (px1 + label_size[0], py1), color, -1)
            cv2.putText(frame, label, (px1, py1 - 3),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # ç»˜åˆ¶äººè„¸æ¡† (ä»…äººè„¸åŒ¹é…æ—¶é«˜äº®ç›®æ ‡äººè„¸)
        for face_idx, face in enumerate(faces):
            fx1, fy1, fx2, fy2 = face.bbox.astype(int)
            if face_idx == target_face_idx and target_person_idx < 0:
                # ä»…äººè„¸åŒ¹é…çš„ç›®æ ‡
                cv2.rectangle(frame, (fx1, fy1), (fx2, fy2), (0, 255, 0), 2)
                cv2.putText(frame, "TARGET(Face)", (fx1, fy1 - 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
            elif state_machine.state == SystemState.IDLE:
                cv2.rectangle(frame, (fx1, fy1), (fx2, fy2), (255, 200, 0), 1)
        
        # ç»˜åˆ¶æ‰‹åŠ¿æŒ‡ç¤ºå™¨ (å«è¿›åº¦æ¡)
        draw_gesture_indicator(frame, gesture, state_machine.state, hold_progress)
        
        # çŠ¶æ€ä¿¡æ¯
        target_info = "None"
        if mv_recognizer.target:
            num_views = mv_recognizer.target.num_views
            # ç»Ÿè®¡æœ‰äººè„¸å’Œæœ‰äººä½“çš„è§†è§’æ•°é‡
            face_views = sum(1 for v in mv_recognizer.target.view_features if v.has_face)
            body_views = sum(1 for v in mv_recognizer.target.view_features if v.part_color_hists is not None)
            target_info = f"Views={num_views} (F:{face_views} B:{body_views})"
        
        # åŒ¹é…ä¿¡æ¯
        match_info = ""
        if current_match_info:
            sim = current_match_info['similarity']
            thresh = current_match_info['threshold']
            mtype = current_match_info['type']
            match_info = f"Match: {mtype} sim={sim:.2f} (>={thresh:.2f})"
        
        info_lines = [
            f"FPS: {fps:.1f}",
            f"State: {state_machine.state.value}",
            f"Persons: {len(persons)}, Faces: {len(faces)}",
            f"Target: {target_info}",
            match_info,
            f"Gesture: {gesture.gesture_type.value}" + (f" ({hold_progress*100:.0f}%)" if hold_progress > 0 else "")
        ]
        
        for i, line in enumerate(info_lines):
            if line:
                # åŒ¹é…ä¿¡æ¯ç”¨ä¸åŒé¢œè‰²
                color = (0, 255, 255) if "Match:" in line else (0, 255, 0)
                cv2.putText(frame, line, (10, 25 + i * 20),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 1)
        
        # æ‰‹åŠ¿æç¤º
        if state_machine.state == SystemState.IDLE:
            cv2.putText(frame, f"Hold OPEN PALM {GESTURE_HOLD_DURATION:.0f}s to START", 
                       (10, h - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        elif state_machine.state == SystemState.TRACKING:
            cv2.putText(frame, f"Hold OPEN PALM {GESTURE_HOLD_DURATION:.0f}s to STOP", 
                       (10, h - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        elif state_machine.state == SystemState.LOST_TARGET:
            cv2.putText(frame, f"Target LOST - Hold PALM to STOP", 
                       (10, h - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 165, 255), 2)
        
        cv2.imshow("Gesture-Controlled Following", frame)
        
        # é”®ç›˜æ§åˆ¶
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            break
        elif key == ord('s'):
            if persons:
                nearest, _ = find_nearest_person(persons, frame_center)
                if nearest:
                    view = extract_view_feature(
                        frame, nearest.bbox, faces, face_recognizer, enhanced_reid
                    )
                    mv_recognizer.set_target(view, nearest.bbox)
                    state_machine.state = SystemState.TRACKING
                    print("[æ‰‹åŠ¨ä¿å­˜] ç›®æ ‡å·²é”å®š")
        elif key == ord('a'):
            if mv_recognizer.target and target_person_idx >= 0:
                person = persons[target_person_idx]
                view = extract_view_feature(
                    frame, person.bbox, faces, face_recognizer, enhanced_reid
                )
                if mv_recognizer.target._is_different_view(view, 0.75):
                    mv_recognizer.target.view_features.append(view)
                    print(f"[æ‰‹åŠ¨æ·»åŠ ] æ–°è§†è§’, æ€»æ•°: {mv_recognizer.target.num_views}")
        elif key == ord('c'):
            mv_recognizer.clear_target()
            state_machine.state = SystemState.IDLE
            print("[æ‰‹åŠ¨æ¸…é™¤] ç›®æ ‡å·²æ¸…é™¤")
        elif key == ord('m'):
            mv_config.auto_learn = not mv_config.auto_learn
            print(f"[è‡ªåŠ¨å­¦ä¹ ] {'å¼€å¯' if mv_config.auto_learn else 'å…³é—­'}")
    
    gesture_detector.release()
    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    main()
