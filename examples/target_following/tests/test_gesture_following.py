"""
æ‰‹åŠ¿æ§åˆ¶ç›®æ ‡è·Ÿéšæµ‹è¯•
Gesture-Controlled Target Following

æ‰‹åŠ¿æ§åˆ¶:
  - ğŸ‘‹ å¼ å¼€æ‰‹æŒæŒç»­3ç§’: Toggle å¯åŠ¨/åœæ­¢è·Ÿéš
    - ç©ºé—²çŠ¶æ€ â†’ å¯åŠ¨è·Ÿéš (é”å®šæœ€è¿‘çš„äºº)
    - è·Ÿè¸ªçŠ¶æ€ â†’ åœæ­¢è·Ÿéš (æ¸…é™¤ç›®æ ‡)

ç³»ç»ŸçŠ¶æ€:
  - IDLE: ç©ºé—²çŠ¶æ€ï¼Œç­‰å¾…æ‰‹åŠ¿å¯åŠ¨
  - TRACKING: è·Ÿéšä¸­ï¼ŒæŒç»­è·Ÿè¸ªç›®æ ‡
  - LOST_TARGET: ç›®æ ‡ä¸¢å¤±ï¼Œç­‰å¾…é‡æ–°å‡ºç°æˆ–æ‰‹åŠ¿åœæ­¢

é”®ç›˜æ§åˆ¶ (å¤‡ç”¨):
  - 's': æ‰‹åŠ¨ä¿å­˜ç›®æ ‡
  - 'a': æ‰‹åŠ¨æ·»åŠ è§†è§’
  - 'c': æ‰‹åŠ¨æ¸…é™¤ç›®æ ‡
  - 'm': åˆ‡æ¢è‡ªåŠ¨å­¦ä¹ 
  - 'q': é€€å‡º
"""

import cv2
import numpy as np
import sys
import os
import time

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import (
    MODELS_DIR, GestureType, GestureConfig, SystemState,
    YOLOv5PersonConfig, FaceDetectorConfig, MobileFaceNetConfig
)
from detectors.yolov5_person_detector import YOLOv5PersonDetector
from detectors.face_detector import FaceDetector
from detectors.mobilefacenet_recognizer import MobileFaceNetRecognizer
from detectors.gesture_detector import GestureDetector, GestureResult
from detectors.enhanced_reid import EnhancedReIDExtractor, EnhancedReIDConfig
from detectors.multiview_recognizer import (
    MultiViewRecognizer, MultiViewConfig, ViewFeature
)
from core.state_machine import StateMachine


# æ‰‹åŠ¿é…ç½®
GESTURE_HOLD_DURATION = 3.0  # è§¦å‘éœ€è¦ä¿æŒçš„ç§’æ•°
GESTURE_COOLDOWN_SECONDS = 3.0  # è§¦å‘åå†·å´ç§’æ•° (é˜²æ­¢è¿ç»­è§¦å‘)

# ============================================
# äººè„¸è´¨é‡çŠ¶æ€å®šä¹‰ï¼ˆæ ¸å¿ƒçŠ¶æ€æœºï¼‰
# ============================================
# äººè„¸çŠ¶æ€åˆ†ä¸ºä¸‰çº§ï¼šç¨³å®š / ä¸ç¨³å®š / ä¸¢å¤±
# ä¸åŒçŠ¶æ€ä½¿ç”¨ä¸åŒçš„åŒ¹é…ç­–ç•¥

# äººè„¸ç¨³å®šçŠ¶æ€é˜ˆå€¼
FACE_STABLE_CONF = 0.70      # ç½®ä¿¡åº¦ >= 0.70
FACE_STABLE_SIZE = 64        # å°ºå¯¸ >= 64px
FACE_STABLE_SIM = 0.60       # ç›¸ä¼¼åº¦ >= 0.60
FACE_STABLE_FRAMES = 3       # è¿ç»­å¸§ >= 3

# äººè„¸ä¸ç¨³å®šçŠ¶æ€é˜ˆå€¼ï¼ˆä¾§è„¸/æ¨¡ç³Šï¼‰
FACE_UNSTABLE_CONF = 0.40    # ç½®ä¿¡åº¦ >= 0.40
FACE_UNSTABLE_SIZE = 48      # å°ºå¯¸ >= 48px
FACE_UNSTABLE_SIM = 0.30     # ç›¸ä¼¼åº¦ >= 0.30
FACE_UNSTABLE_FRAMES = 2     # è¿ç»­å¸§ >= 2

# äººè„¸ä¸¢å¤±é˜ˆå€¼
FACE_LOST_CONF = 0.40        # ç½®ä¿¡åº¦ < 0.40
FACE_LOST_SIZE = 48          # å°ºå¯¸ < 48px
FACE_LOST_FRAMES = 3         # è¿ç»­ä¸¢å¤±å¸§ >= 3

# ä»…äººè„¸åŒ¹é…é˜ˆå€¼ï¼ˆæ— äººä½“æ—¶çš„å¤‡ç”¨ï¼‰
FACE_ONLY_THRESHOLD = 0.70           # ç¨³å®šäººè„¸
FACE_ONLY_THRESHOLD_UNSTABLE = 0.50  # ä¸ç¨³å®šäººè„¸ + motionè¾…åŠ©

# è‡ªåŠ¨å­¦ä¹ é˜ˆå€¼
FACE_LEARN_THRESHOLD = 0.72  # äººè„¸åŒ¹é…å­¦ä¹ é˜ˆå€¼
FACE_LEARN_THRESHOLD_MULTI = 0.78  # å¤šäººåœºæ™¯ä¸‹çš„äººè„¸å­¦ä¹ é˜ˆå€¼
BODY_LEARN_THRESHOLD = 0.68  # äººä½“åŒ¹é…å­¦ä¹ é˜ˆå€¼

# é‡æ–°é”å®šé˜ˆå€¼
RELOCK_FACE_THRESHOLD = 0.70
RELOCK_CONFIRM_FRAMES = 2
AUTO_LEARN_CONFIRM_FRAMES = 1

# è§†è§’åº“æœ€å¤§å®¹é‡ï¼ˆæœ‰è„¸3-4 + æ— è„¸2 = ä¾§èº«+èƒŒé¢ï¼‰
MAX_VIEW_COUNT = 6

# äººè„¸æœ‰æ•ˆå°ºå¯¸ï¼ˆåŒ¹é…ç”¨ï¼‰
MIN_FACE_SIZE = 40
MIN_FACE_SIZE_FOR_LEARN = 50

# ============================================
# å¤šå¸§æŠ•ç¥¨æœºåˆ¶
# ============================================
LOST_CONFIRM_FRAMES = 5
MATCH_HISTORY_SIZE = 5
MOTION_WEIGHT_MULTI_PERSON = 0.6
MOTION_WEIGHT_SINGLE_PERSON = 0.5

# ä¾§è„¸å®¹å¿åº¦
MOTION_TRUST_THRESHOLD = 0.95
FACE_SIDE_VIEW_MIN = 0.35


# ============================================
# äººè„¸è´¨é‡è¯„ä¼°å‡½æ•°
# ============================================
def evaluate_face_quality(face_conf: float, face_size: int, face_sim: float) -> str:
    """
    è¯„ä¼°äººè„¸è´¨é‡ï¼Œè¿”å›çŠ¶æ€: 'stable', 'unstable', 'lost'
    
    stable: é«˜ç½®ä¿¡åº¦+å¤§å°ºå¯¸ï¼Œæˆ– è¶…å¤§å°ºå¯¸å¯å¼¥è¡¥ä½ç½®ä¿¡åº¦ï¼Œæˆ– é«˜ç›¸ä¼¼åº¦å¯å¼¥è¡¥
    unstable: ä¸­ç­‰è´¨é‡ â†’ motionè¾…åŠ©åˆ¤æ–­
    lost: ä½è´¨é‡æˆ–æ— äººè„¸ â†’ åˆ‡æ¢åˆ°äººä½“+motion
    
    å…³é”®æ”¹è¿›ï¼š
    1. å¤§å°ºå¯¸äººè„¸ï¼ˆ>=100pxï¼‰å³ä½¿ç½®ä¿¡åº¦è¾ƒä½ä¹Ÿåº”è§†ä¸ºstable
    2. é«˜ç›¸ä¼¼åº¦ï¼ˆ>=0.60ï¼‰å¯ä»¥å¼¥è¡¥å°å°ºå¯¸/ä½ç½®ä¿¡åº¦ï¼ˆè¯´æ˜embeddingè´¨é‡å¥½ï¼‰
    """
    if face_conf is None or face_size is None:
        return 'lost'
    
    # å…³é”®æ”¹è¿›ï¼šé«˜ç›¸ä¼¼åº¦è¯´æ˜ embedding è´¨é‡å¥½ï¼Œå¯ä»¥æå‡è¯„çº§
    # å³ä½¿äººè„¸å°/æ£€æµ‹ç½®ä¿¡åº¦ä½ï¼Œé«˜ç›¸ä¼¼åº¦ä¹Ÿè¯´æ˜æ˜¯åŒä¸€ä¸ªäºº
    HIGH_SIM_THRESHOLD = 0.60
    MEDIUM_SIM_THRESHOLD = 0.45
    
    if face_sim is not None and face_sim >= HIGH_SIM_THRESHOLD:
        # é«˜ç›¸ä¼¼åº¦ï¼šåªè¦å°ºå¯¸ä¸æ˜¯å¤ªå°ï¼ˆ>=20pxï¼‰å°±ç®— stable
        if face_size >= 20:
            return 'stable'
    
    if face_sim is not None and face_sim >= MEDIUM_SIM_THRESHOLD:
        # ä¸­ç­‰ç›¸ä¼¼åº¦ï¼šåªè¦å°ºå¯¸ä¸æ˜¯å¤ªå°ï¼ˆ>=20pxï¼‰å°±ç®— unstable
        if face_size >= 20:
            return 'unstable'
    
    # å¤§å°ºå¯¸äººè„¸å¯ä»¥å¼¥è¡¥ä½ç½®ä¿¡åº¦
    # size >= 100px æ—¶ï¼Œåªè¦ conf >= 0.50 å°±ç®— stable
    LARGE_FACE_SIZE = 100
    LARGE_FACE_MIN_CONF = 0.50
    
    if face_size >= LARGE_FACE_SIZE and face_conf >= LARGE_FACE_MIN_CONF:
        # å¤§å°ºå¯¸äººè„¸ï¼šåªè¦ç›¸ä¼¼åº¦ä¸å¤ªä½å°±ç®—stable
        if face_sim is None or face_sim >= FACE_UNSTABLE_SIM:
            return 'stable'
    
    # æ­£å¸¸åˆ¤æ–­
    if (face_conf >= FACE_STABLE_CONF and 
        face_size >= FACE_STABLE_SIZE and 
        (face_sim is None or face_sim >= FACE_STABLE_SIM)):
        return 'stable'
    elif (face_conf >= FACE_UNSTABLE_CONF and 
          face_size >= FACE_UNSTABLE_SIZE and
          (face_sim is None or face_sim >= FACE_UNSTABLE_SIM)):
        return 'unstable'
    else:
        return 'lost'


def extract_view_feature(
    frame: np.ndarray,
    person_bbox: np.ndarray,
    faces: list,
    face_recognizer,
    enhanced_reid
) -> ViewFeature:
    """æå–è§†è§’ç‰¹å¾"""
    view = ViewFeature(timestamp=time.time())
    
    px1, py1, px2, py2 = person_bbox.astype(int)
    
    # æŸ¥æ‰¾äººè„¸
    for face in faces:
        fx1, fy1, fx2, fy2 = face.bbox.astype(int)
        fc_x, fc_y = (fx1 + fx2) // 2, (fy1 + fy2) // 2
        
        if px1 <= fc_x <= px2 and py1 <= fc_y <= py2:
            face_feature = face_recognizer.extract_feature(
                frame, face.bbox, face.keypoints
            )
            if face_feature:
                view.has_face = True
                view.face_embedding = face_feature.embedding
            break
    
    # äººä½“ç‰¹å¾
    body_feature = enhanced_reid.extract_feature(frame, person_bbox)
    if body_feature:
        view.part_color_hists = body_feature.part_color_hists
        view.part_lbp_hists = body_feature.part_lbp_hists
        view.geometry = body_feature.geometry
    
    return view


def find_nearest_person(persons: list, frame_center: tuple):
    """æ‰¾åˆ°ç¦»ç”»é¢ä¸­å¿ƒæœ€è¿‘çš„äºº"""
    if not persons:
        return None, -1
    
    min_dist = float('inf')
    nearest_idx = 0
    
    for i, person in enumerate(persons):
        px1, py1, px2, py2 = person.bbox
        cx = (px1 + px2) / 2
        cy = (py1 + py2) / 2
        dist = (cx - frame_center[0])**2 + (cy - frame_center[1])**2
        if dist < min_dist:
            min_dist = dist
            nearest_idx = i
    
    return persons[nearest_idx], nearest_idx


def find_person_with_gesture(persons: list, hand_bbox: np.ndarray):
    """æ‰¾åˆ°åšæ‰‹åŠ¿çš„é‚£ä¸ªäººï¼ˆä¼˜å…ˆæ‰‹åŠ¿åœ¨äººä½“æ¡†å†…ï¼Œå…¶æ¬¡æ‰¾æœ€è¿‘çš„äººä½“ï¼‰"""
    if not persons or hand_bbox is None:
        return None, -1
    
    hx1, hy1, hx2, hy2 = hand_bbox
    hand_center = ((hx1 + hx2) / 2, (hy1 + hy2) / 2)
    
    best_person = None
    best_idx = -1
    best_overlap = 0.0
    
    # ç­–ç•¥1ï¼šä¼˜å…ˆæ‰¾æ‰‹åŠ¿ä¸­å¿ƒåœ¨äººä½“æ¡†å†…çš„
    for i, person in enumerate(persons):
        px1, py1, px2, py2 = person.bbox
        
        # æ£€æŸ¥æ‰‹åŠ¿ä¸­å¿ƒæ˜¯å¦åœ¨äººä½“æ¡†å†…
        if px1 <= hand_center[0] <= px2 and py1 <= hand_center[1] <= py2:
            # è®¡ç®—é‡å ç¨‹åº¦ï¼ˆæ‰‹åŠ¿æ¡†ä¸äººä½“æ¡†çš„IoUï¼‰
            inter_x1 = max(hx1, px1)
            inter_y1 = max(hy1, py1)
            inter_x2 = min(hx2, px2)
            inter_y2 = min(hy2, py2)
            
            if inter_x2 > inter_x1 and inter_y2 > inter_y1:
                inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
                hand_area = (hx2 - hx1) * (hy2 - hy1)
                overlap = inter_area / hand_area if hand_area > 0 else 0
                
                if overlap > best_overlap:
                    best_overlap = overlap
                    best_person = person
                    best_idx = i
    
    # ç­–ç•¥2ï¼šå¦‚æœæ²¡æ‰¾åˆ°å®Œå…¨åŒ…å«çš„ï¼Œæ‰¾æ‰‹åŠ¿æ¡†ä¸äººä½“æ¡†è¾¹ç¼˜æœ€è¿‘çš„
    # è¿™å¤„ç†æ‰‹ä¼¸å‡ºèº«ä½“åšæ‰‹åŠ¿çš„æƒ…å†µ
    if best_person is None:
        min_edge_dist = float('inf')
        for i, person in enumerate(persons):
            px1, py1, px2, py2 = person.bbox
            
            # è®¡ç®—æ‰‹åŠ¿ä¸­å¿ƒåˆ°äººä½“æ¡†è¾¹ç¼˜çš„æœ€çŸ­è·ç¦»
            # å¦‚æœæ‰‹åŠ¿åœ¨æ¡†å†…ï¼Œè·ç¦»ä¸º0
            dx = max(px1 - hand_center[0], 0, hand_center[0] - px2)
            dy = max(py1 - hand_center[1], 0, hand_center[1] - py2)
            edge_dist = (dx**2 + dy**2) ** 0.5
            
            # é¢å¤–æ£€æŸ¥ï¼šæ‰‹åŠ¿åº”è¯¥åœ¨äººä½“çš„åˆç†å»¶ä¼¸èŒƒå›´å†…ï¼ˆå®½åº¦çš„50%ï¼‰
            person_width = px2 - px1
            max_extend = person_width * 0.5
            
            if edge_dist < min_edge_dist and edge_dist < max_extend:
                min_edge_dist = edge_dist
                best_person = person
                best_idx = i
    
    return best_person, best_idx


def draw_gesture_indicator(frame, gesture: GestureResult, state: SystemState, hold_progress: float = 0.0):
    """ç»˜åˆ¶æ‰‹åŠ¿æŒ‡ç¤ºå™¨"""
    h, w = frame.shape[:2]
    
    # ç»˜åˆ¶æ‰‹éƒ¨æ¡†
    if gesture.hand_bbox is not None:
        hx1, hy1, hx2, hy2 = gesture.hand_bbox.astype(int)
        
        if gesture.gesture_type == GestureType.OPEN_PALM:
            color = (0, 255, 255)  # é»„è‰²
            if state == SystemState.IDLE:
                text = "HOLD TO START"
            else:
                text = "HOLD TO STOP"
        else:
            color = (255, 165, 0)
            text = ""
        
        cv2.rectangle(frame, (hx1, hy1), (hx2, hy2), color, 2)
        if text:
            cv2.putText(frame, text, (hx1, hy1 - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        
        # ç»˜åˆ¶æŒç»­è¿›åº¦æ¡
        if hold_progress > 0:
            bar_width = hx2 - hx1
            bar_height = 8
            bar_y = hy2 + 5
            
            # èƒŒæ™¯
            cv2.rectangle(frame, (hx1, bar_y), (hx2, bar_y + bar_height), (50, 50, 50), -1)
            # è¿›åº¦
            progress_width = int(bar_width * hold_progress)
            progress_color = (0, 255, 0) if hold_progress < 1.0 else (0, 255, 255)
            cv2.rectangle(frame, (hx1, bar_y), (hx1 + progress_width, bar_y + bar_height), progress_color, -1)
            # è¾¹æ¡†
            cv2.rectangle(frame, (hx1, bar_y), (hx2, bar_y + bar_height), (255, 255, 255), 1)
            
            # è¿›åº¦ç™¾åˆ†æ¯”
            pct_text = f"{int(hold_progress * 100)}%"
            cv2.putText(frame, pct_text, (hx1, bar_y + bar_height + 15),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
    
    # çŠ¶æ€æŒ‡ç¤ºå™¨ (å³ä¸Šè§’)
    state_colors = {
        SystemState.IDLE: (128, 128, 128),          # ç°è‰²
        SystemState.TRACKING: (0, 255, 0),          # ç»¿è‰²
        SystemState.LOST_TARGET: (0, 165, 255)      # æ©™è‰²
    }
    state_color = state_colors.get(state, (255, 255, 255))
    
    cv2.circle(frame, (w - 30, 30), 15, state_color, -1)
    cv2.putText(frame, state.value, (w - 120, 55),
               cv2.FONT_HERSHEY_SIMPLEX, 0.5, state_color, 1)


def main():
    print("=" * 60)
    print("    æ‰‹åŠ¿æ§åˆ¶ç›®æ ‡è·Ÿéšç³»ç»Ÿ")
    print("=" * 60)
    
    # æ£€æŸ¥æ¨¡å‹
    yolo_path = os.path.join(MODELS_DIR, "yolov5n.onnx")
    scrfd_path = os.path.join(MODELS_DIR, "scrfd_500m_bnkps.onnx")
    mobilefacenet_path = os.path.join(MODELS_DIR, "mobilefacenet.onnx")
    
    missing = []
    if not os.path.exists(yolo_path):
        missing.append("yolov5n.onnx")
    if not os.path.exists(scrfd_path):
        missing.append("scrfd_500m_bnkps.onnx")
    if not os.path.exists(mobilefacenet_path):
        missing.append("mobilefacenet.onnx")
    
    if missing:
        print(f"\n[é”™è¯¯] ç¼ºå°‘æ¨¡å‹: {missing}")
        return
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    person_detector = YOLOv5PersonDetector(YOLOv5PersonConfig(model_path=yolo_path))
    face_detector = FaceDetector(FaceDetectorConfig(model_path=scrfd_path))
    face_recognizer = MobileFaceNetRecognizer(MobileFaceNetConfig(model_path=mobilefacenet_path))
    
    # æ‰‹åŠ¿æ£€æµ‹å™¨ (confirm_frames=1ï¼Œå› ä¸ºæŒç»­æ—¶é—´æ£€æµ‹åœ¨çŠ¶æ€æœºä¸­)
    gesture_config = GestureConfig(
        min_detection_confidence=0.7,
        min_tracking_confidence=0.5,
        gesture_confirm_frames=1  # ç«‹å³å“åº”ï¼ŒæŒç»­æ—¶é—´ç”±çŠ¶æ€æœºæ§åˆ¶
    )
    gesture_detector = GestureDetector(gesture_config)
    
    # å¢å¼ºç‰ˆ ReID
    enhanced_reid = EnhancedReIDExtractor(EnhancedReIDConfig(
        num_horizontal_parts=6,
        use_lbp=True,
        use_geometry=True
    ))
    
    # å¤šè§†è§’è¯†åˆ«å™¨
    mv_config = MultiViewConfig(
        face_weight=0.6,
        body_weight=0.4,
        face_threshold=0.60,      # äººè„¸é˜ˆå€¼ï¼ˆé€‚åº¦é™ä½ä»¥å®¹å¿ä¾§è„¸ï¼‰
        body_threshold=0.58,      # äººä½“é˜ˆå€¼ï¼ˆé€‚åº¦é™ä½ä»¥æé«˜è¿ç»­æ€§ï¼‰
        fused_threshold=0.52,     # èåˆé˜ˆå€¼ï¼ˆé€‚åº¦é™ä½ï¼‰
        motion_weight=0.15,       # æé«˜è¿åŠ¨æƒé‡ï¼ˆä¾§è„¸æ—¶ä¾èµ–è¿åŠ¨è¿ç»­æ€§ï¼‰
        auto_learn=True,
        learn_interval=3.0,       # å­¦ä¹ é—´éš”
        smooth_window=5,
        confirm_threshold=3,
        part_weights=[0.05, 0.12, 0.20, 0.20, 0.25, 0.18],
        max_views=MAX_VIEW_COUNT  # é™åˆ¶è§†è§’æ•°é‡
    )
    mv_recognizer = MultiViewRecognizer(mv_config)
    
    # åŠ è½½æ¨¡å‹
    if not person_detector.load():
        print("[é”™è¯¯] äººä½“æ£€æµ‹å™¨åŠ è½½å¤±è´¥")
        return
    if not face_detector.load():
        print("[é”™è¯¯] äººè„¸æ£€æµ‹å™¨åŠ è½½å¤±è´¥")
        return
    if not face_recognizer.load():
        print("[é”™è¯¯] äººè„¸è¯†åˆ«å™¨åŠ è½½å¤±è´¥")
        return
    if not gesture_detector.load():
        print("[é”™è¯¯] æ‰‹åŠ¿æ£€æµ‹å™¨åŠ è½½å¤±è´¥")
        return
    
    enhanced_reid.load()
    
    # æ‰“å¼€æ‘„åƒå¤´
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("[é”™è¯¯] æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
        return
    
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    # åˆ›å»ºå¯è°ƒæ•´å¤§å°çš„çª—å£
    window_name = "Gesture-Controlled Following"
    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
    cv2.resizeWindow(window_name, 960, 720)  # é»˜è®¤çª—å£å¤§å°
    
    print("\n[æ‰‹åŠ¿æ§åˆ¶]")
    print(f"  ğŸ‘‹ å¼ å¼€æ‰‹æŒæŒç»­ {GESTURE_HOLD_DURATION:.0f} ç§’: Toggle å¯åŠ¨/åœæ­¢è·Ÿéš")
    print("\n[é”®ç›˜æ§åˆ¶]")
    print("  's': æ‰‹åŠ¨ä¿å­˜ç›®æ ‡")
    print("  'a': æ·»åŠ è§†è§’")
    print("  'c': æ¸…é™¤ç›®æ ‡")
    print("  'm': åˆ‡æ¢è‡ªåŠ¨å­¦ä¹ ")
    print("  'q': é€€å‡º")
    print()
    
    # çŠ¶æ€æœº (ä½¿ç”¨ä¹‹å‰å®ç°çš„æŒç»­æ—¶é—´æ£€æµ‹)
    state_machine = StateMachine(
        lost_timeout_frames=30,
        gesture_hold_duration=GESTURE_HOLD_DURATION,
        gesture_cooldown_seconds=GESTURE_COOLDOWN_SECONDS
    )
    
    lost_frames = 0
    max_lost_frames = 30
    
    # è¿ç»­å¸§ç¡®è®¤è®¡æ•°å™¨
    relock_confirm_count = 0  # é‡æ–°é”å®šè¿ç»­åŒ¹é…å¸§æ•°
    relock_candidate_idx = -1  # å½“å‰é‡æ–°é”å®šå€™é€‰äººç´¢å¼•
    auto_learn_confirm_count = 0  # è‡ªåŠ¨å­¦ä¹ è¿ç»­åŒ¹é…å¸§æ•°
    auto_learn_candidate_view = None  # å¾…å­¦ä¹ çš„è§†è§’
    
    frame_count = 0
    fps_start = time.time()
    fps = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame_count += 1
        h, w = frame.shape[:2]
        frame_center = (w // 2, h // 2)
        
        # è®¡ç®— FPS
        if frame_count % 30 == 0:
            fps = 30 / (time.time() - fps_start)
            fps_start = time.time()
        
        # æ£€æµ‹
        persons = person_detector.detect(frame)
        faces = face_detector.detect(frame)
        gesture = gesture_detector.detect(frame)
        
        # ============== æ‰‹åŠ¿æœ‰æ•ˆæ€§è¿‡æ»¤ ==============
        # 1. æ‰‹åŠ¿å¿…é¡»è¶³å¤Ÿå¤§ï¼ˆé¿å…è¯¯è¯†åˆ«è¿œå¤„çš„å°æ‰‹åŠ¿ï¼‰
        # 2. ä¼˜å…ˆè¯†åˆ«å±å¹•ä¸­å¤®åŒºåŸŸçš„æ‰‹åŠ¿
        MIN_HAND_SIZE_FOR_GESTURE = 30  # æ‰‹åŠ¿æœ€å°åƒç´ å°ºå¯¸ï¼ˆé™ä½åˆ°30pxï¼‰
        CENTER_REGION_RATIO = 0.85  # ä¸­å¤®åŒºåŸŸå æ¯”ï¼ˆæ‰©å¤§åˆ°85%ï¼‰
        
        gesture_valid = False
        gesture_reject_reason = None
        
        if gesture.hand_bbox is not None and gesture.gesture_type in (GestureType.OPEN_PALM, GestureType.CLOSED_FIST):
            hx1, hy1, hx2, hy2 = gesture.hand_bbox
            hand_w = hx2 - hx1
            hand_h = hy2 - hy1
            hand_size = min(hand_w, hand_h)
            hand_center = ((hx1 + hx2) / 2, (hy1 + hy2) / 2)
            
            # æ£€æŸ¥1ï¼šæ‰‹åŠ¿å°ºå¯¸
            if hand_size < MIN_HAND_SIZE_FOR_GESTURE:
                gesture_reject_reason = f"æ‰‹åŠ¿å¤ªå°({hand_size:.0f}px<{MIN_HAND_SIZE_FOR_GESTURE}px)"
            else:
                # æ£€æŸ¥2ï¼šæ‰‹åŠ¿æ˜¯å¦åœ¨ä¸­å¤®åŒºåŸŸ
                center_x_min = w * (1 - CENTER_REGION_RATIO) / 2
                center_x_max = w * (1 + CENTER_REGION_RATIO) / 2
                center_y_min = h * (1 - CENTER_REGION_RATIO) / 2
                center_y_max = h * (1 + CENTER_REGION_RATIO) / 2
                
                in_center = (center_x_min <= hand_center[0] <= center_x_max and 
                            center_y_min <= hand_center[1] <= center_y_max)
                
                if in_center:
                    gesture_valid = True
                else:
                    gesture_reject_reason = f"æ‰‹åŠ¿ä¸åœ¨ä¸­å¤®åŒºåŸŸ"
        
        # å¦‚æœæ‰‹åŠ¿æ— æ•ˆï¼Œé‡ç½®ä¸º none
        if not gesture_valid and gesture.gesture_type in (GestureType.OPEN_PALM, GestureType.CLOSED_FIST):
            if gesture_reject_reason and frame_count % 30 == 0:
                print(f"[DEBUG] æ‰‹åŠ¿è¿‡æ»¤: {gesture_reject_reason}")
            # åˆ›å»ºä¸€ä¸ªæ— æ•ˆæ‰‹åŠ¿ç»“æœ
            gesture = GestureResult(gesture_type=GestureType.NONE, confidence=0.0, hand_bbox=None)
        
        # è°ƒè¯•æ—¥å¿— (æ¯30å¸§è¾“å‡ºä¸€æ¬¡)
        if frame_count % 30 == 0:
            print(f"[DEBUG] Frame {frame_count}: persons={len(persons)}, faces={len(faces)}, gesture={gesture.gesture_type.value}")
            if faces:
                for i, face in enumerate(faces):
                    fx1, fy1, fx2, fy2 = face.bbox.astype(int)
                    face_size = min(fx2-fx1, fy2-fy1)
                    print(f"        Face[{i}]: bbox={[fx1,fy1,fx2,fy2]}, conf={face.confidence:.2f}, size={face_size}px")
            if persons:
                for i, person in enumerate(persons):
                    print(f"        Person[{i}]: bbox={person.bbox.astype(int).tolist()}, conf={person.confidence:.2f}")
        
        # ============== æ‰‹åŠ¿çŠ¶æ€æœº (æŒç»­æ—¶é—´æ£€æµ‹) ==============
        current_time = time.time()
        old_state = state_machine.state
        
        # å¤„ç†æ‰‹åŠ¿ (éœ€è¦æŒç»­ GESTURE_HOLD_DURATION ç§’)
        state_changed = state_machine.process_gesture(gesture.gesture_type, current_time, debug=False)
        
        # è·å–æŒç»­è¿›åº¦
        hold_progress = state_machine.get_gesture_hold_progress()
        
        # çŠ¶æ€æœºè°ƒè¯•æ—¥å¿—
        if hold_progress > 0 and frame_count % 10 == 0:
            print(f"[STATE] gesture={gesture.gesture_type.value}, hold={hold_progress*100:.0f}%, state={state_machine.state.value}")
        
        # çŠ¶æ€å˜æ›´å¤„ç†
        if state_changed:
            if state_machine.state == SystemState.TRACKING and old_state == SystemState.IDLE:
                # ============================================
                # å¯åŠ¨è·Ÿéš - æ–¹æ¡ˆDï¼šå¿…é¡»æœ‰äººè„¸ï¼Œå¯ä»¥æ²¡äººä½“
                # ============================================
                # ä¼˜å…ˆçº§1: æœ‰äººä½“ + æ‰‹åŠ¿åœ¨æ¡†å†… + æ¡†å†…æœ‰äººè„¸ â†’ é”å®š
                # ä¼˜å…ˆçº§2: æ— äººä½“ + æœ‰äººè„¸(è´¨é‡å¤Ÿ) â†’ é”å®šï¼ˆç›´æ’­åœºæ™¯ï¼‰
                # å…¶ä»–æƒ…å†µ â†’ æ‹’ç»å¯åŠ¨
                # ============================================
                
                MIN_FACE_CONF_FOR_START = 0.65  # å¯åŠ¨æ—¶äººè„¸æœ€ä½ç½®ä¿¡åº¦
                MIN_FACE_SIZE_FOR_START = 50    # å¯åŠ¨æ—¶äººè„¸æœ€å°å°ºå¯¸ï¼ˆæ ‡å‡†ï¼‰
                MIN_FACE_SIZE_FOR_START_RELAXED = 30  # é«˜ç½®ä¿¡åº¦æ—¶å¯æ”¾å®½åˆ°30px
                HIGH_CONF_FOR_RELAXED_SIZE = 0.75     # ç½®ä¿¡åº¦>=0.75æ—¶æ”¾å®½å°ºå¯¸è¦æ±‚
                
                target_locked = False
                
                # ========== åœºæ™¯1: æœ‰äººä½“æ£€æµ‹ ==========
                if persons:
                    target_person = None
                    target_idx = -1
                    face_in_target = None
                    
                    # 1. æ‰¾æ‰‹åŠ¿æ‰€åœ¨çš„äººä½“
                    if gesture.hand_bbox is not None:
                        print(f"[DEBUG] æ‰‹åŠ¿æ¡†: {gesture.hand_bbox.astype(int).tolist()}")
                        for pi, p in enumerate(persons):
                            px1, py1, px2, py2 = p.bbox.astype(int)
                            hc = ((gesture.hand_bbox[0] + gesture.hand_bbox[2]) / 2,
                                  (gesture.hand_bbox[1] + gesture.hand_bbox[3]) / 2)
                            in_box = px1 <= hc[0] <= px2 and py1 <= hc[1] <= py2
                            print(f"[DEBUG] Person[{pi}] bbox: [{px1}, {py1}, {px2}, {py2}], æ‰‹åŠ¿åœ¨æ¡†å†…: {in_box}")
                            
                            if in_box:
                                target_person = p
                                target_idx = pi
                                break
                    
                    if target_person is None:
                        # æ‰‹åŠ¿ä¸åœ¨ä»»ä½•äººä½“æ¡†å†…
                        state_machine.state = SystemState.IDLE
                        print("[æç¤º] æ‰‹åŠ¿æœªè½åœ¨ä»»ä½•äººä½“æ¡†å†…ï¼Œè¯·å°†æ‰‹æ”¾åœ¨èº«ä½“å‰æ–¹å†åšæ‰‹åŠ¿")
                    else:
                        # 2. æ£€æŸ¥è¯¥äººä½“æ¡†å†…æ˜¯å¦æœ‰äººè„¸
                        px1, py1, px2, py2 = target_person.bbox.astype(int)
                        best_face_in_person = None
                        best_face_info = None
                        
                        for face in faces:
                            fx1, fy1, fx2, fy2 = face.bbox.astype(int)
                            fc_x, fc_y = (fx1 + fx2) // 2, (fy1 + fy2) // 2
                            face_w, face_h = fx2 - fx1, fy2 - fy1
                            face_size = min(face_w, face_h)
                            
                            # æ£€æŸ¥äººè„¸æ˜¯å¦åœ¨äººä½“æ¡†å†…
                            if not (px1 <= fc_x <= px2 and py1 <= fc_y <= py2):
                                continue
                            
                            # è®°å½•è¿™ä¸ªäººè„¸çš„ä¿¡æ¯ç”¨äºè°ƒè¯•
                            best_face_info = {
                                'conf': face.confidence,
                                'size': face_size,
                                'bbox': [fx1, fy1, fx2, fy2]
                            }
                            
                            # äººè„¸ä¸­å¿ƒåœ¨äººä½“æ¡†å†… + è´¨é‡è¾¾æ ‡
                            # é«˜ç½®ä¿¡åº¦(>=0.80)å¯æ¥å—æ›´å°å°ºå¯¸(30px)
                            size_ok = face_size >= MIN_FACE_SIZE_FOR_START
                            size_ok_relaxed = (face.confidence >= HIGH_CONF_FOR_RELAXED_SIZE and 
                                               face_size >= MIN_FACE_SIZE_FOR_START_RELAXED)
                            
                            if (face.confidence >= MIN_FACE_CONF_FOR_START and
                                (size_ok or size_ok_relaxed)):
                                face_in_target = face
                                break
                        
                        if face_in_target is None:
                            # äººä½“æ¡†å†…æ²¡æœ‰åˆæ ¼çš„äººè„¸ - æ‰“å°è¯¦ç»†è°ƒè¯•ä¿¡æ¯
                            state_machine.state = SystemState.IDLE
                            if best_face_info:
                                conf = best_face_info['conf']
                                size = best_face_info['size']
                                # è®¡ç®—å·®è·
                                conf_gap = MIN_FACE_CONF_FOR_START - conf if conf < MIN_FACE_CONF_FOR_START else 0
                                size_gap = MIN_FACE_SIZE_FOR_START - size if size < MIN_FACE_SIZE_FOR_START else 0
                                size_gap_relaxed = MIN_FACE_SIZE_FOR_START_RELAXED - size if size < MIN_FACE_SIZE_FOR_START_RELAXED else 0
                                
                                print(f"[å¯åŠ¨æ£€æµ‹] å½“å‰äººè„¸: conf={conf:.2f}, size={size}px")
                                print(f"           æ ‡å‡†æ¡ä»¶: conf>={MIN_FACE_CONF_FOR_START} ({'+' if conf>=MIN_FACE_CONF_FOR_START else 'âœ—'}) + size>={MIN_FACE_SIZE_FOR_START}px ({'+' if size>=MIN_FACE_SIZE_FOR_START else 'âœ—'})")
                                print(f"           æ”¾å®½æ¡ä»¶: conf>={HIGH_CONF_FOR_RELAXED_SIZE} ({'+' if conf>=HIGH_CONF_FOR_RELAXED_SIZE else 'âœ—'}) + size>={MIN_FACE_SIZE_FOR_START_RELAXED}px ({'+' if size>=MIN_FACE_SIZE_FOR_START_RELAXED else 'âœ—'})")
                                
                                # ç»™å‡ºå…·ä½“å»ºè®®
                                if conf < MIN_FACE_CONF_FOR_START:
                                    print(f"           ğŸ’¡ å»ºè®®: æ­£é¢æœå‘é•œå¤´ (confå·®{conf_gap:.2f})")
                                elif size < MIN_FACE_SIZE_FOR_START and conf < HIGH_CONF_FOR_RELAXED_SIZE:
                                    print(f"           ğŸ’¡ å»ºè®®: é è¿‘é•œå¤´ (sizeå·®{size_gap}px) æˆ–æ­£é¢æœå‘ (confå·®{HIGH_CONF_FOR_RELAXED_SIZE-conf:.2f})")
                                elif size < MIN_FACE_SIZE_FOR_START_RELAXED:
                                    print(f"           ğŸ’¡ å»ºè®®: é è¿‘é•œå¤´ (sizeå·®{size_gap_relaxed}px)")
                            else:
                                print(f"[å¯åŠ¨æ£€æµ‹] äººä½“æ¡†å†…æœªæ£€æµ‹åˆ°äººè„¸ï¼Œè¯·é¢å¯¹é•œå¤´")
                        else:
                            # 3. é”å®šç›®æ ‡ï¼ˆäººä½“+äººè„¸ï¼‰
                            print(f"[DEBUG] é”å®š Person[{target_idx}]: bbox={target_person.bbox.astype(int).tolist()}")
                            view = extract_view_feature(
                                frame, target_person.bbox, faces, 
                                face_recognizer, enhanced_reid
                            )
                            print(f"[DEBUG] æå–ç‰¹å¾: has_face={view.has_face}, has_body={view.part_color_hists is not None}")
                            if view.has_face and view.face_embedding is not None:
                                print(f"[DEBUG] äººè„¸embedding: shape={view.face_embedding.shape}, norm={np.linalg.norm(view.face_embedding):.3f}")
                                mv_recognizer.set_target(view, target_person.bbox)
                                mv_recognizer.clear_match_history()
                                lost_frames = 0
                                target_locked = True
                                print(f"[æ‰‹åŠ¿å¯åŠ¨] ç›®æ ‡å·²é”å®š (äººä½“+äººè„¸)")
                            else:
                                state_machine.state = SystemState.IDLE
                                print("[æç¤º] äººè„¸ç‰¹å¾æå–å¤±è´¥ï¼Œè¯·é‡è¯•")
                
                # ========== åœºæ™¯2: æ— äººä½“ï¼Œä»…äººè„¸ï¼ˆç›´æ’­åœºæ™¯ï¼‰==========
                elif faces:
                    # æ‰¾æœ€ä½³äººè„¸
                    best_face = None
                    best_face_score = -1
                    
                    for face in faces:
                        fx1, fy1, fx2, fy2 = face.bbox.astype(int)
                        face_w, face_h = fx2 - fx1, fy2 - fy1
                        face_size = min(face_w, face_h)
                        face_conf = face.confidence
                        
                        # é«˜ç½®ä¿¡åº¦(>=0.80)å¯æ¥å—æ›´å°å°ºå¯¸(30px)
                        size_ok = face_size >= MIN_FACE_SIZE_FOR_START
                        size_ok_relaxed = (face_conf >= HIGH_CONF_FOR_RELAXED_SIZE and 
                                           face_size >= MIN_FACE_SIZE_FOR_START_RELAXED)
                        
                        if face_conf >= MIN_FACE_CONF_FOR_START and (size_ok or size_ok_relaxed):
                            score = face_conf + face_size / 200.0
                            if score > best_face_score:
                                best_face_score = score
                                best_face = face
                    
                    if best_face is not None:
                        fx1, fy1, fx2, fy2 = best_face.bbox.astype(int)
                        face_w, face_h = fx2 - fx1, fy2 - fy1
                        face_size = min(face_w, face_h)
                        print(f"[DEBUG] ä»…äººè„¸æ¨¡å¼: bbox={best_face.bbox.astype(int).tolist()}, conf={best_face.confidence:.2f}, size={face_size}px")
                        
                        # ç”¨äººè„¸æ¡†æ‰©å±•ä¸ºä¼ªäººä½“æ¡†
                        pseudo_bbox = np.array([
                            max(0, fx1 - face_w * 0.5),
                            fy1,
                            min(w, fx2 + face_w * 0.5),
                            min(h, fy2 + face_h * 5)
                        ])
                        print(f"[DEBUG] ä¼ªäººä½“æ¡†: {pseudo_bbox.astype(int).tolist()}")
                        
                        # æå–äººè„¸ç‰¹å¾
                        view = ViewFeature(timestamp=time.time())
                        view.has_face = True
                        face_feature = face_recognizer.extract_feature(
                            frame, best_face.bbox, best_face.keypoints
                        )
                        if face_feature and face_feature.embedding is not None:
                            view.face_embedding = face_feature.embedding
                            print(f"[DEBUG] äººè„¸ç‰¹å¾: shape={face_feature.embedding.shape}, norm={np.linalg.norm(face_feature.embedding):.3f}")
                            
                            mv_recognizer.set_target(view, pseudo_bbox)
                            mv_recognizer.clear_match_history()
                            lost_frames = 0
                            target_locked = True
                            print(f"[æ‰‹åŠ¿å¯åŠ¨] ç›®æ ‡å·²é”å®š (ä»…äººè„¸æ¨¡å¼ï¼Œç­‰å¾…äººä½“è¡¥å……)")
                        else:
                            state_machine.state = SystemState.IDLE
                            print("[æç¤º] äººè„¸ç‰¹å¾æå–å¤±è´¥ï¼Œè¯·é‡è¯•")
                    else:
                        state_machine.state = SystemState.IDLE
                        print(f"[æç¤º] äººè„¸è´¨é‡ä¸è¶³ (éœ€è¦conf>={MIN_FACE_CONF_FOR_START}+size>={MIN_FACE_SIZE_FOR_START}px, æˆ–conf>={HIGH_CONF_FOR_RELAXED_SIZE}+size>={MIN_FACE_SIZE_FOR_START_RELAXED}px)")
                
                # ========== åœºæ™¯3: æ— æ£€æµ‹ ==========
                else:
                    state_machine.state = SystemState.IDLE
                    print("[æç¤º] æœªæ£€æµ‹åˆ°äººè„¸ï¼Œæ— æ³•å¯åŠ¨")
            
            elif state_machine.state == SystemState.IDLE and old_state == SystemState.TRACKING:
                # åœæ­¢è·Ÿéš - åªæœ‰ä» TRACKING çŠ¶æ€æ‰èƒ½åœæ­¢
                mv_recognizer.clear_target()
                mv_recognizer.clear_match_history()  # æ¸…ç©ºå†å²
                lost_frames = 0
                print("[æ‰‹åŠ¿åœæ­¢] è·Ÿéšå·²åœæ­¢")
        
        # ============== ç›®æ ‡è·Ÿè¸ª ==============
        target_person_idx = -1
        target_face_idx = -1  # ä»…äººè„¸åŒ¹é…æ—¶çš„ç´¢å¼•
        current_match_info = None  # å½“å‰å¸§åŒ¹é…ä¿¡æ¯ï¼Œç”¨äºç•Œé¢æ˜¾ç¤º
        
        # ============== åœºæ™¯åˆ¤æ–­ ==============
        # å•äººåœºæ™¯çš„ä¸¥æ ¼å®šä¹‰:
        #   1. åªæœ‰å•è„¸ï¼ˆæ— äººä½“ï¼‰
        #   2. åªæœ‰å•äººä½“ï¼ˆæ— è„¸ï¼‰
        #   3. å•è„¸ + å•äººä½“ï¼Œä¸”è„¸åœ¨äººä½“æ¡†å†…
        # å¤šäººåœºæ™¯:
        #   1. å¤šä¸ªäººä½“
        #   2. å¤šä¸ªäººè„¸
        #   3. å•è„¸ + å•äººä½“ï¼Œä½†è„¸ä¸åœ¨äººä½“æ¡†å†…ï¼ˆä¸¤ä¸ªä¸åŒçš„äººï¼‰
        num_persons = len(persons)
        num_faces = len(faces)
        
        # æ£€æŸ¥å•è„¸+å•äººä½“æ—¶ï¼Œè„¸æ˜¯å¦åœ¨äººä½“æ¡†å†…
        face_in_person_for_scene = False
        if num_faces == 1 and num_persons == 1:
            fx1, fy1, fx2, fy2 = faces[0].bbox.astype(int)
            fc_x, fc_y = (fx1 + fx2) // 2, (fy1 + fy2) // 2
            px1, py1, px2, py2 = persons[0].bbox.astype(int)
            face_in_person_for_scene = (px1 <= fc_x <= px2 and py1 <= fc_y <= py2)
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºå•äººåœºæ™¯
        if num_persons == 0 and num_faces == 0:
            is_single_person_scene = True  # æ²¡äºº
        elif num_persons == 0 and num_faces == 1:
            is_single_person_scene = True  # åªæœ‰å•è„¸
        elif num_persons == 1 and num_faces == 0:
            is_single_person_scene = True  # åªæœ‰å•äººä½“
        elif num_persons == 1 and num_faces == 1 and face_in_person_for_scene:
            is_single_person_scene = True  # å•è„¸+å•äººä½“ï¼Œè„¸åœ¨æ¡†å†…
        else:
            is_single_person_scene = False  # å…¶ä»–éƒ½æ˜¯å¤šäººåœºæ™¯
        
        is_multi_person_scene = not is_single_person_scene
        
        # ============================================
        # äº¤æ±‡æ£€æµ‹ï¼šä¸¤äººæ¡†é‡å æ—¶éœ€è¦ç‰¹æ®Šå¤„ç†
        # ============================================
        # åœºæ™¯ï¼šä¸¤äººäº¤å‰èµ°è¿‡ï¼Œéç›®æ ‡ç«™åˆ°å‰é¢ï¼Œé®æŒ¡ç›®æ ‡
        # é£é™©ï¼šå¦‚æœäººè„¸å¤ªå°æ— æ³•éªŒè¯ï¼Œå¯èƒ½è¯¯è·Ÿè¸ªåˆ°éç›®æ ‡
        # ç­–ç•¥ï¼šæ£€æµ‹åˆ°äº¤æ±‡æ—¶ï¼Œæé«˜åŒ¹é…é˜ˆå€¼ï¼Œå®å¯ä¸¢å¤±ä¹Ÿä¸è¯¯è·Ÿè¸ª
        is_crossing_scene = False
        crossing_iou = 0.0
        
        if num_persons >= 2:
            # è®¡ç®—æ‰€æœ‰äººä½“æ¡†ä¹‹é—´çš„æœ€å¤§IoU
            for i in range(num_persons):
                for j in range(i + 1, num_persons):
                    box1 = persons[i].bbox.astype(int)
                    box2 = persons[j].bbox.astype(int)
                    
                    # è®¡ç®—IoU
                    x1 = max(box1[0], box2[0])
                    y1 = max(box1[1], box2[1])
                    x2 = min(box1[2], box2[2])
                    y2 = min(box1[3], box2[3])
                    
                    if x1 < x2 and y1 < y2:
                        inter_area = (x2 - x1) * (y2 - y1)
                        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
                        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
                        union_area = area1 + area2 - inter_area
                        iou = inter_area / union_area if union_area > 0 else 0
                        
                        if iou > crossing_iou:
                            crossing_iou = iou
            
            # IoU > 0.15 è®¤ä¸ºæ˜¯äº¤æ±‡åœºæ™¯
            CROSSING_IOU_THRESHOLD = 0.15
            is_crossing_scene = crossing_iou > CROSSING_IOU_THRESHOLD
            
            if is_crossing_scene and frame_count % 30 == 0:
                print(f"[DEBUG] âš ï¸ æ£€æµ‹åˆ°äº¤æ±‡åœºæ™¯ (IoU={crossing_iou:.2f}), å¯ç”¨ä¸¥æ ¼åŒ¹é…æ¨¡å¼")
        
        if state_machine.state == SystemState.TRACKING:
            matched_any = False
            
            # è°ƒè¯•: æ˜¾ç¤ºç›®æ ‡ä¿¡æ¯å’Œåœºæ™¯ç±»å‹
            if frame_count % 30 == 0:
                scene_type = "å¤šäºº" if is_multi_person_scene else "å•äºº"
                extra_info = ""
                if num_persons == 1 and num_faces == 1:
                    extra_info = f", è„¸åœ¨æ¡†å†…={face_in_person_for_scene}"
                print(f"[DEBUG] åœºæ™¯: {scene_type} (persons={num_persons}, faces={num_faces}{extra_info})")
                if mv_recognizer.target:
                    t = mv_recognizer.target
                    print(f"[DEBUG] Target: num_views={t.num_views}, has_face_view={t.has_face_view}")
                    for vi, v in enumerate(t.view_features):
                        print(f"        View[{vi}]: has_face={v.has_face}, has_body={v.part_color_hists is not None}")
            
            # 1. é€šè¿‡äººä½“åŒ¹é… - ä½¿ç”¨"æœ€ä½³åŒ¹é…"ç­–ç•¥ï¼ˆè€Œä¸æ˜¯"ç¬¬ä¸€ä¸ªåŒ¹é…"ï¼‰
            # æ”¶é›†æ‰€æœ‰å€™é€‰åŒ¹é…ï¼Œé€‰æ‹©æœ€é«˜åˆ†çš„
            all_person_matches = []  # [(idx, similarity, method, view, face_in_person, face_verified, face_sim, body_sim)]
            
            # è®°å½•äººä½“è¢«æ‹’ç»çš„åŸå› ï¼Œç”¨äºå†³å®šæ˜¯å¦å…è®¸ä»…äººè„¸åŒ¹é…
            persons_rejected_by_face_mismatch = 0  # å› "äººè„¸æ˜ç¡®ä¸åŒ¹é…"è¢«æ‹’ç»çš„äººä½“æ•°
            persons_total_checked = 0
            
            # å…³é”®ä¿æŠ¤ï¼šå¦‚æœç›®æ ‡æœ‰äººè„¸ç‰¹å¾ï¼Œå€™é€‰äººä¹Ÿæœ‰äººè„¸æ—¶å¿…é¡»é€šè¿‡äººè„¸éªŒè¯
            target_has_face = mv_recognizer.target and mv_recognizer.target.has_face_view
            target_has_body = mv_recognizer.target and any(v.has_body for v in mv_recognizer.target.view_features)
            
            # =====================================================================
            # åœºæ™¯Ã—ç›®æ ‡çŠ¶æ€ åˆ†æçŸ©é˜µ
            # =====================================================================
            # 
            # ç”»é¢å†…å®¹:
            #   å•äººåœºæ™¯: persons<=1 ä¸” faces<=1
            #   å¤šäººåœºæ™¯: persons>1 æˆ– faces>1
            #
            # ç›®æ ‡åœ¨ç”»é¢ä¸­çš„çŠ¶æ€:
            #   A: ç›®æ ‡ä»¥ äººè„¸+äººä½“ å‡ºç°
            #   B: ç›®æ ‡ä»…ä»¥ äººè„¸ å‡ºç°ï¼ˆäººä½“è¢«é®æŒ¡æˆ–å¤ªè¿œï¼‰
            #   C: ç›®æ ‡ä»…ä»¥ äººä½“ å‡ºç°ï¼ˆèƒŒå¯¹/ä½å¤´/é®æŒ¡è„¸ï¼‰
            #   D: ç›®æ ‡ä¸åœ¨ç”»é¢ä¸­
            #
            # å¤„ç†ç­–ç•¥:
            # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            # â”‚ Step1: éå†äººä½“åŒ¹é… â†’ è¦†ç›–çŠ¶æ€ A, C                              â”‚
            # â”‚   - A: äººè„¸éªŒè¯é€šè¿‡ â†’ ç¡®è®¤åŒ¹é…                                   â”‚
            # â”‚   - C: æ— äººè„¸å¯éªŒè¯ï¼Œä½¿ç”¨bodyåŒ¹é…                                â”‚
            # â”‚                                                                  â”‚
            # â”‚ Step2: ä»…äººè„¸åŒ¹é… â†’ è¦†ç›–çŠ¶æ€ B                                   â”‚
            # â”‚   - æ— äººä½“åŒ¹é…æˆåŠŸæ—¶ï¼Œå°è¯•ç‹¬ç«‹äººè„¸åŒ¹é…                           â”‚
            # â”‚                                                                  â”‚
            # â”‚ æœªåŒ¹é… â†’ çŠ¶æ€ D æˆ–åŒ¹é…å¤±è´¥                                       â”‚
            # â”‚   - ç´¯ç§¯ lost_frames â†’ è§¦å‘ LOST_TARGET                          â”‚
            # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            #
            # å…³é”®é£é™©: ç›®æ ‡ä¸åœ¨(D) ä½†è¯¯åŒ¹é…åˆ°è¡£ç€ç›¸ä¼¼çš„ä»–äºº
            #
            # ä¿æŠ¤æªæ–½æ±‡æ€»:
            # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            # â”‚ åœºæ™¯               â”‚ ä¿æŠ¤ç­–ç•¥                                    â”‚
            # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            # â”‚ å¤šäºº+ç›®æ ‡æœ‰è„¸+     â”‚ face_sim < FACE_REJECT â†’ æ‹’ç»              â”‚
            # â”‚ å€™é€‰æœ‰è„¸           â”‚ face_sim < FACE_UNCERTAIN ä¸”               â”‚
            # â”‚                    â”‚ body_sim < HIGH_BODY â†’ æ‹’ç»                 â”‚
            # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            # â”‚ å¤šäºº+ç›®æ ‡æœ‰è„¸+     â”‚ body_sim < BACK_VIEW_BODY â†’ æ‹’ç»           â”‚
            # â”‚ å€™é€‰æ— è„¸           â”‚ (å¯èƒ½æ˜¯ä»–äººèƒŒé¢)                            â”‚
            # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            # â”‚ å•äºº+ç›®æ ‡æœ‰è„¸+     â”‚ åŒä¸Šä¿æŠ¤é€»è¾‘                                â”‚
            # â”‚ å€™é€‰æœ‰è„¸           â”‚ å› ä¸ºé‚£ä¸ª"å•äºº"å¯èƒ½ä¸æ˜¯ç›®æ ‡                  â”‚
            # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            # â”‚ å•äºº+ç›®æ ‡æœ‰è„¸+     â”‚ å¦‚æœ‰å…¶ä»–äººè„¸: body_sim < 0.65 â†’ æ‹’ç»       â”‚
            # â”‚ å€™é€‰æ— è„¸           â”‚ æ— å…¶ä»–äººè„¸: ä½¿ç”¨æ ‡å‡†é˜ˆå€¼                    â”‚
            # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            # â”‚ ä»…äººè„¸åŒ¹é…         â”‚ å¤šäººè„¸: +0.05 é˜ˆå€¼æƒ©ç½š                      â”‚
            # â”‚                    â”‚ è¿œå¤„äººè„¸: +0.10 é˜ˆå€¼æƒ©ç½š                    â”‚
            # â”‚                    â”‚ äººè„¸åœ¨ä¸åŒ¹é…äººä½“æ¡†å†…: è·³è¿‡                  â”‚
            # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            # =====================================================================
            
            for idx, person in enumerate(persons):
                view = extract_view_feature(
                    frame, person.bbox, faces, face_recognizer, enhanced_reid
                )
                
                # ä½¿ç”¨ return_details=True è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å« face_simï¼‰
                result = mv_recognizer.is_same_target(
                    view, person.bbox, return_details=True
                )
                # è¿”å›å€¼æ˜¯ (is_match, similarity, method, details)
                is_match = result[0]
                similarity = result[1]
                method = result[2]
                details = result[3] if len(result) > 3 else {}
                
                # æå–è¯¦ç»†ç›¸ä¼¼åº¦
                face_sim = details.get('face_sim')  # å¯èƒ½ä¸º Noneï¼ˆå€™é€‰äººæ²¡æœ‰äººè„¸ï¼‰
                body_sim = details.get('body_sim', 0.0)
                
                # æå–è¿åŠ¨è¿ç»­æ€§åˆ†æ•°
                motion_score = details.get('motion_sim', 0.0)
                if 'M:' in method:
                    try:
                        motion_str = method.split('M:')[1].split(')')[0].split(' ')[0]
                        motion_score = float(motion_str)
                    except:
                        pass
                
                if frame_count % 30 == 0:
                    face_str = f"F:{face_sim:.2f}" if face_sim is not None else "F:None"
                    print(f"[DEBUG] Person[{idx}] match: is_match={is_match}, sim={similarity:.3f}, {face_str}, B:{body_sim:.2f}, M:{motion_score:.2f}, method={method}")
                
                if is_match:
                    face_in_person = view.has_face and view.face_embedding is not None
                    
                    # ============================================
                    # åˆ†å±‚åŒ¹é…é€»è¾‘ï¼ˆåŸºäºäººè„¸è´¨é‡åˆ†çº§ï¼‰
                    # ============================================
                    # å¯¹äºã€æœ‰æ•ˆäººè„¸ã€‘(size>=50px, conf>=0.65):
                    #   F >= 0.65: face_priority (é«˜ç½®ä¿¡åº¦ï¼Œä»…é äººè„¸)
                    #   0.45 <= F < 0.65: face + motion (ä¸­ç­‰ç½®ä¿¡åº¦)
                    #   0.30 <= F < 0.45: body + motion (ä½ç½®ä¿¡åº¦äººè„¸)
                    #   F < 0.30: æ˜ç¡®æ‹’ç» (å³ä½¿body+motioné«˜ä¹Ÿæ‹’ç»)
                    # 
                    # å¯¹äºã€æ— æ•ˆäººè„¸ã€‘(å°/ä½ç½®ä¿¡åº¦/æ— äººè„¸):
                    #   åªèƒ½é  body + motion
                    # ============================================
                    
                    # äººè„¸ç›¸ä¼¼åº¦åˆ†å±‚é˜ˆå€¼
                    FACE_HIGH_THRESHOLD = 0.65      # é«˜ç½®ä¿¡åº¦ï¼šä»…é äººè„¸
                    FACE_MEDIUM_THRESHOLD = 0.45    # ä¸­ç­‰ç½®ä¿¡åº¦ï¼šäººè„¸+motion
                    FACE_LOW_THRESHOLD = 0.30       # ä½ç½®ä¿¡åº¦ä¸´ç•Œå€¼
                    FACE_REJECT_THRESHOLD = 0.30    # ä½äºæ­¤å€¼æ˜ç¡®æ‹’ç»
                    
                    FACE_MATCH_THRESHOLD = 0.55     # äººè„¸åŒ¹é…é˜ˆå€¼ï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰
                    BODY_MOTION_THRESHOLD = 0.65    # body + motion ç»¼åˆé˜ˆå€¼
                    MULTI_PERSON_BODY_THRESHOLD = 0.70  # å¤šäººåœºæ™¯ä¸‹ä»…bodyåŒ¹é…çš„é˜ˆå€¼
                    
                    # æœ‰æ•ˆäººè„¸çš„å®šä¹‰
                    MIN_FACE_SIZE_FOR_VALID = 30    # æœ‰æ•ˆäººè„¸æœ€å°å°ºå¯¸ï¼ˆæµ‹è¯•éªŒè¯30pxå³å¯å‡†ç¡®è¯†åˆ«ï¼‰
                    MIN_FACE_CONF_FOR_VALID = 0.65  # æœ‰æ•ˆäººè„¸æœ€ä½ç½®ä¿¡åº¦
                    MIN_FACE_SIZE_RELAXED = 30      # æ”¾å®½æ¡ä»¶çš„æœ€å°å°ºå¯¸
                    
                    # æ£€æŸ¥äººè„¸å°ºå¯¸æ˜¯å¦è¶³å¤Ÿå¤§
                    face_size_valid = False
                    face_size_valid_relaxed = False  # æ”¾å®½æ¡ä»¶ï¼ˆå•äºº+é«˜ç›¸ä¼¼åº¦ï¼‰
                    current_face_size = 0
                    for face in faces:
                        fx1, fy1, fx2, fy2 = face.bbox.astype(int)
                        fc_x, fc_y = (fx1 + fx2) // 2, (fy1 + fy2) // 2
                        px1, py1, px2, py2 = person.bbox.astype(int)
                        if px1 <= fc_x <= px2 and py1 <= fc_y <= py2:
                            face_w = fx2 - fx1
                            face_h = fy2 - fy1
                            current_face_size = min(face_w, face_h)
                            face_size_valid = current_face_size >= MIN_FACE_SIZE
                            face_size_valid_relaxed = current_face_size >= MIN_FACE_SIZE_RELAXED
                            break
                    
                    # è®¡ç®— body + motion ç»¼åˆåˆ†æ•°
                    # å¤šäººåœºæ™¯ä¸‹å¢åŠ  motion æƒé‡ï¼Œå› ä¸ºè¿åŠ¨è½¨è¿¹æ›´å¯é 
                    if is_multi_person_scene:
                        motion_weight = MOTION_WEIGHT_MULTI_PERSON
                    else:
                        motion_weight = MOTION_WEIGHT_SINGLE_PERSON
                    body_weight = 1.0 - motion_weight
                    body_motion_score = body_sim * body_weight + motion_score * motion_weight
                    
                    # åˆ¤æ–­åŒ¹é…ç±»å‹
                    # äººè„¸æœ‰æ•ˆæ¡ä»¶ï¼š
                    #   - æ ‡å‡†: ç›¸ä¼¼åº¦>=0.55 ä¸” å°ºå¯¸>=40px
                    #   - æ”¾å®½(å•äºº+é«˜ç½®ä¿¡): ç›¸ä¼¼åº¦>=0.65 ä¸” å°ºå¯¸>=20px
                    face_matched_standard = (face_sim is not None and 
                                             face_sim >= FACE_MATCH_THRESHOLD and 
                                             face_size_valid)
                    face_matched_relaxed = (face_sim is not None and 
                                            face_sim >= FACE_HIGH_THRESHOLD and  # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å¸¸é‡å
                                            face_size_valid_relaxed and 
                                            is_single_person_scene)
                    face_matched = face_matched_standard or face_matched_relaxed
                    body_motion_matched = body_motion_score >= BODY_MOTION_THRESHOLD
                    
                    if frame_count % 30 == 0 and face_sim is not None:
                        # åªæœ‰å½“é€šè¿‡æ”¾å®½æ¡ä»¶è€Œéæ ‡å‡†æ¡ä»¶æ—¶æ‰æ˜¾ç¤º relaxed
                        relaxed_info = ""
                        if face_matched_relaxed and not face_matched_standard:
                            relaxed_info = ", relaxed=True"
                        print(f"[DEBUG] Person[{idx}] face_size={current_face_size}px, valid={face_size_valid}, face_matched={face_matched}{relaxed_info}")
                    
                    # å†³ç­–é€»è¾‘
                    accept = False
                    match_type = ""
                    persons_total_checked += 1
                    
                    # ============================================
                    # åˆ†å±‚å†³ç­–ï¼šåŸºäºäººè„¸æœ‰æ•ˆæ€§å’Œç›¸ä¼¼åº¦
                    # ============================================
                    # 1. å…ˆåˆ¤æ–­äººè„¸æ˜¯å¦"æœ‰æ•ˆ"ï¼ˆå¯ç”¨äºåˆ¤æ–­èº«ä»½ï¼‰
                    # 2. æœ‰æ•ˆäººè„¸ï¼šæ ¹æ®ç›¸ä¼¼åº¦åˆ†å±‚å†³ç­–
                    # 3. æ— æ•ˆäººè„¸ï¼šåªèƒ½é  body + motion
                    # ============================================
                    
                    # è·å–äººè„¸ç½®ä¿¡åº¦ï¼ˆç”¨äºåˆ¤æ–­æœ‰æ•ˆæ€§ï¼‰
                    current_face_conf = 0.0
                    for face in faces:
                        fx1, fy1, fx2, fy2 = face.bbox.astype(int)
                        fc_x, fc_y = (fx1 + fx2) // 2, (fy1 + fy2) // 2
                        px1, py1, px2, py2 = person.bbox.astype(int)
                        if px1 <= fc_x <= px2 and py1 <= fc_y <= py2:
                            current_face_conf = face.confidence
                            break
                    
                    # åˆ¤æ–­äººè„¸æ˜¯å¦"æœ‰æ•ˆ"ï¼ˆå¯ç”¨äºèº«ä»½åˆ¤æ–­ï¼‰
                    face_is_valid = (face_in_person and 
                                    face_sim is not None and 
                                    current_face_size >= MIN_FACE_SIZE_FOR_VALID and
                                    current_face_conf >= MIN_FACE_CONF_FOR_VALID)
                    
                    # æ”¾å®½çš„æœ‰æ•ˆæ¡ä»¶ï¼ˆé«˜ç›¸ä¼¼åº¦æ—¶å¯æ¥å—è¾ƒå°äººè„¸ï¼‰
                    face_is_valid_relaxed = (face_in_person and 
                                            face_sim is not None and 
                                            current_face_size >= MIN_FACE_SIZE_RELAXED and
                                            current_face_conf >= 0.60)
                    
                    if frame_count % 30 == 0:
                        face_str = f"F:{face_sim:.2f}" if face_sim is not None else "F:None"
                        print(f"[DEBUG] Person[{idx}] äººè„¸æœ‰æ•ˆæ€§: size={current_face_size}px, conf={current_face_conf:.2f}, valid={face_is_valid}, relaxed_valid={face_is_valid_relaxed}")
                    
                    if face_is_valid:
                        # ========== æœ‰æ•ˆäººè„¸ï¼šåŸºäºç›¸ä¼¼åº¦åˆ†å±‚ ==========
                        if face_sim >= FACE_HIGH_THRESHOLD:
                            # Layer 1: F >= 0.65 â†’ é«˜ç½®ä¿¡åº¦ï¼Œä»…é äººè„¸
                            accept = True
                            match_type = "face"
                            if frame_count % 30 == 0:
                                print(f"[DEBUG] Person[{idx}] âœ“ æœ‰æ•ˆäººè„¸é«˜ç½®ä¿¡åº¦ (F:{face_sim:.2f}>=0.65) â†’ face_priority")
                        elif face_sim >= FACE_MEDIUM_THRESHOLD:
                            # Layer 2: 0.45 <= F < 0.65 â†’ ä¸­ç­‰ç½®ä¿¡åº¦ï¼Œéœ€è¦motionè¾…åŠ©
                            # è¦æ±‚ motion >= 0.5 æˆ– ç»¼åˆåˆ†æ•°å¤Ÿé«˜
                            if motion_score >= 0.5 or body_motion_score >= BODY_MOTION_THRESHOLD:
                                accept = True
                                match_type = "face_motion"
                                if frame_count % 30 == 0:
                                    print(f"[DEBUG] Person[{idx}] âœ“ æœ‰æ•ˆäººè„¸ä¸­ç­‰ç½®ä¿¡åº¦ (F:{face_sim:.2f}, M:{motion_score:.2f}) â†’ face+motion")
                            else:
                                accept = False
                                if frame_count % 30 == 0:
                                    print(f"[DEBUG] Person[{idx}] âœ— æœ‰æ•ˆäººè„¸ä¸­ç­‰ä½†motionä¸è¶³ (F:{face_sim:.2f}, M:{motion_score:.2f}<0.5)")
                        elif face_sim >= FACE_LOW_THRESHOLD:
                            # Layer 3: 0.30 <= F < 0.45 â†’ ä½ç½®ä¿¡åº¦ï¼Œéœ€è¦body+motion
                            if body_motion_score >= BODY_MOTION_THRESHOLD:
                                accept = True
                                match_type = "body_motion"
                                if frame_count % 30 == 0:
                                    print(f"[DEBUG] Person[{idx}] âœ“ æœ‰æ•ˆäººè„¸ä½ç½®ä¿¡åº¦ (F:{face_sim:.2f}) + body+motioné«˜ â†’ body+motion")
                            else:
                                accept = False
                                if frame_count % 30 == 0:
                                    print(f"[DEBUG] Person[{idx}] âœ— æœ‰æ•ˆäººè„¸ä½ç½®ä¿¡åº¦ (F:{face_sim:.2f}) ä¸”body+motionä¸è¶³")
                        else:
                            # Layer 4: F < 0.30 â†’ æ˜ç¡®ä¸åŒ¹é…ï¼Œæ‹’ç»ï¼
                            # â˜…â˜…â˜… æ ¸å¿ƒä¿®å¤ï¼šå³ä½¿body+motioné«˜ä¹Ÿæ‹’ç» â˜…â˜…â˜…
                            accept = False
                            persons_rejected_by_face_mismatch += 1
                            if frame_count % 30 == 0:
                                scene_type = "å¤šäºº" if is_multi_person_scene else "å•äºº"
                                print(f"[DEBUG] Person[{idx}] âœ—âœ— {scene_type}æœ‰æ•ˆäººè„¸æ˜ç¡®ä¸åŒ¹é… (F:{face_sim:.2f}<0.30) â†’ ç›´æ¥æ‹’ç»")
                    
                    elif face_is_valid_relaxed and face_sim is not None and face_sim >= FACE_HIGH_THRESHOLD:
                        # ========== æ”¾å®½æ¡ä»¶ï¼šè¾ƒå°äººè„¸ä½†é«˜ç›¸ä¼¼åº¦ ==========
                        accept = True
                        match_type = "face"
                        if frame_count % 30 == 0:
                            print(f"[DEBUG] Person[{idx}] âœ“ æ”¾å®½æœ‰æ•ˆäººè„¸é«˜ç½®ä¿¡åº¦ (F:{face_sim:.2f}>=0.65, size={current_face_size}px)")
                    
                    elif face_is_valid_relaxed and face_sim is not None and face_sim < FACE_REJECT_THRESHOLD:
                        # ========== æ”¾å®½æ¡ä»¶ï¼šè¾ƒå°äººè„¸ä½†æ˜ç¡®ä¸åŒ¹é… ==========
                        accept = False
                        persons_rejected_by_face_mismatch += 1
                        if frame_count % 30 == 0:
                            scene_type = "å¤šäºº" if is_multi_person_scene else "å•äºº"
                            print(f"[DEBUG] Person[{idx}] âœ—âœ— {scene_type}è¾ƒå°äººè„¸æ˜ç¡®ä¸åŒ¹é… (F:{face_sim:.2f}<0.30, size={current_face_size}px) â†’ æ‹’ç»")
                    
                    elif body_motion_matched:
                        # ========== æ— æœ‰æ•ˆäººè„¸ï¼šé  body + motion ==========
                        
                        # â˜…â˜…â˜… é‡è¦ï¼šå°äººè„¸ï¼ˆ<30pxï¼‰çš„Få€¼ä¸å¯é ï¼Œä¸èƒ½ç”¨äºæ‹’ç»å†³ç­–ï¼â˜…â˜…â˜…
                        # åªæœ‰"æ”¾å®½æœ‰æ•ˆ"çš„äººè„¸ï¼ˆ>=30pxï¼‰æ‰èƒ½ç”¨F<0.30æ¥åˆ¤æ–­ä¸åŒ¹é…
                        # å°äººè„¸çš„ä½Få€¼å¯èƒ½æ˜¯ç‰¹å¾æå–ä¸å‡†ï¼Œè€Œä¸æ˜¯çœŸçš„ä¸åŒ¹é…
                        
                        if face_is_valid_relaxed and face_sim is not None and face_sim < FACE_REJECT_THRESHOLD:
                            # æ”¾å®½æœ‰æ•ˆçš„äººè„¸ï¼ˆ>=30pxï¼‰ï¼ŒF<0.30 â†’ æ˜ç¡®ä¸åŒ¹é…
                            accept = False
                            persons_rejected_by_face_mismatch += 1
                            if frame_count % 30 == 0:
                                scene_type = "å¤šäºº" if is_multi_person_scene else "å•äºº"
                                print(f"[DEBUG] Person[{idx}] âœ—âœ— {scene_type}äººè„¸æ˜ç¡®ä¸åŒ¹é… (F:{face_sim:.2f}<0.30, size={current_face_size}px>=30) â†’ æ‹’ç»")
                        # äº¤æ±‡åœºæ™¯ç‰¹æ®Šå¤„ç†
                        elif is_crossing_scene and target_has_face:
                            # äº¤æ±‡æ—¶æ²¡æœ‰æœ‰æ•ˆäººè„¸éªŒè¯ â†’ å®å¯çŸ­æš‚ä¸¢å¤±
                            if frame_count % 30 == 0:
                                face_str = f"F:{face_sim:.2f}" if face_sim is not None else "F:None"
                                print(f"[DEBUG] Person[{idx}] âš ï¸ äº¤æ±‡åœºæ™¯æ— æœ‰æ•ˆäººè„¸({face_str}, size={current_face_size}px), æš‚åœåŒ¹é…")
                            accept = False
                        # å¤šäººåœºæ™¯ï¼šbodyé˜ˆå€¼æé«˜
                        elif is_multi_person_scene and target_has_face and body_sim < MULTI_PERSON_BODY_THRESHOLD - 0.01:
                            if frame_count % 30 == 0:
                                print(f"[DEBUG] Person[{idx}] âœ— å¤šäººåœºæ™¯æ— æœ‰æ•ˆäººè„¸ä¸”bodyä¸è¶³({body_sim:.2f}<{MULTI_PERSON_BODY_THRESHOLD-0.01:.2f})")
                            accept = False
                        else:
                            accept = True
                            match_type = "body_motion"
                            if frame_count % 30 == 0:
                                print(f"[DEBUG] Person[{idx}] âœ“ æ— æœ‰æ•ˆäººè„¸ï¼Œbody+motioné€šè¿‡ (B:{body_sim:.2f}+M:{motion_score:.2f}={body_motion_score:.2f})")
                    else:
                        # ========== ä»€ä¹ˆéƒ½ä¸å¤Ÿ ==========
                        if frame_count % 30 == 0:
                            face_str = f"F:{face_sim:.2f}" if face_sim is not None else "F:None"
                            print(f"[DEBUG] Person[{idx}] âœ— æ— æœ‰æ•ˆäººè„¸ä¸”body+motionä¸è¶³ ({face_str}, BM:{body_motion_score:.2f})")
                        accept = False
                    
                    if accept:
                        # tuple: (idx, similarity, method, view, face_in_person, face_matched, face_sim, body_sim, motion_score, match_type)
                        all_person_matches.append((idx, similarity, method, view, face_in_person, face_matched, face_sim, body_sim, motion_score, match_type))
            
            # é€‰æ‹©æœ€ä½³åŒ¹é…
            if all_person_matches:
                # ç­–ç•¥: 
                #   1. ä¼˜å…ˆé€‰äººè„¸åŒ¹é…çš„ï¼ˆèº«ä»½æœ€å¯é ï¼‰ï¼šface, face_motion
                #   2. äººè„¸åŒ¹é…ä¸­ä¼˜å…ˆé€‰ motion é«˜çš„ï¼ˆè½¨è¿¹æœ€ä¸€è‡´ï¼‰
                #   3. å…¶æ¬¡é€‰ body+motion åŒ¹é…çš„
                #   4. body+motion ä¸­å¤šäººåœºæ™¯ä¼˜å…ˆé€‰ motion é«˜çš„
                # tuple: (idx, similarity, method, view, face_in_person, face_matched, face_sim, body_sim, motion_score, match_type)
                matches_by_face = [m for m in all_person_matches if m[9] in ("face", "face_motion")]  # m[9] = match_type
                matches_by_body_motion = [m for m in all_person_matches if m[9] == "body_motion"]
                
                best_match = None
                if matches_by_face:
                    # äººè„¸åŒ¹é…ä¸­ï¼Œä¼˜å…ˆé€‰ motion é«˜çš„ï¼ˆè½¨è¿¹ä¸€è‡´æ€§ï¼‰
                    # æ’åºä¾æ®: face_sim * 0.6 + motion * 0.4
                    best_match = max(matches_by_face, key=lambda x: (x[6] if x[6] is not None else 0) * 0.6 + x[8] * 0.4)
                    if frame_count % 30 == 0 and len(all_person_matches) > 1:
                        print(f"[DEBUG] é€‰æ‹©äººè„¸åŒ¹é… Person[{best_match[0]}] (F:{best_match[6]:.2f}, M:{best_match[8]:.2f}, å…±{len(all_person_matches)}å€™é€‰)")
                elif matches_by_body_motion:
                    # body+motion åŒ¹é…ä¸­ï¼Œå¤šäººåœºæ™¯å¼ºè°ƒ motionï¼Œå•äººåœºæ™¯å¹³è¡¡
                    if is_multi_person_scene:
                        # å¤šäºº: motion ä¼˜å…ˆï¼ˆè½¨è¿¹ä¸€è‡´æœ€é‡è¦ï¼‰
                        best_match = max(matches_by_body_motion, key=lambda x: x[8] * 0.7 + x[7] * 0.3)
                    else:
                        # å•äºº: å¹³è¡¡ body å’Œ motion
                        best_match = max(matches_by_body_motion, key=lambda x: x[7] * 0.5 + x[8] * 0.5)
                    if frame_count % 30 == 0:
                        print(f"[DEBUG] é€‰æ‹©body+motionåŒ¹é… Person[{best_match[0]}], B:{best_match[7]:.2f}, M:{best_match[8]:.2f}")
                
                if best_match:
                    # è§£åŒ…: (idx, similarity, method, view, face_in_person, face_matched, face_sim, body_sim, motion_score, match_type)
                    idx, similarity, method, view, face_in_person, face_matched, match_face_sim, match_body_sim, match_motion_score, match_type = best_match
                    matched_any = True
                    target_person_idx = idx
                    lost_frames = 0
                    
                    # â˜…â˜…â˜… å…³é”®æ—¥å¿—ï¼šæ ‡æ³¨æœ€ç»ˆé€‰æ‹©çš„ç›®æ ‡ â˜…â˜…â˜…
                    if frame_count % 30 == 0:
                        px1, py1, px2, py2 = persons[idx].bbox.astype(int)
                        face_str = f"F:{match_face_sim:.2f}" if match_face_sim is not None else "F:None"
                        print(f"[â˜…ç›®æ ‡â˜…] Person[{idx}] è¢«é€‰ä¸ºç›®æ ‡ (ç»¿æ¡†)")
                        print(f"         bbox=[{px1},{py1},{px2},{py2}], {face_str}, B:{match_body_sim:.2f}, M:{match_motion_score:.2f}")
                        print(f"         åŒ¹é…ç±»å‹: {match_type}, æ–¹æ³•: {method[:50]}")
                    
                    # ä¿å­˜å½“å‰åŒ¹é…ä¿¡æ¯ç”¨äºæ˜¾ç¤º
                    current_match_info = {
                        'type': 'person',
                        'similarity': similarity,
                        'method': method,
                        'match_type': match_type,  # "face" or "body_motion"
                        'threshold': FACE_MATCH_THRESHOLD if match_type == "face" else BODY_MOTION_THRESHOLD
                    }
                    
                    # æ›´æ–°è·Ÿè¸ª
                    mv_recognizer.update_tracking(persons[idx].bbox)
                    
                    # ============================================
                    # ç®€åŒ–çš„è‡ªåŠ¨å­¦ä¹ ç­–ç•¥
                    # ============================================
                    # æ ¸å¿ƒåŸåˆ™ï¼š
                    #   1. äººè„¸åŒ¹é… + bodyä¸åŒ¹é…ä½†motion+bodyé«˜ â†’ å­¦ä¹ bodyï¼ˆå‰æï¼šäººè„¸åœ¨äººä½“æ¡†å†…ï¼‰
                    #   2. motion+bodyåŒ¹é… + äººè„¸ä½ä½†>æŸå€¼ â†’ å­¦ä¹ äººè„¸ï¼ˆå‰æï¼šäººè„¸åœ¨äººä½“æ¡†å†…ï¼‰
                    #   3. å…³é”®çº¦æŸï¼šæœ‰äººè„¸+æœ‰äººä½“æ—¶ï¼Œå­¦ä¹ å¿…é¡»ä¿è¯äººè„¸åœ¨äººä½“æ¡†å†…
                    #   4. è§†è§’åº“æ»¡æ—¶ï¼šç”¨æ›¿æ¢ç­–ç•¥è€Œéåœæ­¢å­¦ä¹ 
                    # ============================================
                    
                    should_learn = False
                    learn_what = ""  # "body" or "face" or "both"
                    learn_reason = ""
                    use_replace_strategy = False  # æ˜¯å¦ä½¿ç”¨æ›¿æ¢ç­–ç•¥
                    
                    target_has_body = (mv_recognizer.target is not None and 
                                       any(v.has_body for v in mv_recognizer.target.view_features))
                    
                    # å®¹é‡æ£€æŸ¥ï¼šè§†è§’åº“æ»¡æ—¶æ”¹ç”¨æ›¿æ¢ç­–ç•¥
                    current_view_count = mv_recognizer.target.num_views if mv_recognizer.target else 0
                    if current_view_count >= MAX_VIEW_COUNT:
                        # ä¸åœæ­¢å­¦ä¹ ï¼Œè€Œæ˜¯æ£€æŸ¥æ˜¯å¦å€¼å¾—æ›¿æ¢
                        use_replace_strategy = True
                        if frame_count % 60 == 0:
                            print(f"[DEBUG] è§†è§’åº“å·²æ»¡({current_view_count})ï¼Œå¯ç”¨æ›¿æ¢ç­–ç•¥")
                    
                    # å¤šäººåœºæ™¯ + æ²¡æœ‰äººè„¸åŒ¹é… = ç¦æ­¢å­¦ä¹ 
                    # å¤šäººåœºæ™¯ + äººè„¸-äººä½“ä¸ä¸€è‡´ = ç¦æ­¢å­¦ä¹ ï¼ˆé˜²æ­¢å…³è”é”™è¯¯å¯¼è‡´å­¦ä¹ æ±¡æŸ“ï¼‰
                    if is_multi_person_scene and match_type != "face":
                        if frame_count % 30 == 0:
                            print(f"[DEBUG] å¤šäººåœºæ™¯æ— äººè„¸åŒ¹é…ï¼Œç¦æ­¢å­¦ä¹ ")
                        should_learn = False
                    elif is_multi_person_scene and match_type == "face":
                        # å¤šäººåœºæ™¯ä¸‹äººè„¸åŒ¹é…ï¼šæ£€æŸ¥äººè„¸-äººä½“ä¸€è‡´æ€§
                        # å¦‚æœäººè„¸é«˜åŒ¹é…(F>=0.55)ä½†èº«ä½“ä½åŒ¹é…(B<0.60)ï¼Œå¯èƒ½æ˜¯å…³è”é”™è¯¯
                        match_face_sim_check = best_match[6] if best_match[6] is not None else 0.0
                        match_body_sim_check = best_match[7]
                        
                        # è®¡ç®—å·®è·ï¼šäººè„¸ç›¸ä¼¼åº¦ - èº«ä½“ç›¸ä¼¼åº¦
                        face_body_gap = match_face_sim_check - match_body_sim_check
                        
                        # å¦‚æœå·®è·è¿‡å¤§ï¼ˆ>=0.25ï¼‰ï¼Œæˆ–è€…èº«ä½“ç›¸ä¼¼åº¦å¤ªä½ï¼ˆ<0.55ï¼‰ï¼Œç¦æ­¢å­¦ä¹ 
                        FACE_BODY_CONSISTENCY_GAP = 0.25  # å…è®¸çš„æœ€å¤§å·®è·
                        BODY_MIN_FOR_LEARN_MULTI = 0.55   # å¤šäººåœºæ™¯ä¸‹å­¦ä¹ éœ€è¦çš„æœ€ä½bodyç›¸ä¼¼åº¦
                        
                        if match_body_sim_check < BODY_MIN_FOR_LEARN_MULTI:
                            if frame_count % 30 == 0:
                                print(f"[DEBUG] å¤šäººåœºæ™¯äººè„¸-äººä½“ä¸ä¸€è‡´(F:{match_face_sim_check:.2f}, B:{match_body_sim_check:.2f}<{BODY_MIN_FOR_LEARN_MULTI})ï¼Œç¦æ­¢å­¦ä¹ ")
                            should_learn = False
                        elif face_body_gap > FACE_BODY_CONSISTENCY_GAP:
                            if frame_count % 30 == 0:
                                print(f"[DEBUG] å¤šäººåœºæ™¯äººè„¸-äººä½“å·®è·è¿‡å¤§(F:{match_face_sim_check:.2f}-B:{match_body_sim_check:.2f}={face_body_gap:.2f}>{FACE_BODY_CONSISTENCY_GAP})ï¼Œç¦æ­¢å­¦ä¹ ")
                            should_learn = False
                    
                    # åªæœ‰æœªè¢«ç¦æ­¢å­¦ä¹ æ—¶æ‰ç»§ç»­å­¦ä¹ é€»è¾‘
                    if should_learn is not False or (not is_multi_person_scene):
                        # æå–åŒ¹é…ä¿¡æ¯
                        match_face_sim = best_match[6] if best_match[6] is not None else 0.0
                        match_body_sim = best_match[7]
                        match_motion = best_match[8]
                        body_motion_combined = match_body_sim * 0.5 + match_motion * 0.5
                        
                        # å­¦ä¹ é˜ˆå€¼
                        FACE_LEARN_THRESHOLD_LOCAL = 0.65  # äººè„¸å­¦ä¹ é˜ˆå€¼
                        BODY_MOTION_LEARN_THRESHOLD = 0.70  # body+motion å­¦ä¹ é˜ˆå€¼
                        FACE_MIN_FOR_BODY_LEARN = 0.50  # å­¦ä¹ bodyæ—¶äººè„¸çš„æœ€ä½è¦æ±‚
                        
                        # æ£€æŸ¥å½“å‰äººè„¸å°ºå¯¸æ˜¯å¦è¶³å¤Ÿå¤§ï¼ˆç”¨äºå­¦ä¹ ï¼‰
                        current_face_size_for_learn = 0
                        face_size_ok_for_learn = False
                        for face in faces:
                            fx1, fy1, fx2, fy2 = face.bbox.astype(int)
                            fc_x, fc_y = (fx1 + fx2) // 2, (fy1 + fy2) // 2
                            px1, py1, px2, py2 = persons[idx].bbox.astype(int)
                            if px1 <= fc_x <= px2 and py1 <= fc_y <= py2:
                                face_w = fx2 - fx1
                                face_h = fy2 - fy1
                                current_face_size_for_learn = min(face_w, face_h)
                                face_size_ok_for_learn = current_face_size_for_learn >= MIN_FACE_SIZE_FOR_LEARN
                                break
                        
                        # Case 1: äººè„¸åŒ¹é…é€šè¿‡ â†’ å¯ä»¥å­¦ä¹ body
                        if match_type == "face":
                            # æ£€æŸ¥ç›®æ ‡æ˜¯å¦è¿˜æ²¡æœ‰äººä½“è§†è§’ï¼ˆä»…äººè„¸æ¨¡å¼å¯åŠ¨çš„æƒ…å†µï¼‰
                            target_has_body_view = mv_recognizer.target is not None and any(v.has_body for v in mv_recognizer.target.view_features)
                            
                            # Case 1a: ç›®æ ‡æ²¡æœ‰äººä½“è§†è§’ï¼ˆä»…äººè„¸æ¨¡å¼å¯åŠ¨ï¼‰+ äººè„¸åŒ¹é…æˆåŠŸ + body+motioné«˜ â†’ å‡çº§åˆå§‹è§†è§’
                            if not target_has_body_view and face_in_person and body_motion_combined >= 0.70:
                                initial_view = mv_recognizer.target.view_features[0] if mv_recognizer.target.view_features else None
                                if initial_view and initial_view.has_face and not initial_view.has_body:
                                    # å‡çº§åˆå§‹è§†è§’ï¼šæŠŠäººä½“ç‰¹å¾åŠ åˆ°åˆå§‹è§†è§’ä¸Š
                                    # åˆå¹¶ï¼šä¿ç•™åˆå§‹çš„äººè„¸ç‰¹å¾ + æ–°çš„äººä½“ç‰¹å¾
                                    if view.part_color_hists is not None:
                                        initial_view.part_color_hists = view.part_color_hists
                                        initial_view.timestamp = time.time()
                                        print(f"[åˆå§‹è§†è§’å‡çº§] ä»…äººè„¸â†’æœ‰äººä½“(F:{match_face_sim:.2f}, BM:{body_motion_combined:.2f})")
                                        should_learn = False  # å·²ç»å‡çº§ï¼Œä¸éœ€è¦å†å­¦ä¹ 
                                    else:
                                        should_learn = True
                                        learn_what = "body"
                                        learn_reason = f"é¦–æ¬¡å­¦ä¹ äººä½“(F:{match_face_sim:.2f}, BM:{body_motion_combined:.2f})"
                                else:
                                    should_learn = True
                                    learn_what = "body"
                                    learn_reason = f"äººè„¸åŒ¹é…(F:{match_face_sim:.2f})å­¦ä¹ body(BM:{body_motion_combined:.2f})"
                            
                            # Case 1b: ç›®æ ‡å·²æœ‰äººä½“è§†è§’ + äººè„¸åŒ¹é… + body+motioné«˜ â†’ å­¦ä¹ body
                            elif target_has_body_view and body_motion_combined >= BODY_MOTION_LEARN_THRESHOLD:
                                # å…³é”®çº¦æŸï¼šäººè„¸å¿…é¡»åœ¨äººä½“æ¡†å†…ï¼
                                if face_in_person:
                                    should_learn = True
                                    learn_what = "body"
                                    learn_reason = f"äººè„¸åŒ¹é…(F:{match_face_sim:.2f})å­¦ä¹ body(BM:{body_motion_combined:.2f})"
                                else:
                                    if frame_count % 30 == 0:
                                        print(f"[DEBUG] äººè„¸ä¸åœ¨äººä½“æ¡†å†…ï¼Œä¸å­¦ä¹ body")
                            elif match_face_sim >= FACE_LEARN_THRESHOLD_LOCAL and face_size_ok_for_learn:
                                # äººè„¸å¤Ÿé«˜ + å°ºå¯¸å¤Ÿå¤§ â†’ ç›´æ¥å­¦ä¹ å½“å‰è§†è§’
                                should_learn = True
                                learn_what = "face"
                                learn_reason = f"äººè„¸é«˜ç½®ä¿¡(F:{match_face_sim:.2f}, size={current_face_size_for_learn}px)"
                        
                        # Case 2: body+motionåŒ¹é…é€šè¿‡ â†’ å¯ä»¥å­¦ä¹ äººè„¸
                        # æ³¨æ„ï¼šæ–¹æ¡ˆDç¡®ä¿å¯åŠ¨æ—¶ä¸€å®šæœ‰äººè„¸ï¼Œæ‰€ä»¥ä¸éœ€è¦"é¦–æ¬¡å­¦ä¹ äººè„¸"é€»è¾‘
                        elif match_type == "body_motion":
                            # Case 2a: äººè„¸ç›¸ä¼¼åº¦å¤Ÿé«˜ â†’ å­¦ä¹ /æ›´æ–°äººè„¸
                            if face_in_person and match_face_sim >= FACE_MIN_FOR_BODY_LEARN and face_size_ok_for_learn:
                                # å…³é”®çº¦æŸï¼šäººè„¸å¿…é¡»åœ¨äººä½“æ¡†å†… ä¸” å°ºå¯¸è¶³å¤Ÿå¤§ï¼
                                should_learn = True
                                learn_what = "face"
                                learn_reason = f"body+motionåŒ¹é…(BM:{body_motion_combined:.2f})å­¦ä¹ face(F:{match_face_sim:.2f}, size={current_face_size_for_learn}px)"
                            
                            # Case 2b: æ— äººè„¸/èƒŒé¢/ä¾§é¢ â†’ å­¦ä¹ bodyè§†è§’
                            elif not face_in_person and body_motion_combined >= BODY_MOTION_LEARN_THRESHOLD:
                                # çº¯èƒŒé¢/ä¾§é¢ï¼Œå­¦ä¹ bodyè§†è§’
                                # æ³¨æ„ï¼šè¿™é‡Œ face_in_person=False å¯èƒ½æ˜¯ï¼š
                                #   1. çœŸæ­£çš„èƒŒé¢ï¼ˆæ²¡æœ‰äººè„¸æ£€æµ‹ï¼‰
                                #   2. äººè„¸æ£€æµ‹æ¼æ£€ï¼ˆç¬æ—¶ï¼‰
                                #   3. äººè„¸ä¸åœ¨äººä½“æ¡†å†…ï¼ˆæ£€æµ‹åç§»ï¼‰
                                should_learn = True
                                learn_what = "body"
                                reason_detail = "èƒŒé¢/æ— è„¸" if len(faces) == 0 else "è„¸ä¸åœ¨æ¡†å†…"
                                learn_reason = f"{reason_detail}åŒ¹é…(BM:{body_motion_combined:.2f})"
                    
                    # æ‰§è¡Œå­¦ä¹ ï¼ˆæ™®é€šæˆ–æ›¿æ¢æ¨¡å¼ï¼‰
                    if should_learn:
                        if use_replace_strategy:
                            # æ›¿æ¢ç­–ç•¥ï¼šæ‰¾åˆ°æœ€å·®/æœ€è€çš„è§†è§’æ›¿æ¢
                            # è¯„ä¼°å½“å‰è§†è§’è´¨é‡
                            current_quality = 0.0
                            if learn_what == "face" and match_face_sim >= FACE_LEARN_THRESHOLD_LOCAL:
                                current_quality = match_face_sim
                            elif learn_what == "body" and body_motion_combined >= BODY_MOTION_LEARN_THRESHOLD:
                                current_quality = body_motion_combined
                            
                            # åªæœ‰å½“å‰è§†è§’è´¨é‡å¤Ÿé«˜æ‰è€ƒè™‘æ›¿æ¢
                            if current_quality >= 0.75:  # æ›¿æ¢é—¨æ§›è¦é«˜
                                learned, op_info = mv_recognizer.auto_learn(view, persons[idx].bbox, True, replace_mode=True)
                                if learned:
                                    print(f"[æ›¿æ¢å­¦ä¹ ] {learn_reason} (quality={current_quality:.2f}) -> {op_info}")
                            else:
                                if frame_count % 60 == 0:
                                    print(f"[DEBUG] å½“å‰è´¨é‡({current_quality:.2f})<0.75ï¼Œä¸æ›¿æ¢")
                        else:
                            # æ™®é€šå­¦ä¹ æ¨¡å¼
                            learned, op_info = mv_recognizer.auto_learn(view, persons[idx].bbox, True)
                            if learned:
                                print(f"[è‡ªåŠ¨å­¦ä¹  F{frame_count}] {learn_reason} -> {op_info}")
            
            # 2. å¦‚æœäººä½“æ²¡åŒ¹é…åˆ°ï¼Œå°è¯•ä»…é€šè¿‡äººè„¸åŒ¹é…
            # ============================================
            # æ ¹æ®äººè„¸è´¨é‡ä½¿ç”¨ä¸åŒç­–ç•¥:
            #   - stable (é«˜è´¨é‡): çº¯äººè„¸åŒ¹é…ï¼Œé˜ˆå€¼0.70
            #   - unstable (ä¸­ç­‰): äººè„¸+motionè¾…åŠ©ï¼Œé˜ˆå€¼0.50
            #   - lost (ä½è´¨é‡): æ— æ³•åŒ¹é…ï¼Œç­‰å¾…äººä½“å‡ºç°
            # ============================================
            if not matched_any and faces and mv_recognizer.target and mv_recognizer.target.has_face_view:
                
                best_face_match = None
                best_face_sim = 0.0
                best_face_idx = -1
                best_view_idx = -1
                best_face_quality = 'lost'
                best_face_conf = 0.0
                best_face_size = 0
                
                # å¤šäººè„¸åœºæ™¯éœ€è¦æ›´ä¸¥æ ¼çš„é˜ˆå€¼
                multi_face_penalty = 0.05 if num_faces > 1 else 0.0
                    
                for face_idx, face in enumerate(faces):
                    fx1, fy1, fx2, fy2 = face.bbox.astype(int)
                    fc_x, fc_y = (fx1 + fx2) // 2, (fy1 + fy2) // 2
                    face_w = fx2 - fx1
                    face_h = fy2 - fy1
                    face_size = min(face_w, face_h)
                    face_conf = float(face.score) if hasattr(face, 'score') else 0.5
                    
                    # æ£€æŸ¥äººè„¸æ˜¯å¦åœ¨æŸä¸ªäººä½“æ¡†å†…
                    face_in_any_person = False
                    if len(persons) > 0:
                        for p_idx, person in enumerate(persons):
                            px1, py1, px2, py2 = person.bbox.astype(int)
                            if px1 <= fc_x <= px2 and py1 <= fc_y <= py2:
                                face_in_any_person = True
                                break
                    
                    # å¤šäººåœºæ™¯ï¼Œäººè„¸åœ¨ä¸åŒ¹é…çš„äººä½“æ¡†å†… â†’ é€šå¸¸è·³è¿‡
                    # ä½†ä¾‹å¤–ï¼šå¦‚æœæ‰€æœ‰äººä½“éƒ½æ˜¯å› ä¸º"äººè„¸æ˜ç¡®ä¸åŒ¹é…"è¢«æ‹’ç»çš„ï¼Œ
                    # è¯´æ˜ç›®æ ‡å¯èƒ½æ˜¯å¦ä¸€ä¸ªäººè„¸ï¼ˆæ£€æµ‹é”™ä½ï¼‰ï¼Œåº”è¯¥ç»§ç»­å°è¯•åŒ¹é…
                    all_rejected_by_face_mismatch = (persons_total_checked > 0 and 
                                                     persons_rejected_by_face_mismatch == persons_total_checked)
                    
                    if num_persons > 1 and face_in_any_person and not all_rejected_by_face_mismatch:
                        if frame_count % 30 == 0:
                            print(f"[DEBUG] Face[{face_idx}] åœ¨ä¸åŒ¹é…çš„äººä½“æ¡†å†…(å¤šäººåœºæ™¯)ï¼Œè·³è¿‡")
                        continue
                    elif num_persons > 1 and face_in_any_person and all_rejected_by_face_mismatch:
                        if frame_count % 30 == 0:
                            print(f"[DEBUG] Face[{face_idx}] æ‰€æœ‰äººä½“å› äººè„¸ä¸åŒ¹é…è¢«æ‹’ç»ï¼Œå°è¯•ä»…äººè„¸åŒ¹é…")
                    
                    # è¿œå¤„äººè„¸ä½¿ç”¨æ›´é«˜é˜ˆå€¼
                    is_distant_face = num_persons > 0 and not face_in_any_person
                    
                    face_feature = face_recognizer.extract_feature(
                        frame, face.bbox, face.keypoints
                    )
                    if face_feature and face_feature.embedding is not None:
                        # ä¸ç›®æ ‡äººè„¸ç‰¹å¾æ¯”è¾ƒï¼Œæ‰¾æœ€é«˜ç›¸ä¼¼åº¦
                        for vi, view in enumerate(mv_recognizer.target.view_features):
                            if view.has_face and view.face_embedding is not None:
                                sim = float(np.dot(face_feature.embedding, view.face_embedding))
                                
                                # è¯„ä¼°äººè„¸è´¨é‡
                                face_quality = evaluate_face_quality(face_conf, face_size, sim)
                                
                                # æ ¹æ®è´¨é‡å†³å®šé˜ˆå€¼
                                if face_quality == 'stable':
                                    current_threshold = FACE_ONLY_THRESHOLD + multi_face_penalty
                                    if is_distant_face:
                                        # é«˜ç›¸ä¼¼åº¦(>=0.75)å‡å°‘distantæƒ©ç½š
                                        if sim >= 0.75:
                                            current_threshold += 0.05  # å‡åŠæƒ©ç½š
                                        else:
                                            current_threshold += 0.10
                                elif face_quality == 'unstable':
                                    # ä¸ç¨³å®šäººè„¸: ä½¿ç”¨æ›´ä½é˜ˆå€¼ï¼Œä½†éœ€è¦motionè¾…åŠ©éªŒè¯
                                    current_threshold = FACE_ONLY_THRESHOLD_UNSTABLE + multi_face_penalty
                                    if is_distant_face:
                                        current_threshold += 0.05
                                else:
                                    current_threshold = 1.0  # æ— æ³•åŒ¹é…
                                
                                if frame_count % 30 == 0:
                                    print(f"[DEBUG] Face[{face_idx}] vs View[{vi}]: sim={sim:.3f}, conf={face_conf:.2f}, size={face_size}px, quality={face_quality}, threshold={current_threshold:.2f}")
                                
                                if sim >= current_threshold and sim > best_face_sim:
                                    best_face_sim = sim
                                    best_face_idx = face_idx
                                    best_view_idx = vi
                                    best_face_quality = face_quality
                                    best_face_conf = face_conf
                                    best_face_size = face_size
                
                # æ ¹æ®äººè„¸è´¨é‡å†³å®šæ˜¯å¦åŒ¹é…æˆåŠŸ
                face_match_success = False
                
                if best_face_quality == 'stable' and best_face_sim >= FACE_ONLY_THRESHOLD:
                    # ç¨³å®šäººè„¸: çº¯äººè„¸åŒ¹é…
                    face_match_success = True
                    if frame_count % 30 == 0:
                        print(f"[DEBUG] ç¨³å®šäººè„¸åŒ¹é…æˆåŠŸ! face_idx={best_face_idx}, sim={best_face_sim:.3f}")
                        
                elif best_face_quality == 'unstable' and best_face_sim >= FACE_ONLY_THRESHOLD_UNSTABLE:
                    # ä¸ç¨³å®šäººè„¸: éœ€è¦motionè¾…åŠ©éªŒè¯
                    # è·å–motionåˆ†æ•°ï¼ˆä½¿ç”¨æœ€è¿‘çš„ä½ç½®é¢„æµ‹ï¼‰
                    motion_score = 0.0
                    last_bbox = mv_recognizer.target.last_bbox if mv_recognizer.target else None
                    if last_bbox is not None and best_face_idx >= 0:
                        # è®¡ç®—äººè„¸æ¡†ä¸é¢„æµ‹ä½ç½®çš„IOU
                        face_bbox = faces[best_face_idx].bbox
                        pred_bbox = last_bbox
                        
                        # ç®€åŒ–: ç”¨ä¸­å¿ƒç‚¹è·ç¦»ä»£æ›¿IOU
                        fc_x = (face_bbox[0] + face_bbox[2]) / 2
                        fc_y = (face_bbox[1] + face_bbox[3]) / 2
                        pc_x = (pred_bbox[0] + pred_bbox[2]) / 2
                        pc_y = (pred_bbox[1] + pred_bbox[3]) / 2
                        
                        # è®¡ç®—å½’ä¸€åŒ–è·ç¦»
                        frame_h, frame_w = frame.shape[:2]
                        dist = np.sqrt((fc_x - pc_x)**2 + (fc_y - pc_y)**2)
                        max_dist = np.sqrt(frame_w**2 + frame_h**2) * 0.3  # å…è®¸30%ç”»é¢è·ç¦»
                        motion_score = max(0, 1.0 - dist / max_dist)
                    
                    # ä¸ç¨³å®šäººè„¸ + motionè¾…åŠ©
                    combined_score = best_face_sim * 0.6 + motion_score * 0.4
                    if combined_score >= 0.50:  # ç»¼åˆåˆ†æ•°é˜ˆå€¼
                        face_match_success = True
                        if frame_count % 30 == 0:
                            print(f"[DEBUG] ä¸ç¨³å®šäººè„¸+motionåŒ¹é…æˆåŠŸ! face={best_face_sim:.2f}, motion={motion_score:.2f}, combined={combined_score:.2f}")
                    else:
                        if frame_count % 30 == 0:
                            print(f"[DEBUG] ä¸ç¨³å®šäººè„¸+motionä¸è¶³ (face={best_face_sim:.2f}, motion={motion_score:.2f}, combined={combined_score:.2f}<0.50)")
                
                if face_match_success:
                    matched_any = True
                    target_face_idx = best_face_idx
                    lost_frames = 0
                    
                    current_match_info = {
                        'type': 'face_only',
                        'similarity': best_face_sim,
                        'method': f'face_only_{best_face_quality} (vs View[{best_view_idx}])',
                        'threshold': FACE_ONLY_THRESHOLD if best_face_quality == 'stable' else FACE_ONLY_THRESHOLD_UNSTABLE
                    }
                    
                    mv_recognizer.update_tracking(faces[best_face_idx].bbox)
                    
                    # ä»…äººè„¸åŒ¹é…æ—¶çš„è‡ªåŠ¨å­¦ä¹  - åªæœ‰ç¨³å®šäººè„¸æ‰å­¦ä¹ 
                    if best_face_quality == 'stable' and best_face_sim >= 0.80 and is_single_person_scene:
                        face_only_view = ViewFeature(timestamp=time.time())
                        face_feature = face_recognizer.extract_feature(
                            frame, faces[best_face_idx].bbox, faces[best_face_idx].keypoints
                        )
                        if face_feature:
                            face_only_view.has_face = True
                            face_only_view.face_embedding = face_feature.embedding
                            learned, op_info = mv_recognizer.auto_learn(face_only_view, faces[best_face_idx].bbox, True)
                            if learned:
                                print(f"[è‡ªåŠ¨å­¦ä¹  F{frame_count}] ä»…äººè„¸(sim={best_face_sim:.2f}) -> {op_info}")
                    elif frame_count % 30 == 0 and best_face_sim >= 0.60:
                        if best_face_quality == 'unstable':
                            print(f"[DEBUG] ä¸ç¨³å®šäººè„¸ä¸å­¦ä¹ ")
                        elif not is_single_person_scene:
                            print(f"[DEBUG] ä»…äººè„¸åŒ¹é…ä¸å­¦ä¹ : å¤šäººåœºæ™¯")
                        else:
                            print(f"[DEBUG] ä»…äººè„¸åŒ¹é…ä¸å­¦ä¹ : ç›¸ä¼¼åº¦ä¸è¶³({best_face_sim:.2f}<0.80)")
                            
                elif frame_count % 30 == 0 and best_face_sim > 0:
                    print(f"[DEBUG] äººè„¸åŒ¹é…å¤±è´¥: sim={best_face_sim:.3f}, quality={best_face_quality}")
            
            if not matched_any:
                lost_frames += 1
                # æ¸…ç©ºåŒ¹é…å†å²ï¼Œé˜²æ­¢è¯¯åŒ¹é…
                mv_recognizer.clear_match_history()
                if frame_count % 30 == 0:
                    print(f"[DEBUG F{frame_count}] æœªåŒ¹é…, lost_frames={lost_frames}/{max_lost_frames}")
                if lost_frames >= max_lost_frames:
                    state_machine.state = SystemState.LOST_TARGET
                    print("[ç›®æ ‡ä¸¢å¤±] ç­‰å¾…é‡æ–°å‡ºç°æˆ–æ‰‹åŠ¿åœæ­¢")
        
        elif state_machine.state == SystemState.LOST_TARGET:
            # å°è¯•é‡æ–°åŒ¹é… - ä½¿ç”¨æœ€ä½³åŒ¹é…ç­–ç•¥ + è¿ç»­å¸§ç¡®è®¤
            # å…³é”®ï¼šLOST_TARGET é‡æ–°é”å®šéœ€è¦è¿ç»­Nå¸§åŒ¹é…æˆåŠŸæ‰ç¡®è®¤
            
            # é‡æ–°é”å®šçš„é˜ˆå€¼ - é€‚åº¦é™ä½ä»¥æé«˜å¯ç”¨æ€§
            RELOCK_BODY_THRESHOLD = 0.75  # ä»…äººä½“æ—¶çš„é˜ˆå€¼
            RELOCK_FUSED_THRESHOLD = 0.65  # æœ‰äººè„¸æ—¶çš„ç»¼åˆé˜ˆå€¼
            RELOCK_FACE_SIM_THRESHOLD = 0.55  # äººè„¸ç›¸ä¼¼åº¦ä¸‹é™
            
            # å¤šäººåœºæ™¯ä¸‹ï¼Œå¿…é¡»æœ‰äººè„¸éªŒè¯æ‰èƒ½é‡æ–°é”å®š
            # æ³¨æ„ï¼šLOST_TARGET çŠ¶æ€éœ€è¦é‡æ–°è®¡ç®—åœºæ™¯ç±»å‹
            relock_is_multi_person = len(persons) > 1 or len(faces) > 1
            require_face_for_relock = relock_is_multi_person or (mv_recognizer.target and mv_recognizer.target.has_face_view)
            
            # å½“å‰å¸§æœ€ä½³åŒ¹é…
            current_best_match = None
            current_best_idx = -1
            
            for idx, person in enumerate(persons):
                view = extract_view_feature(
                    frame, person.bbox, faces, face_recognizer, enhanced_reid
                )
                
                # ä½¿ç”¨ return_details=True è·å–è¯¦ç»†ä¿¡æ¯
                result = mv_recognizer.is_same_target(
                    view, person.bbox, return_details=True
                )
                # è¿”å›å€¼æ˜¯ (is_match, similarity, method, details)
                is_match = result[0]
                similarity = result[1]
                method = result[2]
                details = result[3] if len(result) > 3 else {}
                
                # é‡æ–°é”å®šéœ€è¦æ›´ä¸¥æ ¼çš„éªŒè¯
                if is_match:
                    face_in_person = view.has_face and view.face_embedding is not None
                    
                    # ä» details ä¸­è·å–äººè„¸ç›¸ä¼¼åº¦
                    face_sim = details.get('face_sim', 0.0) if details else 0.0
                    
                    # æ£€æŸ¥æ˜¯å¦æ»¡è¶³é˜ˆå€¼è¦æ±‚
                    if ('fused' in method or 'face_priority' in method) and face_in_person:
                        # æœ‰äººè„¸éªŒè¯ï¼šæ£€æŸ¥äººè„¸ç›¸ä¼¼åº¦æ˜¯å¦è¶³å¤Ÿé«˜
                        if similarity >= RELOCK_FUSED_THRESHOLD and face_sim >= RELOCK_FACE_SIM_THRESHOLD:
                            if current_best_match is None or similarity > current_best_match[1]:
                                current_best_match = (idx, similarity, method, view, True, face_sim)
                                current_best_idx = idx
                    elif not require_face_for_relock and similarity >= RELOCK_BODY_THRESHOLD:
                        # ä»…äººä½“åŒ¹é…ï¼šåªåœ¨å•äººåœºæ™¯ä¸”ç›®æ ‡æ²¡æœ‰äººè„¸ç‰¹å¾æ—¶å…è®¸
                        if current_best_match is None or similarity > current_best_match[1]:
                            current_best_match = (idx, similarity, method, view, False, 0.0)
                            current_best_idx = idx
            
            # è¿ç»­å¸§ç¡®è®¤æœºåˆ¶
            if current_best_match:
                idx, similarity, method, view, has_face, face_sim = current_best_match
                
                # æ£€æŸ¥æ˜¯å¦ä¸ä¸Šä¸€å¸§å€™é€‰äººç›¸åŒ
                if current_best_idx == relock_candidate_idx:
                    relock_confirm_count += 1
                else:
                    # å€™é€‰äººå˜åŒ–ï¼Œé‡æ–°è®¡æ•°
                    relock_candidate_idx = current_best_idx
                    relock_confirm_count = 1
                
                if frame_count % 30 == 0:
                    print(f"[DEBUG] é‡æ–°é”å®šå€™é€‰: Person[{idx}], sim={similarity:.2f}, è¿ç»­å¸§={relock_confirm_count}/{RELOCK_CONFIRM_FRAMES}")
                
                # è¾¾åˆ°è¿ç»­å¸§è¦æ±‚ï¼Œç¡®è®¤é‡æ–°é”å®š
                if relock_confirm_count >= RELOCK_CONFIRM_FRAMES:
                    state_machine.state = SystemState.TRACKING
                    target_person_idx = idx
                    lost_frames = 0
                    relock_confirm_count = 0
                    relock_candidate_idx = -1
                    mv_recognizer.update_tracking(persons[idx].bbox)
                    relock_type = "äººä½“+äººè„¸" if has_face else "ä»…äººä½“"
                    if has_face:
                        print(f"[é‡æ–°é”å®š] ç›®æ ‡å·²æ¢å¤ ({relock_type}, sim={similarity:.2f}, face={face_sim:.2f}, è¿ç»­ç¡®è®¤)")
                    else:
                        print(f"[é‡æ–°é”å®š] ç›®æ ‡å·²æ¢å¤ ({relock_type}, sim={similarity:.2f}, è¿ç»­ç¡®è®¤)")
            else:
                # æ— åŒ¹é…ï¼Œé‡ç½®è¿ç»­å¸§è®¡æ•°
                if relock_confirm_count > 0:
                    relock_confirm_count = 0
                    relock_candidate_idx = -1
                    if frame_count % 30 == 0:
                        print(f"[DEBUG] é‡æ–°é”å®šå€™é€‰ä¸¢å¤±ï¼Œé‡ç½®è®¡æ•°")
            
            # ç¦ç”¨ä»…äººè„¸é‡æ–°é”å®š - å¤ªå®¹æ˜“è¯¯è¯†åˆ«è¿œå¤„çš„ç›¸ä¼¼äººè„¸
            # åªæœ‰å½“äººè„¸åœ¨äººä½“æ¡†å†…æ—¶æ‰èƒ½é€šè¿‡äººä½“+äººè„¸è”åˆåŒ¹é…æ¥é”å®š
            # åŸå› ï¼šä»…äººè„¸åŒ¹é…ç¼ºå°‘ä½ç½®ã€èº«ä½“ç‰¹å¾ç­‰å…³è”ä¿¡æ¯ï¼Œå®¹æ˜“è¯¯åŒ¹é…
            # if not matched_any and faces and mv_recognizer.target and mv_recognizer.target.has_face_view:
            #     for face_idx, face in enumerate(faces):
            #         ...
        
        # ============== ç»˜åˆ¶ ==============
        # â˜…â˜…â˜… ç»˜åˆ¶å‰æ—¥å¿—ï¼šæ˜ç¡® target_person_idx çš„å€¼ â˜…â˜…â˜…
        if frame_count % 30 == 0:
            print(f"\n[ç»˜åˆ¶] target_person_idx={target_person_idx}, state={state_machine.state.value}")
            for idx, person in enumerate(persons):
                px1, py1, px2, py2 = person.bbox.astype(int)
                is_target = (idx == target_person_idx)
                print(f"       Person[{idx}]: bbox=[{px1},{py1},{px2},{py2}], æ˜¯ç›®æ ‡={is_target}")
        
        # ç»˜åˆ¶äººä½“æ¡†
        for idx, person in enumerate(persons):
            px1, py1, px2, py2 = person.bbox.astype(int)
            
            if state_machine.state == SystemState.IDLE:
                color = (255, 165, 0)  # æ©™è‰²
                label = "Candidate"
            elif idx == target_person_idx:
                color = (0, 255, 0)  # ç»¿è‰²
                label = f"TARGET[{idx}]"  # æ ‡æ³¨ç´¢å¼•
            else:
                color = (0, 0, 255)  # çº¢è‰²
                label = f"Other[{idx}]"  # æ ‡æ³¨ç´¢å¼•
            
            cv2.rectangle(frame, (px1, py1), (px2, py2), color, 2)
            
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
            cv2.rectangle(frame, (px1, py1 - label_size[1] - 5),
                         (px1 + label_size[0], py1), color, -1)
            cv2.putText(frame, label, (px1, py1 - 3),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # ç»˜åˆ¶äººè„¸æ¡† - ä½¿ç”¨ä¹‹å‰åŒ¹é…è¿‡ç¨‹ä¸­çš„ç»“æœï¼Œé¿å…é‡å¤è®¡ç®—
        # target_face_idx æ˜¯ä»…äººè„¸åŒ¹é…æ—¶ç¡®å®šçš„ç›®æ ‡äººè„¸
        # å¯¹äºæœ‰äººä½“åŒ¹é…çš„æƒ…å†µï¼Œåªæœ‰ face_in_person=True ä¸”é€šè¿‡ face_priority éªŒè¯çš„æ‰æ ‡è®°ä¸ºç›®æ ‡
        for face_idx, face in enumerate(faces):
            fx1, fy1, fx2, fy2 = face.bbox.astype(int)
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºç›®æ ‡äººè„¸
            # å…³é”®ï¼šä¸èƒ½åªçœ‹æ˜¯å¦åœ¨ç›®æ ‡äººä½“æ¡†å†…ï¼Œå¿…é¡»æ˜¯åŒ¹é…è¿‡ç¨‹ä¸­éªŒè¯è¿‡çš„
            # ä½¿ç”¨ current_match_info æ¥åˆ¤æ–­æ˜¯å¦ç»è¿‡äº†äººè„¸éªŒè¯
            is_target_face = False
            
            if target_person_idx >= 0 and target_person_idx < len(persons):
                px1, py1, px2, py2 = persons[target_person_idx].bbox
                fc_x, fc_y = (fx1 + fx2) // 2, (fy1 + fy2) // 2
                if px1 <= fc_x <= px2 and py1 <= fc_y <= py2:
                    # äººè„¸åœ¨ç›®æ ‡äººä½“æ¡†å†…
                    # åªæœ‰å½“åŒ¹é…æ–¹æ³•åŒ…å« face_priority æˆ– fused æ—¶ï¼Œæ‰è¡¨ç¤ºäººè„¸å·²éªŒè¯
                    if current_match_info:
                        method = current_match_info.get('method', '')
                        if 'face_priority' in method or 'fused' in method:
                            # äººè„¸å·²ç»é€šè¿‡éªŒè¯
                            is_target_face = True
                        # å¦åˆ™æ˜¯çº¯äººä½“åŒ¹é…ï¼Œä¸èƒ½ç¡®å®šäººè„¸æ˜¯å¦å±äºç›®æ ‡
                    # æ³¨æ„ï¼šä¸å†ä½¿ç”¨ "åªæœ‰ä¸€ä¸ªäººè„¸åœ¨æ¡†å†…å°±è®¤ä¸ºæ˜¯ç›®æ ‡" çš„é€»è¾‘
                    # å› ä¸ºåœ¨é®æŒ¡åœºæ™¯ä¸‹ï¼Œé®æŒ¡è€…çš„äººè„¸å¯èƒ½æ­£å¥½åœ¨ç›®æ ‡äººä½“æ¡†å†…
                    # è¿™ä¼šå¯¼è‡´é”™è¯¯çš„ç»¿æ¡†
            
            # äººè„¸æ¡†ç»˜åˆ¶æ—¥å¿—
            if frame_count % 30 == 0:
                print(f"       Face[{face_idx}]: is_target_face={is_target_face}, target_face_idx={target_face_idx}")
            
            if face_idx == target_face_idx and target_person_idx < 0:
                # ä»…äººè„¸åŒ¹é…çš„ç›®æ ‡
                cv2.rectangle(frame, (fx1, fy1), (fx2, fy2), (0, 255, 0), 2)
                cv2.putText(frame, "TARGET(Face)", (fx1, fy1 - 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
            elif is_target_face:
                # ç›®æ ‡äººä½“å†…çš„äººè„¸ - ç”¨ç»¿è‰²é«˜äº®
                cv2.rectangle(frame, (fx1, fy1), (fx2, fy2), (0, 255, 0), 2)
            elif state_machine.state == SystemState.IDLE:
                # ç©ºé—²çŠ¶æ€æ˜¾ç¤ºæ‰€æœ‰äººè„¸
                cv2.rectangle(frame, (fx1, fy1), (fx2, fy2), (255, 200, 0), 1)
            else:
                # è·Ÿè¸ªçŠ¶æ€æ˜¾ç¤ºéç›®æ ‡äººè„¸ï¼ˆæ·¡è‰²ï¼‰
                cv2.rectangle(frame, (fx1, fy1), (fx2, fy2), (128, 128, 128), 1)
        
        # ç»˜åˆ¶æ‰‹åŠ¿æŒ‡ç¤ºå™¨ (å«è¿›åº¦æ¡)
        draw_gesture_indicator(frame, gesture, state_machine.state, hold_progress)
        
        # çŠ¶æ€ä¿¡æ¯
        target_info = "None"
        if mv_recognizer.target:
            num_views = mv_recognizer.target.num_views
            # ç»Ÿè®¡æœ‰äººè„¸å’Œæœ‰äººä½“çš„è§†è§’æ•°é‡
            face_views = sum(1 for v in mv_recognizer.target.view_features if v.has_face)
            body_views = sum(1 for v in mv_recognizer.target.view_features if v.part_color_hists is not None)
            target_info = f"Views={num_views} (F:{face_views} B:{body_views})"
        
        # åŒ¹é…ä¿¡æ¯
        match_info = ""
        if current_match_info:
            sim = current_match_info['similarity']
            thresh = current_match_info['threshold']
            mtype = current_match_info['type']
            match_info = f"Match: {mtype} sim={sim:.2f} (>={thresh:.2f})"
        
        info_lines = [
            f"FPS: {fps:.1f}",
            f"State: {state_machine.state.value}",
            f"Persons: {len(persons)}, Faces: {len(faces)}",
            f"Target: {target_info}",
            match_info,
            f"Gesture: {gesture.gesture_type.value}" + (f" ({hold_progress*100:.0f}%)" if hold_progress > 0 else "")
        ]
        
        for i, line in enumerate(info_lines):
            if line:
                # åŒ¹é…ä¿¡æ¯ç”¨ä¸åŒé¢œè‰²
                color = (0, 255, 255) if "Match:" in line else (0, 255, 0)
                cv2.putText(frame, line, (10, 25 + i * 20),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 1)
        
        # æ‰‹åŠ¿æç¤º
        if state_machine.state == SystemState.IDLE:
            cv2.putText(frame, f"Hold OPEN PALM {GESTURE_HOLD_DURATION:.0f}s to START", 
                       (10, h - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        elif state_machine.state == SystemState.TRACKING:
            cv2.putText(frame, f"Hold OPEN PALM {GESTURE_HOLD_DURATION:.0f}s to STOP", 
                       (10, h - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        elif state_machine.state == SystemState.LOST_TARGET:
            cv2.putText(frame, f"Target LOST - Hold PALM to STOP", 
                       (10, h - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 165, 255), 2)
        
        cv2.imshow(window_name, frame)
        
        # é”®ç›˜æ§åˆ¶
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            break
        elif key == ord('s'):
            if persons:
                nearest, _ = find_nearest_person(persons, frame_center)
                if nearest:
                    view = extract_view_feature(
                        frame, nearest.bbox, faces, face_recognizer, enhanced_reid
                    )
                    mv_recognizer.set_target(view, nearest.bbox)
                    state_machine.state = SystemState.TRACKING
                    print("[æ‰‹åŠ¨ä¿å­˜] ç›®æ ‡å·²é”å®š")
        elif key == ord('a'):
            if mv_recognizer.target and target_person_idx >= 0:
                person = persons[target_person_idx]
                view = extract_view_feature(
                    frame, person.bbox, faces, face_recognizer, enhanced_reid
                )
                if mv_recognizer.target._is_different_view(view, 0.75):
                    mv_recognizer.target.view_features.append(view)
                    print(f"[æ‰‹åŠ¨æ·»åŠ ] æ–°è§†è§’, æ€»æ•°: {mv_recognizer.target.num_views}")
        elif key == ord('c'):
            mv_recognizer.clear_target()
            state_machine.state = SystemState.IDLE
            print("[æ‰‹åŠ¨æ¸…é™¤] ç›®æ ‡å·²æ¸…é™¤")
        elif key == ord('m'):
            mv_config.auto_learn = not mv_config.auto_learn
            print(f"[è‡ªåŠ¨å­¦ä¹ ] {'å¼€å¯' if mv_config.auto_learn else 'å…³é—­'}")
    
    gesture_detector.release()
    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    main()
