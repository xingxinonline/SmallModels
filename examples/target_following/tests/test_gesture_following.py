"""
æ‰‹åŠ¿æ§åˆ¶ç›®æ ‡è·Ÿéšæµ‹è¯•
Gesture-Controlled Target Following

æ‰‹åŠ¿æ§åˆ¶:
  - ğŸ‘‹ å¼ å¼€æ‰‹æŒæŒç»­3ç§’: Toggle å¯åŠ¨/åœæ­¢è·Ÿéš
    - ç©ºé—²çŠ¶æ€ â†’ å¯åŠ¨è·Ÿéš (é”å®šæœ€è¿‘çš„äºº)
    - è·Ÿè¸ªçŠ¶æ€ â†’ åœæ­¢è·Ÿéš (æ¸…é™¤ç›®æ ‡)

ç³»ç»ŸçŠ¶æ€:
  - IDLE: ç©ºé—²çŠ¶æ€ï¼Œç­‰å¾…æ‰‹åŠ¿å¯åŠ¨
  - TRACKING: è·Ÿéšä¸­ï¼ŒæŒç»­è·Ÿè¸ªç›®æ ‡
  - LOST_TARGET: ç›®æ ‡ä¸¢å¤±ï¼Œç­‰å¾…é‡æ–°å‡ºç°æˆ–æ‰‹åŠ¿åœæ­¢

é”®ç›˜æ§åˆ¶ (å¤‡ç”¨):
  - 's': æ‰‹åŠ¨ä¿å­˜ç›®æ ‡
  - 'a': æ‰‹åŠ¨æ·»åŠ è§†è§’
  - 'c': æ‰‹åŠ¨æ¸…é™¤ç›®æ ‡
  - 'm': åˆ‡æ¢è‡ªåŠ¨å­¦ä¹ 
  - 'q': é€€å‡º
"""

import cv2
import numpy as np
import sys
import os
import time

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import (
    MODELS_DIR, GestureType, GestureConfig, SystemState,
    YOLOv5PersonConfig, FaceDetectorConfig, MobileFaceNetConfig
)
from detectors.yolov5_person_detector import YOLOv5PersonDetector
from detectors.face_detector import FaceDetector
from detectors.mobilefacenet_recognizer import MobileFaceNetRecognizer
from detectors.gesture_detector import GestureDetector, GestureResult
from detectors.enhanced_reid import EnhancedReIDExtractor, EnhancedReIDConfig
from detectors.multiview_recognizer import (
    MultiViewRecognizer, MultiViewConfig, ViewFeature
)
from core.state_machine import StateMachine


# æ‰‹åŠ¿é…ç½®
GESTURE_HOLD_DURATION = 3.0  # è§¦å‘éœ€è¦ä¿æŒçš„ç§’æ•°
GESTURE_COOLDOWN_SECONDS = 10.0  # è§¦å‘åå†·å´ç§’æ•° (é˜²æ­¢è¿ç»­è§¦å‘)


def extract_view_feature(
    frame: np.ndarray,
    person_bbox: np.ndarray,
    faces: list,
    face_recognizer,
    enhanced_reid
) -> ViewFeature:
    """æå–è§†è§’ç‰¹å¾"""
    view = ViewFeature(timestamp=time.time())
    
    px1, py1, px2, py2 = person_bbox.astype(int)
    
    # æŸ¥æ‰¾äººè„¸
    for face in faces:
        fx1, fy1, fx2, fy2 = face.bbox.astype(int)
        fc_x, fc_y = (fx1 + fx2) // 2, (fy1 + fy2) // 2
        
        if px1 <= fc_x <= px2 and py1 <= fc_y <= py2:
            face_feature = face_recognizer.extract_feature(
                frame, face.bbox, face.keypoints
            )
            if face_feature:
                view.has_face = True
                view.face_embedding = face_feature.embedding
            break
    
    # äººä½“ç‰¹å¾
    body_feature = enhanced_reid.extract_feature(frame, person_bbox)
    if body_feature:
        view.part_color_hists = body_feature.part_color_hists
        view.part_lbp_hists = body_feature.part_lbp_hists
        view.geometry = body_feature.geometry
    
    return view


def find_nearest_person(persons: list, frame_center: tuple):
    """æ‰¾åˆ°ç¦»ç”»é¢ä¸­å¿ƒæœ€è¿‘çš„äºº"""
    if not persons:
        return None, -1
    
    min_dist = float('inf')
    nearest_idx = 0
    
    for i, person in enumerate(persons):
        px1, py1, px2, py2 = person.bbox
        cx = (px1 + px2) / 2
        cy = (py1 + py2) / 2
        dist = (cx - frame_center[0])**2 + (cy - frame_center[1])**2
        if dist < min_dist:
            min_dist = dist
            nearest_idx = i
    
    return persons[nearest_idx], nearest_idx


def draw_gesture_indicator(frame, gesture: GestureResult, state: SystemState, hold_progress: float = 0.0):
    """ç»˜åˆ¶æ‰‹åŠ¿æŒ‡ç¤ºå™¨"""
    h, w = frame.shape[:2]
    
    # ç»˜åˆ¶æ‰‹éƒ¨æ¡†
    if gesture.hand_bbox is not None:
        hx1, hy1, hx2, hy2 = gesture.hand_bbox.astype(int)
        
        if gesture.gesture_type == GestureType.OPEN_PALM:
            color = (0, 255, 255)  # é»„è‰²
            if state == SystemState.IDLE:
                text = "HOLD TO START"
            else:
                text = "HOLD TO STOP"
        else:
            color = (255, 165, 0)
            text = ""
        
        cv2.rectangle(frame, (hx1, hy1), (hx2, hy2), color, 2)
        if text:
            cv2.putText(frame, text, (hx1, hy1 - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        
        # ç»˜åˆ¶æŒç»­è¿›åº¦æ¡
        if hold_progress > 0:
            bar_width = hx2 - hx1
            bar_height = 8
            bar_y = hy2 + 5
            
            # èƒŒæ™¯
            cv2.rectangle(frame, (hx1, bar_y), (hx2, bar_y + bar_height), (50, 50, 50), -1)
            # è¿›åº¦
            progress_width = int(bar_width * hold_progress)
            progress_color = (0, 255, 0) if hold_progress < 1.0 else (0, 255, 255)
            cv2.rectangle(frame, (hx1, bar_y), (hx1 + progress_width, bar_y + bar_height), progress_color, -1)
            # è¾¹æ¡†
            cv2.rectangle(frame, (hx1, bar_y), (hx2, bar_y + bar_height), (255, 255, 255), 1)
            
            # è¿›åº¦ç™¾åˆ†æ¯”
            pct_text = f"{int(hold_progress * 100)}%"
            cv2.putText(frame, pct_text, (hx1, bar_y + bar_height + 15),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
    
    # çŠ¶æ€æŒ‡ç¤ºå™¨ (å³ä¸Šè§’)
    state_colors = {
        SystemState.IDLE: (128, 128, 128),          # ç°è‰²
        SystemState.TRACKING: (0, 255, 0),          # ç»¿è‰²
        SystemState.LOST_TARGET: (0, 165, 255)      # æ©™è‰²
    }
    state_color = state_colors.get(state, (255, 255, 255))
    
    cv2.circle(frame, (w - 30, 30), 15, state_color, -1)
    cv2.putText(frame, state.value, (w - 120, 55),
               cv2.FONT_HERSHEY_SIMPLEX, 0.5, state_color, 1)


def main():
    print("=" * 60)
    print("    æ‰‹åŠ¿æ§åˆ¶ç›®æ ‡è·Ÿéšç³»ç»Ÿ")
    print("=" * 60)
    
    # æ£€æŸ¥æ¨¡å‹
    yolo_path = os.path.join(MODELS_DIR, "yolov5n.onnx")
    scrfd_path = os.path.join(MODELS_DIR, "scrfd_500m_bnkps.onnx")
    mobilefacenet_path = os.path.join(MODELS_DIR, "mobilefacenet.onnx")
    
    missing = []
    if not os.path.exists(yolo_path):
        missing.append("yolov5n.onnx")
    if not os.path.exists(scrfd_path):
        missing.append("scrfd_500m_bnkps.onnx")
    if not os.path.exists(mobilefacenet_path):
        missing.append("mobilefacenet.onnx")
    
    if missing:
        print(f"\n[é”™è¯¯] ç¼ºå°‘æ¨¡å‹: {missing}")
        return
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    person_detector = YOLOv5PersonDetector(YOLOv5PersonConfig(model_path=yolo_path))
    face_detector = FaceDetector(FaceDetectorConfig(model_path=scrfd_path))
    face_recognizer = MobileFaceNetRecognizer(MobileFaceNetConfig(model_path=mobilefacenet_path))
    
    # æ‰‹åŠ¿æ£€æµ‹å™¨ (confirm_frames=1ï¼Œå› ä¸ºæŒç»­æ—¶é—´æ£€æµ‹åœ¨çŠ¶æ€æœºä¸­)
    gesture_config = GestureConfig(
        min_detection_confidence=0.7,
        min_tracking_confidence=0.5,
        gesture_confirm_frames=1  # ç«‹å³å“åº”ï¼ŒæŒç»­æ—¶é—´ç”±çŠ¶æ€æœºæ§åˆ¶
    )
    gesture_detector = GestureDetector(gesture_config)
    
    # å¢å¼ºç‰ˆ ReID
    enhanced_reid = EnhancedReIDExtractor(EnhancedReIDConfig(
        num_horizontal_parts=6,
        use_lbp=True,
        use_geometry=True
    ))
    
    # å¤šè§†è§’è¯†åˆ«å™¨
    mv_config = MultiViewConfig(
        face_weight=0.6,
        body_weight=0.4,
        face_threshold=0.45,
        body_threshold=0.45,
        fused_threshold=0.45,
        motion_weight=0.20,
        auto_learn=True,
        learn_interval=2.0,
        smooth_window=5,
        confirm_threshold=3,
        part_weights=[0.05, 0.12, 0.20, 0.20, 0.25, 0.18]
    )
    mv_recognizer = MultiViewRecognizer(mv_config)
    
    # åŠ è½½æ¨¡å‹
    if not person_detector.load():
        print("[é”™è¯¯] äººä½“æ£€æµ‹å™¨åŠ è½½å¤±è´¥")
        return
    if not face_detector.load():
        print("[é”™è¯¯] äººè„¸æ£€æµ‹å™¨åŠ è½½å¤±è´¥")
        return
    if not face_recognizer.load():
        print("[é”™è¯¯] äººè„¸è¯†åˆ«å™¨åŠ è½½å¤±è´¥")
        return
    if not gesture_detector.load():
        print("[é”™è¯¯] æ‰‹åŠ¿æ£€æµ‹å™¨åŠ è½½å¤±è´¥")
        return
    
    enhanced_reid.load()
    
    # æ‰“å¼€æ‘„åƒå¤´
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("[é”™è¯¯] æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
        return
    
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    print("\n[æ‰‹åŠ¿æ§åˆ¶]")
    print(f"  ğŸ‘‹ å¼ å¼€æ‰‹æŒæŒç»­ {GESTURE_HOLD_DURATION:.0f} ç§’: Toggle å¯åŠ¨/åœæ­¢è·Ÿéš")
    print("\n[é”®ç›˜æ§åˆ¶]")
    print("  's': æ‰‹åŠ¨ä¿å­˜ç›®æ ‡")
    print("  'a': æ·»åŠ è§†è§’")
    print("  'c': æ¸…é™¤ç›®æ ‡")
    print("  'm': åˆ‡æ¢è‡ªåŠ¨å­¦ä¹ ")
    print("  'q': é€€å‡º")
    print()
    
    # çŠ¶æ€æœº (ä½¿ç”¨ä¹‹å‰å®ç°çš„æŒç»­æ—¶é—´æ£€æµ‹)
    state_machine = StateMachine(
        lost_timeout_frames=30,
        gesture_hold_duration=GESTURE_HOLD_DURATION,
        gesture_cooldown_seconds=GESTURE_COOLDOWN_SECONDS
    )
    
    lost_frames = 0
    max_lost_frames = 30
    
    frame_count = 0
    fps_start = time.time()
    fps = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame_count += 1
        h, w = frame.shape[:2]
        frame_center = (w // 2, h // 2)
        
        # è®¡ç®— FPS
        if frame_count % 30 == 0:
            fps = 30 / (time.time() - fps_start)
            fps_start = time.time()
        
        # æ£€æµ‹
        persons = person_detector.detect(frame)
        faces = face_detector.detect(frame)
        gesture = gesture_detector.detect(frame)
        
        # ============== æ‰‹åŠ¿çŠ¶æ€æœº (æŒç»­æ—¶é—´æ£€æµ‹) ==============
        current_time = time.time()
        old_state = state_machine.state
        
        # å¤„ç†æ‰‹åŠ¿ (éœ€è¦æŒç»­ GESTURE_HOLD_DURATION ç§’)
        state_changed = state_machine.process_gesture(gesture.gesture_type, current_time, debug=False)
        
        # è·å–æŒç»­è¿›åº¦
        hold_progress = state_machine.get_gesture_hold_progress()
        
        # çŠ¶æ€å˜æ›´å¤„ç†
        if state_changed:
            if state_machine.state == SystemState.TRACKING and old_state == SystemState.IDLE:
                # å¯åŠ¨è·Ÿéš
                nearest_person, idx = find_nearest_person(persons, frame_center)
                if nearest_person is not None:
                    view = extract_view_feature(
                        frame, nearest_person.bbox, faces, 
                        face_recognizer, enhanced_reid
                    )
                    mv_recognizer.set_target(view, nearest_person.bbox)
                    lost_frames = 0
                    face_str = "æœ‰äººè„¸" if view.has_face else "æ— äººè„¸"
                    print(f"[æ‰‹åŠ¿å¯åŠ¨] ç›®æ ‡å·²é”å®š ({face_str})")
                else:
                    # æ²¡æœ‰äººï¼Œå›åˆ°ç©ºé—²
                    state_machine.state = SystemState.IDLE
                    print("[æç¤º] æœªæ£€æµ‹åˆ°äººä½“ï¼Œæ— æ³•å¯åŠ¨")
            
            elif state_machine.state == SystemState.IDLE and old_state in [SystemState.TRACKING, SystemState.LOST_TARGET]:
                # åœæ­¢è·Ÿéš
                mv_recognizer.clear_target()
                lost_frames = 0
                print("[æ‰‹åŠ¿åœæ­¢] è·Ÿéšå·²åœæ­¢")
        
        # ============== ç›®æ ‡è·Ÿè¸ª ==============
        target_person_idx = -1
        
        if state_machine.state == SystemState.TRACKING:
            matched_any = False
            
            for idx, person in enumerate(persons):
                view = extract_view_feature(
                    frame, person.bbox, faces, face_recognizer, enhanced_reid
                )
                
                is_match, similarity, method = mv_recognizer.is_same_target(
                    view, person.bbox
                )
                
                if is_match:
                    matched_any = True
                    target_person_idx = idx
                    lost_frames = 0
                    
                    # æ›´æ–°è·Ÿè¸ª
                    mv_recognizer.update_tracking(person.bbox)
                    
                    # è‡ªåŠ¨å­¦ä¹ 
                    if mv_recognizer.auto_learn(view, person.bbox, True):
                        print(f"[è‡ªåŠ¨å­¦ä¹ ] æ–°è§†è§’, æ€»æ•°: {mv_recognizer.target.num_views}")
                    break
            
            if not matched_any:
                lost_frames += 1
                if lost_frames >= max_lost_frames:
                    state_machine.state = SystemState.LOST_TARGET
                    print("[ç›®æ ‡ä¸¢å¤±] ç­‰å¾…é‡æ–°å‡ºç°æˆ–æ‰‹åŠ¿åœæ­¢")
        
        elif state_machine.state == SystemState.LOST_TARGET:
            # å°è¯•é‡æ–°åŒ¹é…
            for idx, person in enumerate(persons):
                view = extract_view_feature(
                    frame, person.bbox, faces, face_recognizer, enhanced_reid
                )
                
                is_match, similarity, method = mv_recognizer.is_same_target(
                    view, person.bbox
                )
                
                if is_match:
                    state_machine.state = SystemState.TRACKING
                    target_person_idx = idx
                    lost_frames = 0
                    mv_recognizer.update_tracking(person.bbox)
                    print("[é‡æ–°é”å®š] ç›®æ ‡å·²æ¢å¤")
                    break
        
        # ============== ç»˜åˆ¶ ==============
        for idx, person in enumerate(persons):
            px1, py1, px2, py2 = person.bbox.astype(int)
            
            if state_machine.state == SystemState.IDLE:
                color = (255, 165, 0)  # æ©™è‰²
                label = "Candidate"
            elif idx == target_person_idx:
                color = (0, 255, 0)  # ç»¿è‰²
                label = "TARGET"
            else:
                color = (0, 0, 255)  # çº¢è‰²
                label = "Other"
            
            cv2.rectangle(frame, (px1, py1), (px2, py2), color, 2)
            
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
            cv2.rectangle(frame, (px1, py1 - label_size[1] - 5),
                         (px1 + label_size[0], py1), color, -1)
            cv2.putText(frame, label, (px1, py1 - 3),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # ç»˜åˆ¶æ‰‹åŠ¿æŒ‡ç¤ºå™¨ (å«è¿›åº¦æ¡)
        draw_gesture_indicator(frame, gesture, state_machine.state, hold_progress)
        
        # çŠ¶æ€ä¿¡æ¯
        target_info = "None"
        if mv_recognizer.target:
            num_views = mv_recognizer.target.num_views
            has_face = "Y" if mv_recognizer.target.has_face_view else "N"
            target_info = f"Views={num_views}, Face={has_face}"
        
        info_lines = [
            f"FPS: {fps:.1f}",
            f"State: {state_machine.state.value}",
            f"Persons: {len(persons)}",
            f"Target: {target_info}",
            f"Gesture: {gesture.gesture_type.value}",
            f"Hold: {hold_progress*100:.0f}%" if hold_progress > 0 else ""
        ]
        
        for i, line in enumerate(info_lines):
            if line:
                cv2.putText(frame, line, (10, 25 + i * 20),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 1)
        
        # æ‰‹åŠ¿æç¤º
        if state_machine.state == SystemState.IDLE:
            cv2.putText(frame, f"Hold OPEN PALM {GESTURE_HOLD_DURATION:.0f}s to START", 
                       (10, h - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        elif state_machine.state == SystemState.TRACKING:
            cv2.putText(frame, f"Hold OPEN PALM {GESTURE_HOLD_DURATION:.0f}s to STOP", 
                       (10, h - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        elif state_machine.state == SystemState.LOST_TARGET:
            cv2.putText(frame, f"Target LOST - Hold PALM to STOP", 
                       (10, h - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 165, 255), 2)
        
        cv2.imshow("Gesture-Controlled Following", frame)
        
        # é”®ç›˜æ§åˆ¶
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            break
        elif key == ord('s'):
            if persons:
                nearest, _ = find_nearest_person(persons, frame_center)
                if nearest:
                    view = extract_view_feature(
                        frame, nearest.bbox, faces, face_recognizer, enhanced_reid
                    )
                    mv_recognizer.set_target(view, nearest.bbox)
                    state_machine.state = SystemState.TRACKING
                    print("[æ‰‹åŠ¨ä¿å­˜] ç›®æ ‡å·²é”å®š")
        elif key == ord('a'):
            if mv_recognizer.target and target_person_idx >= 0:
                person = persons[target_person_idx]
                view = extract_view_feature(
                    frame, person.bbox, faces, face_recognizer, enhanced_reid
                )
                if mv_recognizer.target._is_different_view(view, 0.75):
                    mv_recognizer.target.view_features.append(view)
                    print(f"[æ‰‹åŠ¨æ·»åŠ ] æ–°è§†è§’, æ€»æ•°: {mv_recognizer.target.num_views}")
        elif key == ord('c'):
            mv_recognizer.clear_target()
            state_machine.state = SystemState.IDLE
            print("[æ‰‹åŠ¨æ¸…é™¤] ç›®æ ‡å·²æ¸…é™¤")
        elif key == ord('m'):
            mv_config.auto_learn = not mv_config.auto_learn
            print(f"[è‡ªåŠ¨å­¦ä¹ ] {'å¼€å¯' if mv_config.auto_learn else 'å…³é—­'}")
    
    gesture_detector.release()
    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    main()
