"""
æ‰‹åŠ¿æ§åˆ¶ç›®æ ‡è·Ÿéšæµ‹è¯•
Gesture-Controlled Target Following

æ‰‹åŠ¿æ§åˆ¶:
  - ğŸ‘‹ å¼ å¼€æ‰‹æŒæŒç»­3ç§’: Toggle å¯åŠ¨/åœæ­¢è·Ÿéš
    - ç©ºé—²çŠ¶æ€ â†’ å¯åŠ¨è·Ÿéš (é”å®šæœ€è¿‘çš„äºº)
    - è·Ÿè¸ªçŠ¶æ€ â†’ åœæ­¢è·Ÿéš (æ¸…é™¤ç›®æ ‡)

ç³»ç»ŸçŠ¶æ€:
  - IDLE: ç©ºé—²çŠ¶æ€ï¼Œç­‰å¾…æ‰‹åŠ¿å¯åŠ¨
  - TRACKING: è·Ÿéšä¸­ï¼ŒæŒç»­è·Ÿè¸ªç›®æ ‡
  - LOST_TARGET: ç›®æ ‡ä¸¢å¤±ï¼Œç­‰å¾…é‡æ–°å‡ºç°æˆ–æ‰‹åŠ¿åœæ­¢

é”®ç›˜æ§åˆ¶ (å¤‡ç”¨):
  - 's': æ‰‹åŠ¨ä¿å­˜ç›®æ ‡
  - 'a': æ‰‹åŠ¨æ·»åŠ è§†è§’
  - 'c': æ‰‹åŠ¨æ¸…é™¤ç›®æ ‡
  - 'm': åˆ‡æ¢è‡ªåŠ¨å­¦ä¹ 
  - 'q': é€€å‡º
"""

import cv2
import numpy as np
import sys
import os
import time

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import (
    MODELS_DIR, GestureType, GestureConfig, SystemState,
    YOLOv5PersonConfig, FaceDetectorConfig, MobileFaceNetConfig
)
from detectors.yolov5_person_detector import YOLOv5PersonDetector
from detectors.face_detector import FaceDetector
from detectors.mobilefacenet_recognizer import MobileFaceNetRecognizer
from detectors.gesture_detector import GestureDetector, GestureResult
from detectors.enhanced_reid import EnhancedReIDExtractor, EnhancedReIDConfig
from detectors.multiview_recognizer import (
    MultiViewRecognizer, MultiViewConfig, ViewFeature
)
from core.state_machine import StateMachine


# æ‰‹åŠ¿é…ç½®
GESTURE_HOLD_DURATION = 3.0  # è§¦å‘éœ€è¦ä¿æŒçš„ç§’æ•°
GESTURE_COOLDOWN_SECONDS = 3.0  # è§¦å‘åå†·å´ç§’æ•° (é˜²æ­¢è¿ç»­è§¦å‘)

# ä»…äººè„¸åŒ¹é…é˜ˆå€¼
# é—®é¢˜ï¼šä¸åŒäººä¹‹é—´ä¹Ÿå¯èƒ½æœ‰ 0.55-0.65 çš„ç›¸ä¼¼åº¦
# è§£å†³ï¼šæé«˜é˜ˆå€¼åˆ° 0.70ï¼Œç‰ºç‰²ä¸€äº›å¬å›ç‡æ¢å–ç²¾ç¡®ç‡
FACE_ONLY_THRESHOLD = 0.70

# è‡ªåŠ¨å­¦ä¹ é˜ˆå€¼
# - å¤šäººåœºæ™¯ï¼šéœ€è¦äººè„¸éªŒè¯ + é«˜é˜ˆå€¼ï¼ˆé˜²æ­¢å­¦ä¹ é”™è¯¯äººè„¸ï¼‰
# - å•äººåœºæ™¯ï¼šå¯æ”¾å®½
# å…³é”®ä¿®å¤ï¼š0.65 å¤ªä½ä¼šåœ¨å¤šäººåœºæ™¯å­¦ä¹ åˆ°ä»–äººäººè„¸ï¼Œå¯¼è‡´ç›®æ ‡åˆ‡æ¢
FACE_LEARN_THRESHOLD = 0.72  # äººè„¸åŒ¹é…å­¦ä¹ é˜ˆå€¼ (æé«˜ä»¥é˜²æ­¢å­¦ä¹ é”™è¯¯äººè„¸)
FACE_LEARN_THRESHOLD_MULTI = 0.78  # å¤šäººåœºæ™¯ä¸‹çš„äººè„¸å­¦ä¹ é˜ˆå€¼ï¼ˆæ›´ä¸¥æ ¼ï¼‰
BODY_LEARN_THRESHOLD = 0.68  # äººä½“åŒ¹é…å­¦ä¹ é˜ˆå€¼ï¼ˆæé«˜ï¼‰

# é‡æ–°é”å®šé˜ˆå€¼ - ä»ä¸¢å¤±çŠ¶æ€æ¢å¤éœ€è¦æ›´é«˜ä¿¡å¿ƒ
RELOCK_FACE_THRESHOLD = 0.70  # é™ä½ä»¥ä¾¿æ›´å®¹æ˜“é‡æ–°é”å®š

# è¿ç»­å¸§ç¡®è®¤ - é˜²æ­¢ç¬é—´è¯¯åŒ¹é…å¯¼è‡´çš„è¯¯é”å®š
# é‡æ–°é”å®šéœ€è¦è¿ç»­Nå¸§éƒ½åŒ¹é…æˆåŠŸæ‰ç¡®è®¤
RELOCK_CONFIRM_FRAMES = 2  # è¿ç»­å¸§æ•°è¦æ±‚ (ä»3é™åˆ°2)
AUTO_LEARN_CONFIRM_FRAMES = 1  # è‡ªåŠ¨å­¦ä¹ ä¸éœ€è¦è¿ç»­å¸§ï¼ˆé«˜ç½®ä¿¡åº¦æ—¶ç›´æ¥å­¦ä¹ ï¼‰

# è§†è§’åº“æœ€å¤§å®¹é‡ - é˜²æ­¢ç‰¹å¾åº“æ— é™è†¨èƒ€
MAX_VIEW_COUNT = 8  # æœ€å¤šä¿å­˜8ä¸ªè§†è§’

# äººè„¸æœ‰æ•ˆå°ºå¯¸ - å°äººè„¸embeddingè´¨é‡å·®ï¼Œå®¹æ˜“è¯¯è¯†åˆ«
MIN_FACE_SIZE = 40  # äººè„¸æœ€å°è¾¹é•¿(åƒç´ )
MIN_FACE_SIZE_FOR_LEARN = 50  # å­¦ä¹ æ—¶äººè„¸æœ€å°è¾¹é•¿(æ›´ä¸¥æ ¼)

# ============================================
# å¤šå¸§æŠ•ç¥¨æœºåˆ¶ - é¿å…å•å¸§è¯¯åˆ¤
# ============================================
# è¿ç»­Nå¸§æœªåŒ¹é…æ‰åˆ¤å®šä¸¢å¤±ï¼Œé˜²æ­¢ç¬æ—¶é®æŒ¡è¯¯åˆ¤
LOST_CONFIRM_FRAMES = 5  # è¿ç»­æœªåŒ¹é…å¸§æ•°æ‰ä¸¢å¤± (é»˜è®¤ max_lost_frames=30)
# åŒ¹é…ç»“æœç¼“å†² - ä¿å­˜æœ€è¿‘Nå¸§çš„åŒ¹é…æƒ…å†µç”¨äºæŠ•ç¥¨
MATCH_HISTORY_SIZE = 5  # ä¿å­˜æœ€è¿‘5å¸§åŒ¹é…å†å²
# è¿åŠ¨æƒé‡å¢ç›Š - å¤šäººåœºæ™¯ä¸‹å¢åŠ è¿åŠ¨ä¸€è‡´æ€§æƒé‡
MOTION_WEIGHT_MULTI_PERSON = 0.6  # å¤šäººåœºæ™¯ motion æƒé‡ (body:0.4, motion:0.6)
MOTION_WEIGHT_SINGLE_PERSON = 0.5  # å•äººåœºæ™¯ motion æƒé‡ (body:0.5, motion:0.5)

# ä¾§è„¸å®¹å¿åº¦ - ä¾§è„¸è§’åº¦ä¸‹äººè„¸embeddingå·®å¼‚å¤§ï¼Œéœ€è¦æ›´ä¿¡ä»»è¿åŠ¨è¿ç»­æ€§
# å½“è¿åŠ¨è¿ç»­æ€§æé«˜æ—¶ï¼ˆmotion > 0.95ï¼‰ï¼Œå¯ä»¥å®¹å¿è¾ƒä½çš„äººè„¸ç›¸ä¼¼åº¦
MOTION_TRUST_THRESHOLD = 0.95  # è¿åŠ¨è¿ç»­æ€§ä¿¡ä»»é˜ˆå€¼
FACE_SIDE_VIEW_MIN = 0.35  # ä¾§è„¸æœ€ä½æ¥å—é˜ˆå€¼ï¼ˆé…åˆé«˜è¿åŠ¨è¿ç»­æ€§ï¼‰


def extract_view_feature(
    frame: np.ndarray,
    person_bbox: np.ndarray,
    faces: list,
    face_recognizer,
    enhanced_reid
) -> ViewFeature:
    """æå–è§†è§’ç‰¹å¾"""
    view = ViewFeature(timestamp=time.time())
    
    px1, py1, px2, py2 = person_bbox.astype(int)
    
    # æŸ¥æ‰¾äººè„¸
    for face in faces:
        fx1, fy1, fx2, fy2 = face.bbox.astype(int)
        fc_x, fc_y = (fx1 + fx2) // 2, (fy1 + fy2) // 2
        
        if px1 <= fc_x <= px2 and py1 <= fc_y <= py2:
            face_feature = face_recognizer.extract_feature(
                frame, face.bbox, face.keypoints
            )
            if face_feature:
                view.has_face = True
                view.face_embedding = face_feature.embedding
            break
    
    # äººä½“ç‰¹å¾
    body_feature = enhanced_reid.extract_feature(frame, person_bbox)
    if body_feature:
        view.part_color_hists = body_feature.part_color_hists
        view.part_lbp_hists = body_feature.part_lbp_hists
        view.geometry = body_feature.geometry
    
    return view


def find_nearest_person(persons: list, frame_center: tuple):
    """æ‰¾åˆ°ç¦»ç”»é¢ä¸­å¿ƒæœ€è¿‘çš„äºº"""
    if not persons:
        return None, -1
    
    min_dist = float('inf')
    nearest_idx = 0
    
    for i, person in enumerate(persons):
        px1, py1, px2, py2 = person.bbox
        cx = (px1 + px2) / 2
        cy = (py1 + py2) / 2
        dist = (cx - frame_center[0])**2 + (cy - frame_center[1])**2
        if dist < min_dist:
            min_dist = dist
            nearest_idx = i
    
    return persons[nearest_idx], nearest_idx


def find_person_with_gesture(persons: list, hand_bbox: np.ndarray):
    """æ‰¾åˆ°åšæ‰‹åŠ¿çš„é‚£ä¸ªäººï¼ˆä¼˜å…ˆæ‰‹åŠ¿åœ¨äººä½“æ¡†å†…ï¼Œå…¶æ¬¡æ‰¾æœ€è¿‘çš„äººä½“ï¼‰"""
    if not persons or hand_bbox is None:
        return None, -1
    
    hx1, hy1, hx2, hy2 = hand_bbox
    hand_center = ((hx1 + hx2) / 2, (hy1 + hy2) / 2)
    
    best_person = None
    best_idx = -1
    best_overlap = 0.0
    
    # ç­–ç•¥1ï¼šä¼˜å…ˆæ‰¾æ‰‹åŠ¿ä¸­å¿ƒåœ¨äººä½“æ¡†å†…çš„
    for i, person in enumerate(persons):
        px1, py1, px2, py2 = person.bbox
        
        # æ£€æŸ¥æ‰‹åŠ¿ä¸­å¿ƒæ˜¯å¦åœ¨äººä½“æ¡†å†…
        if px1 <= hand_center[0] <= px2 and py1 <= hand_center[1] <= py2:
            # è®¡ç®—é‡å ç¨‹åº¦ï¼ˆæ‰‹åŠ¿æ¡†ä¸äººä½“æ¡†çš„IoUï¼‰
            inter_x1 = max(hx1, px1)
            inter_y1 = max(hy1, py1)
            inter_x2 = min(hx2, px2)
            inter_y2 = min(hy2, py2)
            
            if inter_x2 > inter_x1 and inter_y2 > inter_y1:
                inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
                hand_area = (hx2 - hx1) * (hy2 - hy1)
                overlap = inter_area / hand_area if hand_area > 0 else 0
                
                if overlap > best_overlap:
                    best_overlap = overlap
                    best_person = person
                    best_idx = i
    
    # ç­–ç•¥2ï¼šå¦‚æœæ²¡æ‰¾åˆ°å®Œå…¨åŒ…å«çš„ï¼Œæ‰¾æ‰‹åŠ¿æ¡†ä¸äººä½“æ¡†è¾¹ç¼˜æœ€è¿‘çš„
    # è¿™å¤„ç†æ‰‹ä¼¸å‡ºèº«ä½“åšæ‰‹åŠ¿çš„æƒ…å†µ
    if best_person is None:
        min_edge_dist = float('inf')
        for i, person in enumerate(persons):
            px1, py1, px2, py2 = person.bbox
            
            # è®¡ç®—æ‰‹åŠ¿ä¸­å¿ƒåˆ°äººä½“æ¡†è¾¹ç¼˜çš„æœ€çŸ­è·ç¦»
            # å¦‚æœæ‰‹åŠ¿åœ¨æ¡†å†…ï¼Œè·ç¦»ä¸º0
            dx = max(px1 - hand_center[0], 0, hand_center[0] - px2)
            dy = max(py1 - hand_center[1], 0, hand_center[1] - py2)
            edge_dist = (dx**2 + dy**2) ** 0.5
            
            # é¢å¤–æ£€æŸ¥ï¼šæ‰‹åŠ¿åº”è¯¥åœ¨äººä½“çš„åˆç†å»¶ä¼¸èŒƒå›´å†…ï¼ˆå®½åº¦çš„50%ï¼‰
            person_width = px2 - px1
            max_extend = person_width * 0.5
            
            if edge_dist < min_edge_dist and edge_dist < max_extend:
                min_edge_dist = edge_dist
                best_person = person
                best_idx = i
    
    return best_person, best_idx


def draw_gesture_indicator(frame, gesture: GestureResult, state: SystemState, hold_progress: float = 0.0):
    """ç»˜åˆ¶æ‰‹åŠ¿æŒ‡ç¤ºå™¨"""
    h, w = frame.shape[:2]
    
    # ç»˜åˆ¶æ‰‹éƒ¨æ¡†
    if gesture.hand_bbox is not None:
        hx1, hy1, hx2, hy2 = gesture.hand_bbox.astype(int)
        
        if gesture.gesture_type == GestureType.OPEN_PALM:
            color = (0, 255, 255)  # é»„è‰²
            if state == SystemState.IDLE:
                text = "HOLD TO START"
            else:
                text = "HOLD TO STOP"
        else:
            color = (255, 165, 0)
            text = ""
        
        cv2.rectangle(frame, (hx1, hy1), (hx2, hy2), color, 2)
        if text:
            cv2.putText(frame, text, (hx1, hy1 - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        
        # ç»˜åˆ¶æŒç»­è¿›åº¦æ¡
        if hold_progress > 0:
            bar_width = hx2 - hx1
            bar_height = 8
            bar_y = hy2 + 5
            
            # èƒŒæ™¯
            cv2.rectangle(frame, (hx1, bar_y), (hx2, bar_y + bar_height), (50, 50, 50), -1)
            # è¿›åº¦
            progress_width = int(bar_width * hold_progress)
            progress_color = (0, 255, 0) if hold_progress < 1.0 else (0, 255, 255)
            cv2.rectangle(frame, (hx1, bar_y), (hx1 + progress_width, bar_y + bar_height), progress_color, -1)
            # è¾¹æ¡†
            cv2.rectangle(frame, (hx1, bar_y), (hx2, bar_y + bar_height), (255, 255, 255), 1)
            
            # è¿›åº¦ç™¾åˆ†æ¯”
            pct_text = f"{int(hold_progress * 100)}%"
            cv2.putText(frame, pct_text, (hx1, bar_y + bar_height + 15),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
    
    # çŠ¶æ€æŒ‡ç¤ºå™¨ (å³ä¸Šè§’)
    state_colors = {
        SystemState.IDLE: (128, 128, 128),          # ç°è‰²
        SystemState.TRACKING: (0, 255, 0),          # ç»¿è‰²
        SystemState.LOST_TARGET: (0, 165, 255)      # æ©™è‰²
    }
    state_color = state_colors.get(state, (255, 255, 255))
    
    cv2.circle(frame, (w - 30, 30), 15, state_color, -1)
    cv2.putText(frame, state.value, (w - 120, 55),
               cv2.FONT_HERSHEY_SIMPLEX, 0.5, state_color, 1)


def main():
    print("=" * 60)
    print("    æ‰‹åŠ¿æ§åˆ¶ç›®æ ‡è·Ÿéšç³»ç»Ÿ")
    print("=" * 60)
    
    # æ£€æŸ¥æ¨¡å‹
    yolo_path = os.path.join(MODELS_DIR, "yolov5n.onnx")
    scrfd_path = os.path.join(MODELS_DIR, "scrfd_500m_bnkps.onnx")
    mobilefacenet_path = os.path.join(MODELS_DIR, "mobilefacenet.onnx")
    
    missing = []
    if not os.path.exists(yolo_path):
        missing.append("yolov5n.onnx")
    if not os.path.exists(scrfd_path):
        missing.append("scrfd_500m_bnkps.onnx")
    if not os.path.exists(mobilefacenet_path):
        missing.append("mobilefacenet.onnx")
    
    if missing:
        print(f"\n[é”™è¯¯] ç¼ºå°‘æ¨¡å‹: {missing}")
        return
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    person_detector = YOLOv5PersonDetector(YOLOv5PersonConfig(model_path=yolo_path))
    face_detector = FaceDetector(FaceDetectorConfig(model_path=scrfd_path))
    face_recognizer = MobileFaceNetRecognizer(MobileFaceNetConfig(model_path=mobilefacenet_path))
    
    # æ‰‹åŠ¿æ£€æµ‹å™¨ (confirm_frames=1ï¼Œå› ä¸ºæŒç»­æ—¶é—´æ£€æµ‹åœ¨çŠ¶æ€æœºä¸­)
    gesture_config = GestureConfig(
        min_detection_confidence=0.7,
        min_tracking_confidence=0.5,
        gesture_confirm_frames=1  # ç«‹å³å“åº”ï¼ŒæŒç»­æ—¶é—´ç”±çŠ¶æ€æœºæ§åˆ¶
    )
    gesture_detector = GestureDetector(gesture_config)
    
    # å¢å¼ºç‰ˆ ReID
    enhanced_reid = EnhancedReIDExtractor(EnhancedReIDConfig(
        num_horizontal_parts=6,
        use_lbp=True,
        use_geometry=True
    ))
    
    # å¤šè§†è§’è¯†åˆ«å™¨
    mv_config = MultiViewConfig(
        face_weight=0.6,
        body_weight=0.4,
        face_threshold=0.60,      # äººè„¸é˜ˆå€¼ï¼ˆé€‚åº¦é™ä½ä»¥å®¹å¿ä¾§è„¸ï¼‰
        body_threshold=0.58,      # äººä½“é˜ˆå€¼ï¼ˆé€‚åº¦é™ä½ä»¥æé«˜è¿ç»­æ€§ï¼‰
        fused_threshold=0.52,     # èåˆé˜ˆå€¼ï¼ˆé€‚åº¦é™ä½ï¼‰
        motion_weight=0.15,       # æé«˜è¿åŠ¨æƒé‡ï¼ˆä¾§è„¸æ—¶ä¾èµ–è¿åŠ¨è¿ç»­æ€§ï¼‰
        auto_learn=True,
        learn_interval=3.0,       # å­¦ä¹ é—´éš”
        smooth_window=5,
        confirm_threshold=3,
        part_weights=[0.05, 0.12, 0.20, 0.20, 0.25, 0.18],
        max_views=MAX_VIEW_COUNT  # é™åˆ¶è§†è§’æ•°é‡
    )
    mv_recognizer = MultiViewRecognizer(mv_config)
    
    # åŠ è½½æ¨¡å‹
    if not person_detector.load():
        print("[é”™è¯¯] äººä½“æ£€æµ‹å™¨åŠ è½½å¤±è´¥")
        return
    if not face_detector.load():
        print("[é”™è¯¯] äººè„¸æ£€æµ‹å™¨åŠ è½½å¤±è´¥")
        return
    if not face_recognizer.load():
        print("[é”™è¯¯] äººè„¸è¯†åˆ«å™¨åŠ è½½å¤±è´¥")
        return
    if not gesture_detector.load():
        print("[é”™è¯¯] æ‰‹åŠ¿æ£€æµ‹å™¨åŠ è½½å¤±è´¥")
        return
    
    enhanced_reid.load()
    
    # æ‰“å¼€æ‘„åƒå¤´
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("[é”™è¯¯] æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
        return
    
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    # åˆ›å»ºå¯è°ƒæ•´å¤§å°çš„çª—å£
    window_name = "Gesture-Controlled Following"
    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
    cv2.resizeWindow(window_name, 960, 720)  # é»˜è®¤çª—å£å¤§å°
    
    print("\n[æ‰‹åŠ¿æ§åˆ¶]")
    print(f"  ğŸ‘‹ å¼ å¼€æ‰‹æŒæŒç»­ {GESTURE_HOLD_DURATION:.0f} ç§’: Toggle å¯åŠ¨/åœæ­¢è·Ÿéš")
    print("\n[é”®ç›˜æ§åˆ¶]")
    print("  's': æ‰‹åŠ¨ä¿å­˜ç›®æ ‡")
    print("  'a': æ·»åŠ è§†è§’")
    print("  'c': æ¸…é™¤ç›®æ ‡")
    print("  'm': åˆ‡æ¢è‡ªåŠ¨å­¦ä¹ ")
    print("  'q': é€€å‡º")
    print()
    
    # çŠ¶æ€æœº (ä½¿ç”¨ä¹‹å‰å®ç°çš„æŒç»­æ—¶é—´æ£€æµ‹)
    state_machine = StateMachine(
        lost_timeout_frames=30,
        gesture_hold_duration=GESTURE_HOLD_DURATION,
        gesture_cooldown_seconds=GESTURE_COOLDOWN_SECONDS
    )
    
    lost_frames = 0
    max_lost_frames = 30
    
    # è¿ç»­å¸§ç¡®è®¤è®¡æ•°å™¨
    relock_confirm_count = 0  # é‡æ–°é”å®šè¿ç»­åŒ¹é…å¸§æ•°
    relock_candidate_idx = -1  # å½“å‰é‡æ–°é”å®šå€™é€‰äººç´¢å¼•
    auto_learn_confirm_count = 0  # è‡ªåŠ¨å­¦ä¹ è¿ç»­åŒ¹é…å¸§æ•°
    auto_learn_candidate_view = None  # å¾…å­¦ä¹ çš„è§†è§’
    
    frame_count = 0
    fps_start = time.time()
    fps = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame_count += 1
        h, w = frame.shape[:2]
        frame_center = (w // 2, h // 2)
        
        # è®¡ç®— FPS
        if frame_count % 30 == 0:
            fps = 30 / (time.time() - fps_start)
            fps_start = time.time()
        
        # æ£€æµ‹
        persons = person_detector.detect(frame)
        faces = face_detector.detect(frame)
        gesture = gesture_detector.detect(frame)
        
        # è°ƒè¯•æ—¥å¿— (æ¯30å¸§è¾“å‡ºä¸€æ¬¡)
        if frame_count % 30 == 0:
            print(f"[DEBUG] Frame {frame_count}: persons={len(persons)}, faces={len(faces)}, gesture={gesture.gesture_type.value}")
            if faces:
                for i, face in enumerate(faces):
                    print(f"        Face[{i}]: bbox={face.bbox.astype(int).tolist()}, conf={face.confidence:.2f}")
            if persons:
                for i, person in enumerate(persons):
                    print(f"        Person[{i}]: bbox={person.bbox.astype(int).tolist()}, conf={person.confidence:.2f}")
        
        # ============== æ‰‹åŠ¿çŠ¶æ€æœº (æŒç»­æ—¶é—´æ£€æµ‹) ==============
        current_time = time.time()
        old_state = state_machine.state
        
        # å¤„ç†æ‰‹åŠ¿ (éœ€è¦æŒç»­ GESTURE_HOLD_DURATION ç§’)
        state_changed = state_machine.process_gesture(gesture.gesture_type, current_time, debug=False)
        
        # è·å–æŒç»­è¿›åº¦
        hold_progress = state_machine.get_gesture_hold_progress()
        
        # çŠ¶æ€æœºè°ƒè¯•æ—¥å¿—
        if hold_progress > 0 and frame_count % 10 == 0:
            print(f"[STATE] gesture={gesture.gesture_type.value}, hold={hold_progress*100:.0f}%, state={state_machine.state.value}")
        
        # çŠ¶æ€å˜æ›´å¤„ç†
        if state_changed:
            if state_machine.state == SystemState.TRACKING and old_state == SystemState.IDLE:
                # å¯åŠ¨è·Ÿéš - ä¼˜å…ˆé”å®šåšæ‰‹åŠ¿çš„äººï¼Œå…¶æ¬¡ç”¨æœ€è¿‘çš„äºº
                target_person = None
                target_idx = -1
                
                # 1. ä¼˜å…ˆæ‰¾åšæ‰‹åŠ¿çš„é‚£ä¸ªäºº
                if gesture.hand_bbox is not None:
                    print(f"[DEBUG] æ‰‹åŠ¿æ¡†: {gesture.hand_bbox.astype(int).tolist()}")
                    for pi, p in enumerate(persons):
                        px1, py1, px2, py2 = p.bbox.astype(int)
                        hc = ((gesture.hand_bbox[0] + gesture.hand_bbox[2]) / 2,
                              (gesture.hand_bbox[1] + gesture.hand_bbox[3]) / 2)
                        in_box = px1 <= hc[0] <= px2 and py1 <= hc[1] <= py2
                        print(f"[DEBUG] Person[{pi}] bbox: [{px1}, {py1}, {px2}, {py2}], æ‰‹åŠ¿åœ¨æ¡†å†…: {in_box}")
                    gesture_person, gesture_idx = find_person_with_gesture(persons, gesture.hand_bbox)
                    if gesture_person is not None:
                        target_person = gesture_person
                        target_idx = gesture_idx
                        print(f"[DEBUG] é”å®šåšæ‰‹åŠ¿çš„äºº Person[{target_idx}]")
                    else:
                        print(f"[DEBUG] æ‰‹åŠ¿æœªè½åœ¨ä»»ä½•äººä½“æ¡†å†…æˆ–é™„è¿‘ï¼")
                
                # 2. å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨ç¦»ç”»é¢ä¸­å¿ƒæœ€è¿‘çš„äºº
                if target_person is None:
                    target_person, target_idx = find_nearest_person(persons, frame_center)
                    if target_person is not None:
                        print(f"[DEBUG] æ— æ³•å®šä½æ‰‹åŠ¿æ‰€åœ¨äººä½“ï¼Œä½¿ç”¨æœ€è¿‘çš„äºº Person[{target_idx}]")
                
                if target_person is not None:
                    # æœ‰äººä½“æ£€æµ‹ç»“æœ
                    print(f"[DEBUG] é”å®šäººä½“: bbox={target_person.bbox.astype(int).tolist()}")
                    view = extract_view_feature(
                        frame, target_person.bbox, faces, 
                        face_recognizer, enhanced_reid
                    )
                    print(f"[DEBUG] æå–ç‰¹å¾: has_face={view.has_face}, has_body={view.part_color_hists is not None}")
                    if view.has_face and view.face_embedding is not None:
                        print(f"[DEBUG] äººè„¸embedding: shape={view.face_embedding.shape}, norm={np.linalg.norm(view.face_embedding):.3f}")
                    mv_recognizer.set_target(view, target_person.bbox)
                    mv_recognizer.clear_match_history()  # æ–°ç›®æ ‡ï¼Œæ¸…ç©ºå†å²
                    lost_frames = 0
                    face_str = "æœ‰äººè„¸" if view.has_face else "æ— äººè„¸"
                    print(f"[æ‰‹åŠ¿å¯åŠ¨] ç›®æ ‡å·²é”å®š (äººä½“+{face_str})")
                elif faces:
                    # æ²¡æœ‰äººä½“ä½†æœ‰äººè„¸ - ç”¨äººè„¸æ¡†ä½œä¸ºä¸´æ—¶ç›®æ ‡
                    # ä¼˜å…ˆæ‰¾ç¦»æ‰‹åŠ¿æœ€è¿‘çš„äººè„¸ï¼Œå…¶æ¬¡æ‰¾ç¦»ç”»é¢ä¸­å¿ƒæœ€è¿‘çš„äººè„¸
                    target_face = None
                    
                    if gesture.hand_bbox is not None:
                        # æ‰¾ç¦»æ‰‹åŠ¿æœ€è¿‘çš„äººè„¸
                        hx1, hy1, hx2, hy2 = gesture.hand_bbox
                        hand_center = ((hx1 + hx2) / 2, (hy1 + hy2) / 2)
                        min_dist = float('inf')
                        for face in faces:
                            fx1, fy1, fx2, fy2 = face.bbox
                            fcx, fcy = (fx1 + fx2) / 2, (fy1 + fy2) / 2
                            dist = (fcx - hand_center[0])**2 + (fcy - hand_center[1])**2
                            if dist < min_dist:
                                min_dist = dist
                                target_face = face
                        if target_face is not None:
                            print(f"[DEBUG] ä»…äººè„¸æ¨¡å¼: ä½¿ç”¨ç¦»æ‰‹åŠ¿æœ€è¿‘çš„äººè„¸")
                    
                    if target_face is None:
                        # æ‰¾ç¦»ç”»é¢ä¸­å¿ƒæœ€è¿‘çš„äººè„¸
                        min_dist = float('inf')
                        for face in faces:
                            fx1, fy1, fx2, fy2 = face.bbox
                            fcx, fcy = (fx1 + fx2) / 2, (fy1 + fy2) / 2
                            dist = (fcx - frame_center[0])**2 + (fcy - frame_center[1])**2
                            if dist < min_dist:
                                min_dist = dist
                                target_face = face
                    
                    if target_face is not None:
                        # ç”¨äººè„¸æ¡†æ‰©å±•ä¸ºä¼ªäººä½“æ¡†ï¼ˆå‘ä¸‹æ‰©å±•3å€ï¼‰
                        fx1, fy1, fx2, fy2 = target_face.bbox
                        face_h = fy2 - fy1
                        face_w = fx2 - fx1
                        print(f"[DEBUG] ä»…äººè„¸æ¨¡å¼: face_bbox={target_face.bbox.astype(int).tolist()}")
                        # äººè„¸å¤§çº¦æ˜¯äººä½“çš„1/7ï¼Œå‘ä¸‹æ‰©å±•
                        pseudo_bbox = np.array([
                            max(0, fx1 - face_w * 0.5),
                            fy1,
                            min(w, fx2 + face_w * 0.5),
                            min(h, fy2 + face_h * 5)
                        ])
                        print(f"[DEBUG] ä¼ªäººä½“æ¡†: pseudo_bbox={pseudo_bbox.astype(int).tolist()}")
                        
                        view = ViewFeature(timestamp=time.time())
                        view.has_face = True
                        face_feature = face_recognizer.extract_feature(
                            frame, target_face.bbox, target_face.keypoints
                        )
                        if face_feature:
                            view.face_embedding = face_feature.embedding
                            print(f"[DEBUG] äººè„¸ç‰¹å¾æå–æˆåŠŸ: embedding_shape={face_feature.embedding.shape}, norm={np.linalg.norm(face_feature.embedding):.3f}")
                        else:
                            print(f"[DEBUG] äººè„¸ç‰¹å¾æå–å¤±è´¥!")
                        
                        mv_recognizer.set_target(view, pseudo_bbox)
                        mv_recognizer.clear_match_history()  # æ–°ç›®æ ‡ï¼Œæ¸…ç©ºå†å²
                        print(f"[DEBUG] ç›®æ ‡å·²è®¾ç½®: has_face_view={mv_recognizer.target.has_face_view if mv_recognizer.target else False}")
                        lost_frames = 0
                        print(f"[æ‰‹åŠ¿å¯åŠ¨] ç›®æ ‡å·²é”å®š (ä»…äººè„¸ï¼Œç­‰å¾…äººä½“è¡¥å……)")
                else:
                    # æ—¢æ²¡æœ‰äººä½“ä¹Ÿæ²¡æœ‰äººè„¸
                    state_machine.state = SystemState.IDLE
                    print("[æç¤º] æœªæ£€æµ‹åˆ°äººä½“æˆ–äººè„¸ï¼Œæ— æ³•å¯åŠ¨")
            
            elif state_machine.state == SystemState.IDLE and old_state == SystemState.TRACKING:
                # åœæ­¢è·Ÿéš - åªæœ‰ä» TRACKING çŠ¶æ€æ‰èƒ½åœæ­¢
                mv_recognizer.clear_target()
                mv_recognizer.clear_match_history()  # æ¸…ç©ºå†å²
                lost_frames = 0
                print("[æ‰‹åŠ¿åœæ­¢] è·Ÿéšå·²åœæ­¢")
        
        # ============== ç›®æ ‡è·Ÿè¸ª ==============
        target_person_idx = -1
        target_face_idx = -1  # ä»…äººè„¸åŒ¹é…æ—¶çš„ç´¢å¼•
        current_match_info = None  # å½“å‰å¸§åŒ¹é…ä¿¡æ¯ï¼Œç”¨äºç•Œé¢æ˜¾ç¤º
        
        # ============== åœºæ™¯åˆ¤æ–­ ==============
        # å…³é”®ï¼šå¤šäººåœºæ™¯åº”è¯¥ç”¨ max(persons, faces) åˆ¤æ–­ï¼Œè€Œä¸æ˜¯ä»…çœ‹ persons
        # åœºæ™¯åˆ†ç±»ï¼š
        #   å•äºº: persons<=1 ä¸” faces<=1
        #   å¤šäºº: persons>1 æˆ– faces>1 (åªè¦æœ‰ä¸€æ–¹>1å°±æ˜¯å¤šäººé£é™©åœºæ™¯)
        num_persons = len(persons)
        num_faces = len(faces)
        is_multi_person_scene = num_persons > 1 or num_faces > 1
        is_single_person_scene = not is_multi_person_scene
        
        if state_machine.state == SystemState.TRACKING:
            matched_any = False
            
            # è°ƒè¯•: æ˜¾ç¤ºç›®æ ‡ä¿¡æ¯å’Œåœºæ™¯ç±»å‹
            if frame_count % 30 == 0:
                scene_type = "å¤šäºº" if is_multi_person_scene else "å•äºº"
                print(f"[DEBUG] åœºæ™¯: {scene_type} (persons={num_persons}, faces={num_faces})")
                if mv_recognizer.target:
                    t = mv_recognizer.target
                    print(f"[DEBUG] Target: num_views={t.num_views}, has_face_view={t.has_face_view}")
                    for vi, v in enumerate(t.view_features):
                        print(f"        View[{vi}]: has_face={v.has_face}, has_body={v.part_color_hists is not None}")
            
            # 1. é€šè¿‡äººä½“åŒ¹é… - ä½¿ç”¨"æœ€ä½³åŒ¹é…"ç­–ç•¥ï¼ˆè€Œä¸æ˜¯"ç¬¬ä¸€ä¸ªåŒ¹é…"ï¼‰
            # æ”¶é›†æ‰€æœ‰å€™é€‰åŒ¹é…ï¼Œé€‰æ‹©æœ€é«˜åˆ†çš„
            all_person_matches = []  # [(idx, similarity, method, view, face_in_person, face_verified, face_sim, body_sim)]
            
            # å…³é”®ä¿æŠ¤ï¼šå¦‚æœç›®æ ‡æœ‰äººè„¸ç‰¹å¾ï¼Œå€™é€‰äººä¹Ÿæœ‰äººè„¸æ—¶å¿…é¡»é€šè¿‡äººè„¸éªŒè¯
            target_has_face = mv_recognizer.target and mv_recognizer.target.has_face_view
            target_has_body = mv_recognizer.target and any(v.has_body for v in mv_recognizer.target.view_features)
            
            # =====================================================================
            # åœºæ™¯Ã—ç›®æ ‡çŠ¶æ€ åˆ†æçŸ©é˜µ
            # =====================================================================
            # 
            # ç”»é¢å†…å®¹:
            #   å•äººåœºæ™¯: persons<=1 ä¸” faces<=1
            #   å¤šäººåœºæ™¯: persons>1 æˆ– faces>1
            #
            # ç›®æ ‡åœ¨ç”»é¢ä¸­çš„çŠ¶æ€:
            #   A: ç›®æ ‡ä»¥ äººè„¸+äººä½“ å‡ºç°
            #   B: ç›®æ ‡ä»…ä»¥ äººè„¸ å‡ºç°ï¼ˆäººä½“è¢«é®æŒ¡æˆ–å¤ªè¿œï¼‰
            #   C: ç›®æ ‡ä»…ä»¥ äººä½“ å‡ºç°ï¼ˆèƒŒå¯¹/ä½å¤´/é®æŒ¡è„¸ï¼‰
            #   D: ç›®æ ‡ä¸åœ¨ç”»é¢ä¸­
            #
            # å¤„ç†ç­–ç•¥:
            # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            # â”‚ Step1: éå†äººä½“åŒ¹é… â†’ è¦†ç›–çŠ¶æ€ A, C                              â”‚
            # â”‚   - A: äººè„¸éªŒè¯é€šè¿‡ â†’ ç¡®è®¤åŒ¹é…                                   â”‚
            # â”‚   - C: æ— äººè„¸å¯éªŒè¯ï¼Œä½¿ç”¨bodyåŒ¹é…                                â”‚
            # â”‚                                                                  â”‚
            # â”‚ Step2: ä»…äººè„¸åŒ¹é… â†’ è¦†ç›–çŠ¶æ€ B                                   â”‚
            # â”‚   - æ— äººä½“åŒ¹é…æˆåŠŸæ—¶ï¼Œå°è¯•ç‹¬ç«‹äººè„¸åŒ¹é…                           â”‚
            # â”‚                                                                  â”‚
            # â”‚ æœªåŒ¹é… â†’ çŠ¶æ€ D æˆ–åŒ¹é…å¤±è´¥                                       â”‚
            # â”‚   - ç´¯ç§¯ lost_frames â†’ è§¦å‘ LOST_TARGET                          â”‚
            # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            #
            # å…³é”®é£é™©: ç›®æ ‡ä¸åœ¨(D) ä½†è¯¯åŒ¹é…åˆ°è¡£ç€ç›¸ä¼¼çš„ä»–äºº
            #
            # ä¿æŠ¤æªæ–½æ±‡æ€»:
            # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            # â”‚ åœºæ™¯               â”‚ ä¿æŠ¤ç­–ç•¥                                    â”‚
            # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            # â”‚ å¤šäºº+ç›®æ ‡æœ‰è„¸+     â”‚ face_sim < FACE_REJECT â†’ æ‹’ç»              â”‚
            # â”‚ å€™é€‰æœ‰è„¸           â”‚ face_sim < FACE_UNCERTAIN ä¸”               â”‚
            # â”‚                    â”‚ body_sim < HIGH_BODY â†’ æ‹’ç»                 â”‚
            # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            # â”‚ å¤šäºº+ç›®æ ‡æœ‰è„¸+     â”‚ body_sim < BACK_VIEW_BODY â†’ æ‹’ç»           â”‚
            # â”‚ å€™é€‰æ— è„¸           â”‚ (å¯èƒ½æ˜¯ä»–äººèƒŒé¢)                            â”‚
            # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            # â”‚ å•äºº+ç›®æ ‡æœ‰è„¸+     â”‚ åŒä¸Šä¿æŠ¤é€»è¾‘                                â”‚
            # â”‚ å€™é€‰æœ‰è„¸           â”‚ å› ä¸ºé‚£ä¸ª"å•äºº"å¯èƒ½ä¸æ˜¯ç›®æ ‡                  â”‚
            # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            # â”‚ å•äºº+ç›®æ ‡æœ‰è„¸+     â”‚ å¦‚æœ‰å…¶ä»–äººè„¸: body_sim < 0.65 â†’ æ‹’ç»       â”‚
            # â”‚ å€™é€‰æ— è„¸           â”‚ æ— å…¶ä»–äººè„¸: ä½¿ç”¨æ ‡å‡†é˜ˆå€¼                    â”‚
            # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            # â”‚ ä»…äººè„¸åŒ¹é…         â”‚ å¤šäººè„¸: +0.05 é˜ˆå€¼æƒ©ç½š                      â”‚
            # â”‚                    â”‚ è¿œå¤„äººè„¸: +0.10 é˜ˆå€¼æƒ©ç½š                    â”‚
            # â”‚                    â”‚ äººè„¸åœ¨ä¸åŒ¹é…äººä½“æ¡†å†…: è·³è¿‡                  â”‚
            # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            # =====================================================================
            
            for idx, person in enumerate(persons):
                view = extract_view_feature(
                    frame, person.bbox, faces, face_recognizer, enhanced_reid
                )
                
                # ä½¿ç”¨ return_details=True è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å« face_simï¼‰
                result = mv_recognizer.is_same_target(
                    view, person.bbox, return_details=True
                )
                # è¿”å›å€¼æ˜¯ (is_match, similarity, method, details)
                is_match = result[0]
                similarity = result[1]
                method = result[2]
                details = result[3] if len(result) > 3 else {}
                
                # æå–è¯¦ç»†ç›¸ä¼¼åº¦
                face_sim = details.get('face_sim')  # å¯èƒ½ä¸º Noneï¼ˆå€™é€‰äººæ²¡æœ‰äººè„¸ï¼‰
                body_sim = details.get('body_sim', 0.0)
                
                # æå–è¿åŠ¨è¿ç»­æ€§åˆ†æ•°
                motion_score = details.get('motion_sim', 0.0)
                if 'M:' in method:
                    try:
                        motion_str = method.split('M:')[1].split(')')[0].split(' ')[0]
                        motion_score = float(motion_str)
                    except:
                        pass
                
                if frame_count % 30 == 0:
                    face_str = f"F:{face_sim:.2f}" if face_sim is not None else "F:None"
                    print(f"[DEBUG] Person[{idx}] match: is_match={is_match}, sim={similarity:.3f}, {face_str}, B:{body_sim:.2f}, M:{motion_score:.2f}, method={method}")
                
                if is_match:
                    face_in_person = view.has_face and view.face_embedding is not None
                    
                    # ============================================
                    # ç®€åŒ–çš„åŒ¹é…é€»è¾‘ï¼ˆé˜²æ­¢è¯¯è·Ÿè¸ªä»–äººï¼‰
                    # ============================================
                    # æ ¸å¿ƒæ€è·¯:
                    #   1. äººè„¸ > é˜ˆå€¼ ä¸” å°ºå¯¸å¤Ÿå¤§ â†’ é äººè„¸åˆ¤æ–­
                    #   2. äººè„¸ < é˜ˆå€¼ æˆ– å°ºå¯¸å¤ªå° â†’ é  motion + body åˆ¤æ–­
                    #   3. motion + body éƒ½ä½ â†’ ç›®æ ‡ä¸¢å¤±
                    # ============================================
                    
                    FACE_MATCH_THRESHOLD = 0.55  # äººè„¸åŒ¹é…é˜ˆå€¼
                    BODY_MOTION_THRESHOLD = 0.65  # body + motion ç»¼åˆé˜ˆå€¼
                    MULTI_PERSON_BODY_THRESHOLD = 0.70  # å¤šäººåœºæ™¯ä¸‹ä»…bodyåŒ¹é…çš„é˜ˆå€¼
                    
                    # æ£€æŸ¥äººè„¸å°ºå¯¸æ˜¯å¦è¶³å¤Ÿå¤§
                    face_size_valid = False
                    current_face_size = 0
                    for face in faces:
                        fx1, fy1, fx2, fy2 = face.bbox.astype(int)
                        fc_x, fc_y = (fx1 + fx2) // 2, (fy1 + fy2) // 2
                        px1, py1, px2, py2 = person.bbox.astype(int)
                        if px1 <= fc_x <= px2 and py1 <= fc_y <= py2:
                            face_w = fx2 - fx1
                            face_h = fy2 - fy1
                            current_face_size = min(face_w, face_h)
                            face_size_valid = current_face_size >= MIN_FACE_SIZE
                            break
                    
                    # è®¡ç®— body + motion ç»¼åˆåˆ†æ•°
                    # å¤šäººåœºæ™¯ä¸‹å¢åŠ  motion æƒé‡ï¼Œå› ä¸ºè¿åŠ¨è½¨è¿¹æ›´å¯é 
                    if is_multi_person_scene:
                        motion_weight = MOTION_WEIGHT_MULTI_PERSON
                    else:
                        motion_weight = MOTION_WEIGHT_SINGLE_PERSON
                    body_weight = 1.0 - motion_weight
                    body_motion_score = body_sim * body_weight + motion_score * motion_weight
                    
                    # åˆ¤æ–­åŒ¹é…ç±»å‹ (äººè„¸æœ‰æ•ˆ = ç›¸ä¼¼åº¦é«˜ ä¸” å°ºå¯¸å¤Ÿå¤§)
                    face_matched = (face_sim is not None and 
                                    face_sim >= FACE_MATCH_THRESHOLD and 
                                    face_size_valid)
                    body_motion_matched = body_motion_score >= BODY_MOTION_THRESHOLD
                    
                    if frame_count % 30 == 0 and face_sim is not None:
                        print(f"[DEBUG] Person[{idx}] face_size={current_face_size}px, valid={face_size_valid}, face_matched={face_matched}")
                    
                    # å†³ç­–é€»è¾‘
                    accept = False
                    match_type = ""
                    
                    if face_matched:
                        # Case 1: äººè„¸åŒ¹é… â†’ ç›´æ¥ä¿¡ä»»
                        accept = True
                        match_type = "face"
                        if frame_count % 30 == 0:
                            print(f"[DEBUG] Person[{idx}] äººè„¸åŒ¹é…é€šè¿‡ (F:{face_sim:.2f}>={FACE_MATCH_THRESHOLD})")
                    elif target_has_face and face_in_person and face_sim is not None and face_sim < 0.30:
                        # Case 2: ç›®æ ‡æœ‰è„¸ + å€™é€‰æœ‰è„¸ + äººè„¸æ˜ç¡®ä¸åŒ¹é… â†’ å¤šäººåœºæ™¯æ‹’ç»ï¼Œå•äººåœºæ™¯çœ‹body+motion
                        if is_multi_person_scene:
                            if frame_count % 30 == 0:
                                print(f"[DEBUG] Person[{idx}] å¤šäººåœºæ™¯äººè„¸æ˜ç¡®ä¸åŒ¹é…(F:{face_sim:.2f}<0.30), æ‹’ç»")
                            accept = False
                        elif body_motion_matched:
                            accept = True
                            match_type = "body_motion"
                            if frame_count % 30 == 0:
                                print(f"[DEBUG] Person[{idx}] å•äººåœºæ™¯äººè„¸ä½(F:{face_sim:.2f})ä½†body+motioné«˜({body_motion_score:.2f}), é€šè¿‡")
                        else:
                            if frame_count % 30 == 0:
                                print(f"[DEBUG] Person[{idx}] å•äººåœºæ™¯äººè„¸ä½ä¸”body+motionä¸è¶³({body_motion_score:.2f}<{BODY_MOTION_THRESHOLD}), æ‹’ç»")
                            accept = False
                    elif body_motion_matched:
                        # Case 3: äººè„¸ä¸å¤Ÿä½† body+motion å¤Ÿ â†’ é€šè¿‡
                        # å¤šäººåœºæ™¯éœ€è¦æ›´é«˜çš„ body é˜ˆå€¼
                        if is_multi_person_scene and target_has_face and body_sim < MULTI_PERSON_BODY_THRESHOLD:
                            if frame_count % 30 == 0:
                                print(f"[DEBUG] Person[{idx}] å¤šäººåœºæ™¯æ— äººè„¸éªŒè¯ä¸”bodyä¸è¶³({body_sim:.2f}<{MULTI_PERSON_BODY_THRESHOLD}), æ‹’ç»")
                            accept = False
                        else:
                            accept = True
                            match_type = "body_motion"
                            if frame_count % 30 == 0:
                                print(f"[DEBUG] Person[{idx}] body+motionåŒ¹é…é€šè¿‡ (B:{body_sim:.2f}+M:{motion_score:.2f}={body_motion_score:.2f})")
                    else:
                        # Case 4: äººè„¸å’Œbody+motionéƒ½ä¸å¤Ÿ â†’ æ‹’ç»
                        if frame_count % 30 == 0:
                            face_str = f"F:{face_sim:.2f}" if face_sim is not None else "F:None"
                            print(f"[DEBUG] Person[{idx}] äººè„¸å’Œbody+motionéƒ½ä¸è¶³ ({face_str}, BM:{body_motion_score:.2f}), æ‹’ç»")
                        accept = False
                    
                    if accept:
                        # tuple: (idx, similarity, method, view, face_in_person, face_matched, face_sim, body_sim, motion_score, match_type)
                        all_person_matches.append((idx, similarity, method, view, face_in_person, face_matched, face_sim, body_sim, motion_score, match_type))
            
            # é€‰æ‹©æœ€ä½³åŒ¹é…
            if all_person_matches:
                # ç­–ç•¥: 
                #   1. ä¼˜å…ˆé€‰äººè„¸åŒ¹é…çš„ï¼ˆèº«ä»½æœ€å¯é ï¼‰
                #   2. äººè„¸åŒ¹é…ä¸­ä¼˜å…ˆé€‰ motion é«˜çš„ï¼ˆè½¨è¿¹æœ€ä¸€è‡´ï¼‰
                #   3. å…¶æ¬¡é€‰ body+motion åŒ¹é…çš„
                #   4. body+motion ä¸­å¤šäººåœºæ™¯ä¼˜å…ˆé€‰ motion é«˜çš„
                # tuple: (idx, similarity, method, view, face_in_person, face_matched, face_sim, body_sim, motion_score, match_type)
                matches_by_face = [m for m in all_person_matches if m[9] == "face"]  # m[9] = match_type
                matches_by_body_motion = [m for m in all_person_matches if m[9] == "body_motion"]
                
                best_match = None
                if matches_by_face:
                    # äººè„¸åŒ¹é…ä¸­ï¼Œä¼˜å…ˆé€‰ motion é«˜çš„ï¼ˆè½¨è¿¹ä¸€è‡´æ€§ï¼‰
                    # æ’åºä¾æ®: face_sim * 0.6 + motion * 0.4
                    best_match = max(matches_by_face, key=lambda x: (x[6] if x[6] is not None else 0) * 0.6 + x[8] * 0.4)
                    if frame_count % 30 == 0 and len(all_person_matches) > 1:
                        print(f"[DEBUG] é€‰æ‹©äººè„¸åŒ¹é… Person[{best_match[0]}] (F:{best_match[6]:.2f}, M:{best_match[8]:.2f}, å…±{len(all_person_matches)}å€™é€‰)")
                elif matches_by_body_motion:
                    # body+motion åŒ¹é…ä¸­ï¼Œå¤šäººåœºæ™¯å¼ºè°ƒ motionï¼Œå•äººåœºæ™¯å¹³è¡¡
                    if is_multi_person_scene:
                        # å¤šäºº: motion ä¼˜å…ˆï¼ˆè½¨è¿¹ä¸€è‡´æœ€é‡è¦ï¼‰
                        best_match = max(matches_by_body_motion, key=lambda x: x[8] * 0.7 + x[7] * 0.3)
                    else:
                        # å•äºº: å¹³è¡¡ body å’Œ motion
                        best_match = max(matches_by_body_motion, key=lambda x: x[7] * 0.5 + x[8] * 0.5)
                    if frame_count % 30 == 0:
                        print(f"[DEBUG] é€‰æ‹©body+motionåŒ¹é… Person[{best_match[0]}], B:{best_match[7]:.2f}, M:{best_match[8]:.2f}")
                
                if best_match:
                    # è§£åŒ…: (idx, similarity, method, view, face_in_person, face_matched, face_sim, body_sim, motion_score, match_type)
                    idx, similarity, method, view, face_in_person, face_matched, match_face_sim, match_body_sim, match_motion_score, match_type = best_match
                    matched_any = True
                    target_person_idx = idx
                    lost_frames = 0
                    
                    # ä¿å­˜å½“å‰åŒ¹é…ä¿¡æ¯ç”¨äºæ˜¾ç¤º
                    current_match_info = {
                        'type': 'person',
                        'similarity': similarity,
                        'method': method,
                        'match_type': match_type,  # "face" or "body_motion"
                        'threshold': FACE_MATCH_THRESHOLD if match_type == "face" else BODY_MOTION_THRESHOLD
                    }
                    
                    # æ›´æ–°è·Ÿè¸ª
                    mv_recognizer.update_tracking(persons[idx].bbox)
                    
                    # ============================================
                    # ç®€åŒ–çš„è‡ªåŠ¨å­¦ä¹ ç­–ç•¥
                    # ============================================
                    # æ ¸å¿ƒåŸåˆ™ï¼š
                    #   1. äººè„¸åŒ¹é… + bodyä¸åŒ¹é…ä½†motion+bodyé«˜ â†’ å­¦ä¹ bodyï¼ˆå‰æï¼šäººè„¸åœ¨äººä½“æ¡†å†…ï¼‰
                    #   2. motion+bodyåŒ¹é… + äººè„¸ä½ä½†>æŸå€¼ â†’ å­¦ä¹ äººè„¸ï¼ˆå‰æï¼šäººè„¸åœ¨äººä½“æ¡†å†…ï¼‰
                    #   3. å…³é”®çº¦æŸï¼šæœ‰äººè„¸+æœ‰äººä½“æ—¶ï¼Œå­¦ä¹ å¿…é¡»ä¿è¯äººè„¸åœ¨äººä½“æ¡†å†…
                    # ============================================
                    
                    should_learn = False
                    learn_what = ""  # "body" or "face" or "both"
                    learn_reason = ""
                    
                    target_has_body = (mv_recognizer.target is not None and 
                                       any(v.has_body for v in mv_recognizer.target.view_features))
                    
                    # å®¹é‡æ£€æŸ¥ï¼šè§†è§’åº“å·²æ»¡æ—¶åœæ­¢å­¦ä¹ 
                    current_view_count = mv_recognizer.target.num_views if mv_recognizer.target else 0
                    if current_view_count >= MAX_VIEW_COUNT:
                        if frame_count % 60 == 0:
                            print(f"[DEBUG] è§†è§’åº“å·²æ»¡({current_view_count}>={MAX_VIEW_COUNT})ï¼Œåœæ­¢å­¦ä¹ ")
                        should_learn = False
                    # å¤šäººåœºæ™¯ + æ²¡æœ‰äººè„¸åŒ¹é… = ç¦æ­¢å­¦ä¹ 
                    elif is_multi_person_scene and match_type != "face":
                        if frame_count % 30 == 0:
                            print(f"[DEBUG] å¤šäººåœºæ™¯æ— äººè„¸åŒ¹é…ï¼Œç¦æ­¢å­¦ä¹ ")
                        should_learn = False
                    else:
                        # æå–åŒ¹é…ä¿¡æ¯
                        match_face_sim = best_match[6] if best_match[6] is not None else 0.0
                        match_body_sim = best_match[7]
                        match_motion = best_match[8]
                        body_motion_combined = match_body_sim * 0.5 + match_motion * 0.5
                        
                        # å­¦ä¹ é˜ˆå€¼
                        FACE_LEARN_THRESHOLD_LOCAL = 0.65  # äººè„¸å­¦ä¹ é˜ˆå€¼
                        BODY_MOTION_LEARN_THRESHOLD = 0.70  # body+motion å­¦ä¹ é˜ˆå€¼
                        FACE_MIN_FOR_BODY_LEARN = 0.50  # å­¦ä¹ bodyæ—¶äººè„¸çš„æœ€ä½è¦æ±‚
                        
                        # æ£€æŸ¥å½“å‰äººè„¸å°ºå¯¸æ˜¯å¦è¶³å¤Ÿå¤§ï¼ˆç”¨äºå­¦ä¹ ï¼‰
                        current_face_size_for_learn = 0
                        face_size_ok_for_learn = False
                        for face in faces:
                            fx1, fy1, fx2, fy2 = face.bbox.astype(int)
                            fc_x, fc_y = (fx1 + fx2) // 2, (fy1 + fy2) // 2
                            px1, py1, px2, py2 = persons[idx].bbox.astype(int)
                            if px1 <= fc_x <= px2 and py1 <= fc_y <= py2:
                                face_w = fx2 - fx1
                                face_h = fy2 - fy1
                                current_face_size_for_learn = min(face_w, face_h)
                                face_size_ok_for_learn = current_face_size_for_learn >= MIN_FACE_SIZE_FOR_LEARN
                                break
                        
                        # Case 1: äººè„¸åŒ¹é…é€šè¿‡ â†’ å¯ä»¥å­¦ä¹ body
                        if match_type == "face":
                            # äººè„¸åŒ¹é… + body+motioné«˜ â†’ å­¦ä¹ bodyï¼ˆå¦‚æœç›®æ ‡è¿˜æ²¡æœ‰bodyæˆ–éœ€è¦æ›´æ–°ï¼‰
                            if body_motion_combined >= BODY_MOTION_LEARN_THRESHOLD:
                                # å…³é”®çº¦æŸï¼šäººè„¸å¿…é¡»åœ¨äººä½“æ¡†å†…ï¼
                                if face_in_person:
                                    should_learn = True
                                    learn_what = "body"
                                    learn_reason = f"äººè„¸åŒ¹é…(F:{match_face_sim:.2f})å­¦ä¹ body(BM:{body_motion_combined:.2f})"
                                else:
                                    if frame_count % 30 == 0:
                                        print(f"[DEBUG] äººè„¸ä¸åœ¨äººä½“æ¡†å†…ï¼Œä¸å­¦ä¹ body")
                            elif match_face_sim >= FACE_LEARN_THRESHOLD_LOCAL and face_size_ok_for_learn:
                                # äººè„¸å¤Ÿé«˜ + å°ºå¯¸å¤Ÿå¤§ â†’ ç›´æ¥å­¦ä¹ å½“å‰è§†è§’
                                should_learn = True
                                learn_what = "face"
                                learn_reason = f"äººè„¸é«˜ç½®ä¿¡(F:{match_face_sim:.2f}, size={current_face_size_for_learn}px)"
                        
                        # Case 2: body+motionåŒ¹é…é€šè¿‡ â†’ å¯ä»¥å­¦ä¹ äººè„¸
                        elif match_type == "body_motion":
                            # body+motionåŒ¹é… + æœ‰äººè„¸ä¸”>æŸå€¼ + äººè„¸å°ºå¯¸å¤Ÿå¤§ â†’ å­¦ä¹ äººè„¸
                            if face_in_person and match_face_sim >= FACE_MIN_FOR_BODY_LEARN and face_size_ok_for_learn:
                                # å…³é”®çº¦æŸï¼šäººè„¸å¿…é¡»åœ¨äººä½“æ¡†å†… ä¸” å°ºå¯¸è¶³å¤Ÿå¤§ï¼
                                should_learn = True
                                learn_what = "face"
                                learn_reason = f"body+motionåŒ¹é…(BM:{body_motion_combined:.2f})å­¦ä¹ face(F:{match_face_sim:.2f}, size={current_face_size_for_learn}px)"
                            elif not face_in_person and body_motion_combined >= BODY_MOTION_LEARN_THRESHOLD:
                                # çº¯èƒŒé¢/ä¾§é¢ï¼Œå­¦ä¹ bodyè§†è§’
                                should_learn = True
                                learn_what = "body"
                                learn_reason = f"èƒŒé¢åŒ¹é…(BM:{body_motion_combined:.2f})"
                    
                    if should_learn:
                        learned, op_info = mv_recognizer.auto_learn(view, persons[idx].bbox, True)
                        if learned:
                            print(f"[è‡ªåŠ¨å­¦ä¹ ] {learn_reason} -> {op_info}")
            
            # 2. å¦‚æœäººä½“æ²¡åŒ¹é…åˆ°ï¼Œå°è¯•ä»…é€šè¿‡äººè„¸åŒ¹é…ï¼ˆä½¿ç”¨æ›´ä¸¥æ ¼çš„é˜ˆå€¼ï¼‰
            # ============================================
            # è¿™é‡Œå¤„ç†ç›®æ ‡çŠ¶æ€ B: ç›®æ ‡ä»…ä»¥äººè„¸å‡ºç°
            # åœºæ™¯åŒ…æ‹¬:
            #   - 1.2: ç”»é¢åªæœ‰äººè„¸ï¼ˆç›®æ ‡è¿œå¤„/è¢«é®æŒ¡ï¼‰
            #   - 2.1-B/2.2-B: å¤šäººåœºæ™¯ï¼Œç›®æ ‡äººä½“è¢«é®æŒ¡åªéœ²è„¸
            #   - 2.3: å¤šä¸ªè¿œå¤„äººè„¸ï¼Œæ— äººä½“
            # ============================================
            if not matched_any and faces and mv_recognizer.target and mv_recognizer.target.has_face_view:
                if frame_count % 30 == 0:
                    print(f"[DEBUG] äººä½“åŒ¹é…å¤±è´¥ï¼Œå°è¯•ä»…äººè„¸åŒ¹é… (é˜ˆå€¼={FACE_ONLY_THRESHOLD})...")
                
                best_face_match = None
                best_face_sim = 0.0
                best_face_idx = -1
                best_view_idx = -1
                
                # å¤šäººè„¸åœºæ™¯éœ€è¦æ›´ä¸¥æ ¼çš„é˜ˆå€¼
                multi_face_penalty = 0.05 if num_faces > 1 else 0.0
                    
                for face_idx, face in enumerate(faces):
                    fx1, fy1, fx2, fy2 = face.bbox
                    fc_x, fc_y = (fx1 + fx2) / 2, (fy1 + fy2) / 2
                    
                    # æ£€æŸ¥äººè„¸æ˜¯å¦åœ¨æŸä¸ªäººä½“æ¡†å†…
                    face_in_any_person = False
                    
                    if len(persons) > 0:
                        for p_idx, person in enumerate(persons):
                            px1, py1, px2, py2 = person.bbox
                            if px1 <= fc_x <= px2 and py1 <= fc_y <= py2:
                                face_in_any_person = True
                                break
                    
                    # æƒ…å†µ1: å¤šäººåœºæ™¯ï¼Œäººè„¸åœ¨ä¸åŒ¹é…çš„äººä½“æ¡†å†… â†’ è·³è¿‡ï¼ˆå±äºåˆ«äººï¼‰
                    # å…³é”®ï¼šè¿™æ˜¯åœºæ™¯ 2.1-B/2.2-B çš„ä¿æŠ¤
                    if num_persons > 1 and face_in_any_person:
                        if frame_count % 30 == 0:
                            print(f"[DEBUG] Face[{face_idx}] åœ¨ä¸åŒ¹é…çš„äººä½“æ¡†å†…(å¤šäººåœºæ™¯)ï¼Œè·³è¿‡")
                        continue
                    
                    # æƒ…å†µ2: æœ‰äººä½“ä½†äººè„¸ä¸åœ¨ä»»ä½•äººä½“æ¡†å†… â†’ è¿œå¤„çš„äººè„¸ï¼Œä½¿ç”¨æ›´ä¸¥æ ¼é˜ˆå€¼
                    # åœºæ™¯ï¼šç›®æ ‡èƒŒå¯¹é•œå¤´ï¼ˆæœ‰äººä½“æ— è„¸ï¼‰ï¼Œè¿œå¤„æœ‰å…¶ä»–äººçš„è„¸ï¼ˆæœ‰è„¸æ— äººä½“ï¼‰
                    is_distant_face = num_persons > 0 and not face_in_any_person
                    current_threshold = FACE_ONLY_THRESHOLD + 0.1 + multi_face_penalty if is_distant_face else FACE_ONLY_THRESHOLD + multi_face_penalty
                    
                    if is_distant_face and frame_count % 30 == 0:
                        print(f"[DEBUG] Face[{face_idx}] ä¸åœ¨ä»»ä½•äººä½“æ¡†å†…(è¿œå¤„äººè„¸)ï¼Œä½¿ç”¨æ›´é«˜é˜ˆå€¼={current_threshold:.2f}")
                    
                    face_feature = face_recognizer.extract_feature(
                        frame, face.bbox, face.keypoints
                    )
                    if face_feature and face_feature.embedding is not None:
                        # ä¸ç›®æ ‡äººè„¸ç‰¹å¾æ¯”è¾ƒï¼Œæ‰¾æœ€é«˜ç›¸ä¼¼åº¦
                        for vi, view in enumerate(mv_recognizer.target.view_features):
                            if view.has_face and view.face_embedding is not None:
                                sim = float(np.dot(face_feature.embedding, view.face_embedding))
                                if frame_count % 30 == 0:
                                    print(f"[DEBUG] Face[{face_idx}] vs View[{vi}]: sim={sim:.3f}, threshold={current_threshold:.2f}")
                                # åªæœ‰è¶…è¿‡å½“å‰é˜ˆå€¼æ‰è®°å½•ä¸ºå€™é€‰
                                if sim >= current_threshold and sim > best_face_sim:
                                    best_face_sim = sim
                                    best_face_idx = face_idx
                                    best_view_idx = vi
                
                # ä½¿ç”¨æ›´ä¸¥æ ¼çš„é˜ˆå€¼åˆ¤æ–­ï¼ˆå·²åœ¨ä¸Šé¢çš„å¾ªç¯ä¸­è¿‡æ»¤ï¼‰
                if best_face_sim >= FACE_ONLY_THRESHOLD:
                    matched_any = True
                    target_face_idx = best_face_idx
                    lost_frames = 0
                    
                    # ä¿å­˜å½“å‰åŒ¹é…ä¿¡æ¯ç”¨äºæ˜¾ç¤º
                    current_match_info = {
                        'type': 'face_only',
                        'similarity': best_face_sim,
                        'method': f'face_only (vs View[{best_view_idx}])',
                        'threshold': FACE_ONLY_THRESHOLD
                    }
                    
                    # ç”¨äººè„¸æ¡†æ›´æ–°ä½ç½®
                    mv_recognizer.update_tracking(faces[best_face_idx].bbox)
                    if frame_count % 30 == 0:
                        print(f"[DEBUG] äººè„¸åŒ¹é…æˆåŠŸ! face_idx={best_face_idx}, sim={best_face_sim:.3f}")
                    
                    # ä»…äººè„¸åŒ¹é…æ—¶çš„è‡ªåŠ¨å­¦ä¹  - æ›´ä¸¥æ ¼çš„æ¡ä»¶
                    # åªæœ‰å•äººåœºæ™¯+é«˜äººè„¸ç›¸ä¼¼åº¦æ‰å­¦ä¹ ï¼Œé¿å…å¤šäººåœºæ™¯è¯¯å­¦ä¹ 
                    # ä½¿ç”¨å¤–å±‚å®šä¹‰çš„ is_single_person_scene
                    if best_face_sim >= 0.80 and is_single_person_scene:
                        face_only_view = ViewFeature(timestamp=time.time())
                        face_feature = face_recognizer.extract_feature(
                            frame, faces[best_face_idx].bbox, faces[best_face_idx].keypoints
                        )
                        if face_feature:
                            face_only_view.has_face = True
                            face_only_view.face_embedding = face_feature.embedding
                            learned, op_info = mv_recognizer.auto_learn(face_only_view, faces[best_face_idx].bbox, True)
                            if learned:
                                print(f"[è‡ªåŠ¨å­¦ä¹ ] ä»…äººè„¸(sim={best_face_sim:.2f}) -> {op_info}")
                    elif frame_count % 30 == 0 and best_face_sim >= 0.70:
                        reason = "å¤šäººåœºæ™¯" if not is_single_person_scene else f"ç›¸ä¼¼åº¦ä¸è¶³({best_face_sim:.2f}<0.80)"
                        print(f"[DEBUG] ä»…äººè„¸åŒ¹é…ä¸å­¦ä¹ : {reason}")
                elif frame_count % 30 == 0 and best_face_sim > 0:
                    print(f"[DEBUG] äººè„¸æœ€é«˜ç›¸ä¼¼åº¦ {best_face_sim:.3f} < é˜ˆå€¼ {FACE_ONLY_THRESHOLD}")
            
            if not matched_any:
                lost_frames += 1
                # æ¸…ç©ºåŒ¹é…å†å²ï¼Œé˜²æ­¢è¯¯åŒ¹é…
                mv_recognizer.clear_match_history()
                if frame_count % 30 == 0:
                    print(f"[DEBUG] æœªåŒ¹é…, lost_frames={lost_frames}/{max_lost_frames}")
                if lost_frames >= max_lost_frames:
                    state_machine.state = SystemState.LOST_TARGET
                    print("[ç›®æ ‡ä¸¢å¤±] ç­‰å¾…é‡æ–°å‡ºç°æˆ–æ‰‹åŠ¿åœæ­¢")
        
        elif state_machine.state == SystemState.LOST_TARGET:
            # å°è¯•é‡æ–°åŒ¹é… - ä½¿ç”¨æœ€ä½³åŒ¹é…ç­–ç•¥ + è¿ç»­å¸§ç¡®è®¤
            # å…³é”®ï¼šLOST_TARGET é‡æ–°é”å®šéœ€è¦è¿ç»­Nå¸§åŒ¹é…æˆåŠŸæ‰ç¡®è®¤
            
            # é‡æ–°é”å®šçš„é˜ˆå€¼ - é€‚åº¦é™ä½ä»¥æé«˜å¯ç”¨æ€§
            RELOCK_BODY_THRESHOLD = 0.75  # ä»…äººä½“æ—¶çš„é˜ˆå€¼
            RELOCK_FUSED_THRESHOLD = 0.65  # æœ‰äººè„¸æ—¶çš„ç»¼åˆé˜ˆå€¼
            RELOCK_FACE_SIM_THRESHOLD = 0.55  # äººè„¸ç›¸ä¼¼åº¦ä¸‹é™
            
            # å¤šäººåœºæ™¯ä¸‹ï¼Œå¿…é¡»æœ‰äººè„¸éªŒè¯æ‰èƒ½é‡æ–°é”å®š
            # æ³¨æ„ï¼šLOST_TARGET çŠ¶æ€éœ€è¦é‡æ–°è®¡ç®—åœºæ™¯ç±»å‹
            relock_is_multi_person = len(persons) > 1 or len(faces) > 1
            require_face_for_relock = relock_is_multi_person or (mv_recognizer.target and mv_recognizer.target.has_face_view)
            
            # å½“å‰å¸§æœ€ä½³åŒ¹é…
            current_best_match = None
            current_best_idx = -1
            
            for idx, person in enumerate(persons):
                view = extract_view_feature(
                    frame, person.bbox, faces, face_recognizer, enhanced_reid
                )
                
                # ä½¿ç”¨ return_details=True è·å–è¯¦ç»†ä¿¡æ¯
                result = mv_recognizer.is_same_target(
                    view, person.bbox, return_details=True
                )
                # è¿”å›å€¼æ˜¯ (is_match, similarity, method, details)
                is_match = result[0]
                similarity = result[1]
                method = result[2]
                details = result[3] if len(result) > 3 else {}
                
                # é‡æ–°é”å®šéœ€è¦æ›´ä¸¥æ ¼çš„éªŒè¯
                if is_match:
                    face_in_person = view.has_face and view.face_embedding is not None
                    
                    # ä» details ä¸­è·å–äººè„¸ç›¸ä¼¼åº¦
                    face_sim = details.get('face_sim', 0.0) if details else 0.0
                    
                    # æ£€æŸ¥æ˜¯å¦æ»¡è¶³é˜ˆå€¼è¦æ±‚
                    if ('fused' in method or 'face_priority' in method) and face_in_person:
                        # æœ‰äººè„¸éªŒè¯ï¼šæ£€æŸ¥äººè„¸ç›¸ä¼¼åº¦æ˜¯å¦è¶³å¤Ÿé«˜
                        if similarity >= RELOCK_FUSED_THRESHOLD and face_sim >= RELOCK_FACE_SIM_THRESHOLD:
                            if current_best_match is None or similarity > current_best_match[1]:
                                current_best_match = (idx, similarity, method, view, True, face_sim)
                                current_best_idx = idx
                    elif not require_face_for_relock and similarity >= RELOCK_BODY_THRESHOLD:
                        # ä»…äººä½“åŒ¹é…ï¼šåªåœ¨å•äººåœºæ™¯ä¸”ç›®æ ‡æ²¡æœ‰äººè„¸ç‰¹å¾æ—¶å…è®¸
                        if current_best_match is None or similarity > current_best_match[1]:
                            current_best_match = (idx, similarity, method, view, False, 0.0)
                            current_best_idx = idx
            
            # è¿ç»­å¸§ç¡®è®¤æœºåˆ¶
            if current_best_match:
                idx, similarity, method, view, has_face, face_sim = current_best_match
                
                # æ£€æŸ¥æ˜¯å¦ä¸ä¸Šä¸€å¸§å€™é€‰äººç›¸åŒ
                if current_best_idx == relock_candidate_idx:
                    relock_confirm_count += 1
                else:
                    # å€™é€‰äººå˜åŒ–ï¼Œé‡æ–°è®¡æ•°
                    relock_candidate_idx = current_best_idx
                    relock_confirm_count = 1
                
                if frame_count % 30 == 0:
                    print(f"[DEBUG] é‡æ–°é”å®šå€™é€‰: Person[{idx}], sim={similarity:.2f}, è¿ç»­å¸§={relock_confirm_count}/{RELOCK_CONFIRM_FRAMES}")
                
                # è¾¾åˆ°è¿ç»­å¸§è¦æ±‚ï¼Œç¡®è®¤é‡æ–°é”å®š
                if relock_confirm_count >= RELOCK_CONFIRM_FRAMES:
                    state_machine.state = SystemState.TRACKING
                    target_person_idx = idx
                    lost_frames = 0
                    relock_confirm_count = 0
                    relock_candidate_idx = -1
                    mv_recognizer.update_tracking(persons[idx].bbox)
                    relock_type = "äººä½“+äººè„¸" if has_face else "ä»…äººä½“"
                    if has_face:
                        print(f"[é‡æ–°é”å®š] ç›®æ ‡å·²æ¢å¤ ({relock_type}, sim={similarity:.2f}, face={face_sim:.2f}, è¿ç»­ç¡®è®¤)")
                    else:
                        print(f"[é‡æ–°é”å®š] ç›®æ ‡å·²æ¢å¤ ({relock_type}, sim={similarity:.2f}, è¿ç»­ç¡®è®¤)")
            else:
                # æ— åŒ¹é…ï¼Œé‡ç½®è¿ç»­å¸§è®¡æ•°
                if relock_confirm_count > 0:
                    relock_confirm_count = 0
                    relock_candidate_idx = -1
                    if frame_count % 30 == 0:
                        print(f"[DEBUG] é‡æ–°é”å®šå€™é€‰ä¸¢å¤±ï¼Œé‡ç½®è®¡æ•°")
            
            # ç¦ç”¨ä»…äººè„¸é‡æ–°é”å®š - å¤ªå®¹æ˜“è¯¯è¯†åˆ«è¿œå¤„çš„ç›¸ä¼¼äººè„¸
            # åªæœ‰å½“äººè„¸åœ¨äººä½“æ¡†å†…æ—¶æ‰èƒ½é€šè¿‡äººä½“+äººè„¸è”åˆåŒ¹é…æ¥é”å®š
            # åŸå› ï¼šä»…äººè„¸åŒ¹é…ç¼ºå°‘ä½ç½®ã€èº«ä½“ç‰¹å¾ç­‰å…³è”ä¿¡æ¯ï¼Œå®¹æ˜“è¯¯åŒ¹é…
            # if not matched_any and faces and mv_recognizer.target and mv_recognizer.target.has_face_view:
            #     for face_idx, face in enumerate(faces):
            #         ...
        
        # ============== ç»˜åˆ¶ ==============
        # ç»˜åˆ¶äººä½“æ¡†
        for idx, person in enumerate(persons):
            px1, py1, px2, py2 = person.bbox.astype(int)
            
            if state_machine.state == SystemState.IDLE:
                color = (255, 165, 0)  # æ©™è‰²
                label = "Candidate"
            elif idx == target_person_idx:
                color = (0, 255, 0)  # ç»¿è‰²
                label = "TARGET"
            else:
                color = (0, 0, 255)  # çº¢è‰²
                label = "Other"
            
            cv2.rectangle(frame, (px1, py1), (px2, py2), color, 2)
            
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
            cv2.rectangle(frame, (px1, py1 - label_size[1] - 5),
                         (px1 + label_size[0], py1), color, -1)
            cv2.putText(frame, label, (px1, py1 - 3),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # ç»˜åˆ¶äººè„¸æ¡† - ä½¿ç”¨ä¹‹å‰åŒ¹é…è¿‡ç¨‹ä¸­çš„ç»“æœï¼Œé¿å…é‡å¤è®¡ç®—
        # target_face_idx æ˜¯ä»…äººè„¸åŒ¹é…æ—¶ç¡®å®šçš„ç›®æ ‡äººè„¸
        # å¯¹äºæœ‰äººä½“åŒ¹é…çš„æƒ…å†µï¼Œåªæœ‰ face_in_person=True ä¸”é€šè¿‡ face_priority éªŒè¯çš„æ‰æ ‡è®°ä¸ºç›®æ ‡
        for face_idx, face in enumerate(faces):
            fx1, fy1, fx2, fy2 = face.bbox.astype(int)
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºç›®æ ‡äººè„¸
            # å…³é”®ï¼šä¸èƒ½åªçœ‹æ˜¯å¦åœ¨ç›®æ ‡äººä½“æ¡†å†…ï¼Œå¿…é¡»æ˜¯åŒ¹é…è¿‡ç¨‹ä¸­éªŒè¯è¿‡çš„
            # ä½¿ç”¨ current_match_info æ¥åˆ¤æ–­æ˜¯å¦ç»è¿‡äº†äººè„¸éªŒè¯
            is_target_face = False
            
            if target_person_idx >= 0 and target_person_idx < len(persons):
                px1, py1, px2, py2 = persons[target_person_idx].bbox
                fc_x, fc_y = (fx1 + fx2) // 2, (fy1 + fy2) // 2
                if px1 <= fc_x <= px2 and py1 <= fc_y <= py2:
                    # äººè„¸åœ¨ç›®æ ‡äººä½“æ¡†å†…
                    # åªæœ‰å½“åŒ¹é…æ–¹æ³•åŒ…å« face_priority æˆ– fused æ—¶ï¼Œæ‰è¡¨ç¤ºäººè„¸å·²éªŒè¯
                    if current_match_info:
                        method = current_match_info.get('method', '')
                        if 'face_priority' in method or 'fused' in method:
                            # äººè„¸å·²ç»é€šè¿‡éªŒè¯
                            is_target_face = True
                        # å¦åˆ™æ˜¯çº¯äººä½“åŒ¹é…ï¼Œä¸èƒ½ç¡®å®šäººè„¸æ˜¯å¦å±äºç›®æ ‡
                    # å¦‚æœåªæœ‰ä¸€ä¸ªäººè„¸åœ¨äººä½“æ¡†å†…ï¼Œä¹Ÿå¯ä»¥è®¤ä¸ºæ˜¯ç›®æ ‡äººè„¸
                    elif len([f for f in faces if px1 <= (f.bbox[0]+f.bbox[2])//2 <= px2 and py1 <= (f.bbox[1]+f.bbox[3])//2 <= py2]) == 1:
                        is_target_face = True
            
            if face_idx == target_face_idx and target_person_idx < 0:
                # ä»…äººè„¸åŒ¹é…çš„ç›®æ ‡
                cv2.rectangle(frame, (fx1, fy1), (fx2, fy2), (0, 255, 0), 2)
                cv2.putText(frame, "TARGET(Face)", (fx1, fy1 - 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
            elif is_target_face:
                # ç›®æ ‡äººä½“å†…çš„äººè„¸ - ç”¨ç»¿è‰²é«˜äº®
                cv2.rectangle(frame, (fx1, fy1), (fx2, fy2), (0, 255, 0), 2)
            elif state_machine.state == SystemState.IDLE:
                # ç©ºé—²çŠ¶æ€æ˜¾ç¤ºæ‰€æœ‰äººè„¸
                cv2.rectangle(frame, (fx1, fy1), (fx2, fy2), (255, 200, 0), 1)
            else:
                # è·Ÿè¸ªçŠ¶æ€æ˜¾ç¤ºéç›®æ ‡äººè„¸ï¼ˆæ·¡è‰²ï¼‰
                cv2.rectangle(frame, (fx1, fy1), (fx2, fy2), (128, 128, 128), 1)
        
        # ç»˜åˆ¶æ‰‹åŠ¿æŒ‡ç¤ºå™¨ (å«è¿›åº¦æ¡)
        draw_gesture_indicator(frame, gesture, state_machine.state, hold_progress)
        
        # çŠ¶æ€ä¿¡æ¯
        target_info = "None"
        if mv_recognizer.target:
            num_views = mv_recognizer.target.num_views
            # ç»Ÿè®¡æœ‰äººè„¸å’Œæœ‰äººä½“çš„è§†è§’æ•°é‡
            face_views = sum(1 for v in mv_recognizer.target.view_features if v.has_face)
            body_views = sum(1 for v in mv_recognizer.target.view_features if v.part_color_hists is not None)
            target_info = f"Views={num_views} (F:{face_views} B:{body_views})"
        
        # åŒ¹é…ä¿¡æ¯
        match_info = ""
        if current_match_info:
            sim = current_match_info['similarity']
            thresh = current_match_info['threshold']
            mtype = current_match_info['type']
            match_info = f"Match: {mtype} sim={sim:.2f} (>={thresh:.2f})"
        
        info_lines = [
            f"FPS: {fps:.1f}",
            f"State: {state_machine.state.value}",
            f"Persons: {len(persons)}, Faces: {len(faces)}",
            f"Target: {target_info}",
            match_info,
            f"Gesture: {gesture.gesture_type.value}" + (f" ({hold_progress*100:.0f}%)" if hold_progress > 0 else "")
        ]
        
        for i, line in enumerate(info_lines):
            if line:
                # åŒ¹é…ä¿¡æ¯ç”¨ä¸åŒé¢œè‰²
                color = (0, 255, 255) if "Match:" in line else (0, 255, 0)
                cv2.putText(frame, line, (10, 25 + i * 20),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 1)
        
        # æ‰‹åŠ¿æç¤º
        if state_machine.state == SystemState.IDLE:
            cv2.putText(frame, f"Hold OPEN PALM {GESTURE_HOLD_DURATION:.0f}s to START", 
                       (10, h - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        elif state_machine.state == SystemState.TRACKING:
            cv2.putText(frame, f"Hold OPEN PALM {GESTURE_HOLD_DURATION:.0f}s to STOP", 
                       (10, h - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        elif state_machine.state == SystemState.LOST_TARGET:
            cv2.putText(frame, f"Target LOST - Hold PALM to STOP", 
                       (10, h - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 165, 255), 2)
        
        cv2.imshow(window_name, frame)
        
        # é”®ç›˜æ§åˆ¶
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            break
        elif key == ord('s'):
            if persons:
                nearest, _ = find_nearest_person(persons, frame_center)
                if nearest:
                    view = extract_view_feature(
                        frame, nearest.bbox, faces, face_recognizer, enhanced_reid
                    )
                    mv_recognizer.set_target(view, nearest.bbox)
                    state_machine.state = SystemState.TRACKING
                    print("[æ‰‹åŠ¨ä¿å­˜] ç›®æ ‡å·²é”å®š")
        elif key == ord('a'):
            if mv_recognizer.target and target_person_idx >= 0:
                person = persons[target_person_idx]
                view = extract_view_feature(
                    frame, person.bbox, faces, face_recognizer, enhanced_reid
                )
                if mv_recognizer.target._is_different_view(view, 0.75):
                    mv_recognizer.target.view_features.append(view)
                    print(f"[æ‰‹åŠ¨æ·»åŠ ] æ–°è§†è§’, æ€»æ•°: {mv_recognizer.target.num_views}")
        elif key == ord('c'):
            mv_recognizer.clear_target()
            state_machine.state = SystemState.IDLE
            print("[æ‰‹åŠ¨æ¸…é™¤] ç›®æ ‡å·²æ¸…é™¤")
        elif key == ord('m'):
            mv_config.auto_learn = not mv_config.auto_learn
            print(f"[è‡ªåŠ¨å­¦ä¹ ] {'å¼€å¯' if mv_config.auto_learn else 'å…³é—­'}")
    
    gesture_detector.release()
    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    main()
