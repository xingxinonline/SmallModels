# Project Context for AI Agents

This repository contains the recommended AI models, architecture analysis, and deployment guides for the **S300 Chip** (NPU/DSP/MCU).

## ğŸ“‚ Documentation Structure

*   **[README.md](README.md)**: Project overview, hardware specifications (NPU/DSP/MCU), and the list of deployable open-source models. **Start here for general context.**
*   **[ARCHITECTURE.md](ARCHITECTURE.md)**: Technical deep dive, including architecture analysis (WebRTC vs Native), performance benchmarking, model validation protocols, and end-to-end system testing.

## ğŸ¤– Hardware Constraints (Critical for Code Generation)

When generating code or suggesting models for this project, **ALWAYS** keep the following S300 hardware constraints in mind:

*   **NPU (Neural Processing Unit)**:
    *   **Precision**: Int8 / Int16 only. **No Float32 support in NPU.**
    *   **Operators**: Conv2D (Kernel <= 7x7), Pooling (Kernel <= 15), ReLU/Leaky-ReLU/Softmax.
    *   **Unsupported**: Complex dynamic control flow, large kernels (>7x7), some advanced activations (e.g., GELU, Swish need approximation).
*   **DSP (SensPro 250)**:
    *   Best for: Audio pre-processing (AEC, AGC, VAD), FFT, Sensor Fusion.
    *   Libraries: CEVA ClearVox, CMSIS-DSP.
*   **MCU (Cortex-M4)**:
    *   Best for: System control, peripheral management, lightweight business logic.
    *   Avoid heavy computation here.

## ğŸ› ï¸ Development Guidelines

### Python Environment (UV)

We use **[uv](https://github.com/astral-sh/uv)** for fast Python package management.

*   **Install**: `pip install uv`
*   **Sync**: `uv pip sync requirements.txt`
*   **Add Package**: `uv add <package>`

### Git Commit Convention

Follow the **Conventional Commits** format:

```
<type>(<scope>): <short description>

WHAT: ...
WHY: ...
HOW: ...
```

*   **Types**: `feat`, `fix`, `docs`, `style`, `refactor`, `perf`, `test`, `build`, `ci`, `chore`, `revert`.
*   **Scopes**: `npu`, `dsp`, `mcu`, `audio`, `vision`, `test`, `docs`, `gesture`, `recognizer`, `matching`.

### Git Commit Splitting (æ‹†åˆ†æäº¤è§„èŒƒ) â­

**å½“ä¸€æ¬¡å¼€å‘æ¶‰åŠå¤šä¸ªç‹¬ç«‹åŠŸèƒ½æ—¶ï¼Œå¿…é¡»æ‹†åˆ†æäº¤**ï¼š

1. **æŒ‰åŠŸèƒ½æ¨¡å—æ‹†åˆ†**ï¼šæ¯ä¸ªç‹¬ç«‹çš„åŠŸèƒ½ç‚¹ä¸€ä¸ªæäº¤
2. **æŒ‰æ–‡ä»¶ç±»å‹æ‹†åˆ†**ï¼šä»£ç ã€æ–‡æ¡£ã€æµ‹è¯•åˆ†å¼€æäº¤
3. **ä¿æŒæäº¤åŸå­æ€§**ï¼šæ¯ä¸ªæäº¤åº”è¯¥æ˜¯å¯ç‹¬ç«‹ç†è§£çš„å®Œæ•´å˜æ›´

**æ‹†åˆ†ç¤ºä¾‹**ï¼š

```bash
# é”™è¯¯ï¼šä¸€ä¸ªå¤§æäº¤åŒ…å«æ‰€æœ‰æ”¹åŠ¨
git commit -m "feat: å®Œæˆç›®æ ‡è·Ÿéšä¼˜åŒ–"

# æ­£ç¡®ï¼šæŒ‰åŠŸèƒ½æ‹†åˆ†
git commit -m "feat(gesture): æ‰‹åŠ¿æ£€æµ‹å™¨ä¼˜åŒ– - é€‰æ‹©åšæœ‰æ•ˆæ‰‹åŠ¿çš„æ‰‹"
git commit -m "refactor(recognizer): å¤šè§†è§’è¯†åˆ«å™¨æ”¹è¿› - è§†è§’åº“ç®¡ç†ç­–ç•¥"
git commit -m "feat(matching): åˆ†å±‚åŒ¹é…ç­–ç•¥ - åŸºäºäººè„¸è´¨é‡åˆ†çº§"
git commit -m "docs: æ›´æ–°æ–¹æ¡ˆDå¯åŠ¨é€»è¾‘å’ŒåŒ¹é…ç­–ç•¥æ–‡æ¡£"
```

**æ‹†åˆ†åŸåˆ™**ï¼š
- âœ… ä¸€ä¸ªåŠŸèƒ½ç‚¹ = ä¸€ä¸ªæäº¤
- âœ… ç›¸å…³çš„ä»£ç +æµ‹è¯•å¯ä»¥æ”¾ä¸€èµ·
- âœ… æ–‡æ¡£æ›´æ–°å•ç‹¬æäº¤
- âŒ ä¸è¦æŠŠä¸ç›¸å…³çš„æ”¹åŠ¨æ··åœ¨ä¸€èµ·
- âŒ ä¸è¦ä¸ºäº†"å¹²å‡€"è€Œ squash æœ‰æ„ä¹‰çš„å†å²

## ğŸ“ Task Instructions

*   If asked to **recommend a model**, check `README.md` first.
*   If asked about **testing or validation**, refer to `ARCHITECTURE.md`.
*   If asked to **write code**, ensure it is compatible with the S300 constraints (e.g., use quantization-aware training, avoid unsupported ops).

## ğŸ’» Local PC Environment (Pre-validation)

Before porting to the S300 chip, models will be validated on the local PC to verify logic and performance baselines.

*   **OS**: Windows 11 Pro (10.0.22631)
*   **CPU**: Intel Core i7-12700KF (12 Cores, 20 Threads)
*   **RAM**: 32 GB
*   **GPU**: NVIDIA GeForce RTX 3070 Ti
*   **Goal**: Run FP32/Int8 models locally to check functional correctness and simulate NPU constraints (e.g., using TFLite interpreter with Int8 delegates).

## ğŸ”„ Development Workflow (å¼€å‘æµç¨‹)

When implementing a new feature or model validation, follow this **complete workflow**:

### 1. Architecture Design (æ¶æ„è®¾è®¡)

*   Define system modules and their responsibilities
*   Draw data flow diagrams
*   Specify interfaces between components
*   Document in `examples/<feature>/README.md`

### 2. Code Implementation (ä»£ç å®ç°)

*   Create modular, reusable code structure
*   Follow Python best practices (type hints, docstrings)
*   Separate concerns: config, capture, inference, visualization

### 3. Dependency Verification (ä¾èµ–éªŒè¯)

```bash
# Install dependencies using uv
uv add <package_name>

# Verify installation
uv run python -c "import <package>; print(<package>.__version__)"
```

### 4. Compile Verification (ç¼–è¯‘éªŒè¯)

```bash
# Check for syntax errors
uv run python -m py_compile <file.py>

# Or use IDE's built-in linting
```

### 5. Execution Testing (æ‰§è¡Œæµ‹è¯•)

```bash
# Run the application
uv run python <main_script.py>
```

### 5.1 Interactive Testing with Background Processes (äº¤äº’å¼åå°æµ‹è¯•)

**âš ï¸ CRITICAL: When running interactive applications (camera, GUI, gesture control, etc.):**

1. **Do NOT use `isBackground=true` for tests requiring user interaction**
   - Background mode cannot capture user input or show real-time output
   - Agent will not receive test results automatically

2. **Correct approach for interactive tests:**
   ```bash
   # Run in foreground (isBackground=false)
   uv run python <interactive_script.py>
   ```

3. **If background mode is necessary:**
   - Wait sufficient time for user to complete testing
   - Use `get_terminal_output` to actively check results
   - Do NOT assume test passed without checking output
   - Agent MUST call `get_terminal_output` after reasonable wait time (5-10 seconds)

4. **Debug logging best practice:**
   - Add debug logs for state transitions and key events
   - Use conditional debug flags: `process_gesture(..., debug=True)`
   - Print logs in a parseable format for automated analysis

**Example - Wrong approach:**
```python
# âŒ Wrong: Start background process and immediately ask user for results
run_in_terminal(command, isBackground=true)
# Then ask user: "è¯·å‘Šè¯‰æˆ‘ç»“æœ"
```

**Example - Correct approach:**
```python
# âœ… Correct: Start background process, wait, then check output
run_in_terminal(command, isBackground=true)
# Wait for user to interact...
get_terminal_output(terminal_id)  # Agent actively fetches results
# Analyze output and provide feedback
```

### 6. Documentation Update (æ–‡æ¡£æ›´æ–°)

*   Update `AGENTS.md` with new workflow requirements
*   Update `README.md` if new models are added
*   Create example-specific documentation

## ğŸ“ Examples Directory Structure

```
examples/
â”œâ”€â”€ face_detection/           # äººè„¸æ£€æµ‹ç¤ºä¾‹
â”‚   â”œâ”€â”€ README.md             # æ¶æ„è®¾è®¡æ–‡æ¡£
â”‚   â”œâ”€â”€ config.py             # é…ç½®å‚æ•°
â”‚   â”œâ”€â”€ camera.py             # æ‘„åƒå¤´é‡‡é›†
â”‚   â”œâ”€â”€ detector.py           # SCRFD æ£€æµ‹å™¨
â”‚   â”œâ”€â”€ visualizer.py         # å¯è§†åŒ–æ¨¡å—
â”‚   â”œâ”€â”€ download_model.py     # æ¨¡å‹ä¸‹è½½
â”‚   â”œâ”€â”€ main.py               # ä¸»ç¨‹åºå…¥å£
â”‚   â””â”€â”€ models/               # æ¨¡å‹æ–‡ä»¶
â”œâ”€â”€ target_following/         # ç›®æ ‡è·Ÿéšç¤ºä¾‹ (æ‰‹åŠ¿æ§åˆ¶)
â”‚   â”œâ”€â”€ README.md             # æ¶æ„è®¾è®¡æ–‡æ¡£
â”‚   â”œâ”€â”€ config.py             # é…ç½®å‚æ•°ä¸çŠ¶æ€æšä¸¾
â”‚   â”œâ”€â”€ main.py               # ä¸»ç¨‹åºå…¥å£
â”‚   â”œâ”€â”€ core/                 # æ ¸å¿ƒæ¨¡å—
â”‚   â”‚   â”œâ”€â”€ camera.py         # æ‘„åƒå¤´é‡‡é›†
â”‚   â”‚   â””â”€â”€ state_machine.py  # çŠ¶æ€æœºæ§åˆ¶å™¨
â”‚   â”œâ”€â”€ detectors/            # æ£€æµ‹å™¨æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ gesture_detector.py    # æ‰‹åŠ¿æ£€æµ‹ (MediaPipe)
â”‚   â”‚   â”œâ”€â”€ face_detector.py       # äººè„¸æ£€æµ‹ (SCRFD)
â”‚   â”‚   â”œâ”€â”€ face_recognizer.py     # äººè„¸è¯†åˆ« (ArcFace)
â”‚   â”‚   â””â”€â”€ person_detector.py     # äººä½“æ£€æµ‹ (YOLOv8-pose)
â”‚   â”œâ”€â”€ trackers/             # è·Ÿè¸ªæ¨¡å—
â”‚   â”‚   â””â”€â”€ target_tracker.py # ç›®æ ‡è·Ÿè¸ªå™¨
â”‚   â”œâ”€â”€ visualizers/          # å¯è§†åŒ–æ¨¡å—
â”‚   â”‚   â””â”€â”€ visualizer.py     # ç»“æœç»˜åˆ¶
â”‚   â”œâ”€â”€ tests/                # å•å…ƒæµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ test_gesture.py   # æ‰‹åŠ¿æ£€æµ‹æµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ test_face.py      # äººè„¸è¯†åˆ«æµ‹è¯•
â”‚   â”‚   â””â”€â”€ test_person.py    # äººä½“æ£€æµ‹æµ‹è¯•
â”‚   â””â”€â”€ models/               # æ¨¡å‹æ–‡ä»¶
â”‚       â”œâ”€â”€ scrfd_500m_bnkps.onnx   # äººè„¸æ£€æµ‹
â”‚       â”œâ”€â”€ w600k_r50.onnx          # äººè„¸è¯†åˆ«
â”‚       â””â”€â”€ yolov8n-pose.onnx       # äººä½“å§¿æ€
â””â”€â”€ <future_examples>/        # æ›´å¤šç¤ºä¾‹...
```

## ğŸ¯ Example: Face Detection Workflow

```bash
# 1. Navigate to example directory
cd examples/face_detection

# 2. Download model
uv run python download_model.py

# 3. Run face detection with camera
uv run python main.py

# Controls:
# - Press 'q' to quit
# - Press 's' to save screenshot
```

## ğŸ¯ Example: Target Following Workflow

```bash
# 1. Navigate to example directory
cd examples/target_following

# 2. Run individual tests first (recommended)
uv run python tests/test_gesture.py  # Test gesture detection
uv run python tests/test_face.py     # Test face recognition
uv run python tests/test_person.py   # Test person detection

# 3. Run integrated target following
uv run python main.py

# Gesture Controls:
# - Open Palm (å¼ å¼€æ‰‹æŒ): Start tracking - locks current face as target
# - Closed Fist (æ¡æ‹³): Stop tracking - returns to idle state
# - Press 'q' to quit

# State Machine:
# IDLE â†’ (Open Palm) â†’ TRACKING â†’ (Closed Fist) â†’ IDLE
#                    â†“
#               LOST_TARGET (if target lost, waits for re-detection)
```

